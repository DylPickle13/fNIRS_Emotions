{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports/Pick Analyses to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import mne\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import matplotlib.patches as mpatches\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import re\n",
    "import seaborn as sns\n",
    "from statsmodels.formula.api import mixedlm, ols\n",
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from preprocess_nirs import *\n",
    "from nirs_functions import *\n",
    "\n",
    "from mne_nirs.channels import picks_pair_to_idx, get_long_channels\n",
    "from mne_nirs.preprocessing import peak_power, scalp_coupling_index_windowed\n",
    "from mne.preprocessing.nirs import source_detector_distances, scalp_coupling_index\n",
    "from mne_connectivity import spectral_connectivity_epochs, spectral_connectivity_time\n",
    "from mne_connectivity.viz import plot_connectivity_circle\n",
    "from mne.viz import circular_layout\n",
    "from mne_nirs.experimental_design import make_first_level_design_matrix\n",
    "from mne_nirs.statistics import run_glm, statsmodels_to_results\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import LeaveOneGroupOut, cross_val_score, GridSearchCV, GroupKFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set the image dpi\n",
    "dpi = 600\n",
    "\n",
    "# Set the number of cores to use\n",
    "n_jobs = 1\n",
    "if os.cpu_count() is not None:\n",
    "    n_jobs = int(np.ceil(os.cpu_count() * 0.75))\n",
    "\n",
    "seed_value = 42\n",
    "# Set the random seed\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "tf.keras.utils.set_random_seed(seed_value)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# Get the mappings of the brain regions\n",
    "mappings = json.load(open(\"processed_data/mappings/mappings.json\"))\n",
    "\n",
    "# Set thresholds for the data\n",
    "sci_threshold = 0.5\n",
    "cv_threshold = 15\n",
    "good_threshold = 0.7\n",
    "peak_power_threshold = 0.1\n",
    "behavioural_accuracy_threshold = 0.6\n",
    "\n",
    "# Preprocess the data\n",
    "process_data = False\n",
    "\n",
    "# Get the participant info plots\n",
    "get_participant_age_plot = False\n",
    "get_meas_dates_plot = False\n",
    "get_short_chns_plot = False\n",
    "get_behavioural_responses = False\n",
    "run_behavioural_responses_stats = False\n",
    "get_behavioural_responses_plots = False\n",
    "\n",
    "# Get the mappings\n",
    "get_mappings = False\n",
    "\n",
    "# Get the non windowed SCI and CV measure\n",
    "get_full_sci = False\n",
    "get_full_cv = False\n",
    "\n",
    "# Get the peak power/scalp coupling index dataframes\n",
    "get_peak_power_sci_df = False\n",
    "get_good_windows_plot = True\n",
    "get_slope_pp_sci_plot = False\n",
    "get_head_size_vs_sci_plot = False\n",
    "get_across_participant_sci_plots = False\n",
    "get_participant_sci_plots = False\n",
    "\n",
    "# Get the epochs\n",
    "get_epochs = False\n",
    "get_face_type_emotion_epochs = False\n",
    "\n",
    "# Get the GLM data/plots\n",
    "get_glm_analysis = False\n",
    "get_ind_glm_plots = False\n",
    "get_group_glm_plots = False\n",
    "get_group_contrast_plots = False\n",
    "\n",
    "# Get the average timeseries activity\n",
    "get_roi_timeseries_activity = False\n",
    "\n",
    "# Get the ERP plots\n",
    "get_erp_plots = False\n",
    "\n",
    "# Get the connectivity data\n",
    "run_ind_connectivity = False\n",
    "get_condition_con_plots = False\n",
    "get_condition_con_gif_plots = False\n",
    "get_variance_con_plots = False\n",
    "run_group_level_t_tests = False\n",
    "get_group_level_t_tests_chord_plots = False\n",
    "get_group_level_t_tests_roi_chord_plots = False\n",
    "\n",
    "# Run decoding analysis\n",
    "run_traditional_raw_within_decoding = False\n",
    "run_traditional_con_within_decoding = False\n",
    "run_traditional_raw_across_decoding = True\n",
    "run_dl_raw_across_decoding = False\n",
    "run_traditional_con_across_decoding = False\n",
    "run_dl_con_across_decoding = False\n",
    "\n",
    "# Get the decoding plots\n",
    "get_decoding_table_scores_plots = False\n",
    "get_decoding_individual_scores_plots = False\n",
    "\n",
    "raws = []\n",
    "raw_ods = []\n",
    "raw_haemos = []\n",
    "raw_haemo_good_recordings = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder path\n",
    "data_path = os.path.join(os.getcwd(), 'data')\n",
    "\n",
    "# Get a list of paths of all the subfolders of the folders labeled 'P_1', 'P_2', etc.\n",
    "participants = [os.path.join(data_path, f) for f in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, f))]\n",
    "\n",
    "participants_with_same_order = []\n",
    "\n",
    "# remove participants P_1 to P_11 but keep P_10, and P_12 onwards\n",
    "# P_1 to P_11 have the same order of faces, and P_86 and P_87 have the same order of faces\n",
    "for i in range(1, 12):\n",
    "    if i != 10:\n",
    "        participants_with_same_order.append(os.path.join(data_path, f'P_{i}'))\n",
    "        #participants.remove(os.path.join(data_path, f'P_{i}'))\n",
    "\n",
    "participants_with_same_order.append(os.path.join(data_path, f'P_87'))\n",
    "#participants.remove(os.path.join(data_path, f'P_87'))\n",
    "\n",
    "# remove participants P_13 due to not recording\n",
    "participants.remove(os.path.join(data_path, f'P_13'))\n",
    "\n",
    "# remove participants P_50 due to ending early\n",
    "participants.remove(os.path.join(data_path, f'P_50'))\n",
    "\n",
    "# participant P_54 used their phone\n",
    "participants.remove(os.path.join(data_path, f'P_54'))\n",
    "\n",
    "# Search recursively for the folder with the .snirf extension\n",
    "fnirs_folders = []\n",
    "for participant in participants:\n",
    "    for root, dirs, files in os.walk(participant):\n",
    "        for file in files:\n",
    "            if file.endswith('.snirf'):\n",
    "                fnirs_folders.append(root)\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    if process_data:\n",
    "        raws = []\n",
    "        raw_ods = []\n",
    "        raw_haemos = []\n",
    "\n",
    "        # Load the snirf files\n",
    "        for folder in fnirs_folders:\n",
    "            # find all the .snirf files in the folder but get the full path\n",
    "            snirf_files = [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith('.snirf')]\n",
    "            if len(snirf_files) == 0:\n",
    "                print(f\"No .snirf files found in {folder}\")\n",
    "                continue\n",
    "            elif len(snirf_files) > 1:\n",
    "                raise Exception(f\"Multiple .snirf files found in {folder}\")\n",
    "            else:\n",
    "                raw = mne.io.read_raw_snirf(snirf_files[0], optode_frame='mri', preload=True, verbose=False)\n",
    "            \n",
    "            # find all the 'description.json' files in the folder but get the full path\n",
    "            description_files = [f for f in os.listdir(folder) if 'description.json' in f]\n",
    "            if len(description_files) == 0:\n",
    "                print(f\"No description.json files found in {folder}\")\n",
    "                continue\n",
    "            elif len(description_files) > 1:\n",
    "                raise Exception(f\"Multiple description.json files found in {folder}\")\n",
    "            else:\n",
    "                description = json.load(open(os.path.join(folder, description_files[0])))\n",
    "                # get the parent's parent folder name\n",
    "                parent_folder = os.path.basename(os.path.dirname(folder))\n",
    "                # find the .csv file in the parent folder\n",
    "                csv_files = [f for f in os.listdir(os.path.join(data_path, parent_folder)) if f.endswith('.csv')]\n",
    "                if len(csv_files) == 0:\n",
    "                    print(f\"No .csv files found in {os.path.join(data_path, parent_folder)}\")\n",
    "                elif len(csv_files) > 1:\n",
    "                    raise Exception(f\"Multiple .csv files found in {os.path.join(data_path, parent_folder)}\")\n",
    "                else:\n",
    "                    # add the path to the .csv file to the description\n",
    "                    description['behavioural_data'] = os.path.join(data_path, parent_folder, csv_files[0])\n",
    "\n",
    "            distances = np.array(source_detector_distances(raw.info))\n",
    "            distance_counts = {\n",
    "                \"short\": int(sum(distances < 0.01)),\n",
    "                \"long\": int(sum(distances >= 0.01))\n",
    "            }\n",
    "\n",
    "            description['distance_counts'] = distance_counts\n",
    "\n",
    "            # add the description to the raw object\n",
    "            raw.info['description'] = str(description)\n",
    "            raws.append(raw)\n",
    "\n",
    "        # sort the raws by the measurement date\n",
    "        raws = sorted(raws, key=lambda x: x.info['meas_date'])\n",
    "\n",
    "        i = 1\n",
    "        for raw in raws:\n",
    "            raw_od, raw_haemo = preprocess_data(raw)\n",
    "            raw_ods.append(raw_od)\n",
    "            raw_haemos.append(raw_haemo)\n",
    "            i += 1\n",
    "\n",
    "        # clear any files in each folder\n",
    "        for folder in ['processed_data/raws', 'processed_data/raw_ods', 'processed_data/raw_haemos']:\n",
    "            for f in os.listdir(folder):\n",
    "                os.remove(os.path.join(folder, f))\n",
    "\n",
    "        for i, (raw, raw_od, raw_haemo) in enumerate(zip(raws, raw_ods, raw_haemos), 1):\n",
    "            # save raw as a fif file\n",
    "            raw.save(f'processed_data/raws/raw{i}.fif', overwrite=True, verbose=False)\n",
    "\n",
    "            # save raw_od as a fif file\n",
    "            raw_od.save(f'processed_data/raw_ods/raw_od{i}.fif', overwrite=True, verbose=False)\n",
    "\n",
    "            # save raw_haemo as a fif file\n",
    "            raw_haemo.save(f'processed_data/raw_haemos/raw_haemo{i}.fif', overwrite=True, verbose=False)\n",
    "        \n",
    "        raws = []\n",
    "        raw_ods = []\n",
    "        raw_haemos = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Participant Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Participant Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_participant_age_plot:\n",
    "    raw_haemo_good_recordings = load_data('raw_haemos', 'good') if len(raw_haemo_good_recordings) == 0 else raw_haemo_good_recordings\n",
    "\n",
    "    ages = []\n",
    "    for raw_haemo in raw_haemo_good_recordings:\n",
    "        description = get_info(raw_haemo)\n",
    "        age = description['age']\n",
    "        ages.append(age)\n",
    "\n",
    "    # Convert ages to numeric type\n",
    "    ages = np.array(ages, dtype=float)\n",
    "\n",
    "    # Sort the ages\n",
    "    ages = sorted(ages)\n",
    "\n",
    "    # Get the number of participants\n",
    "    n_participants = len(ages)\n",
    "\n",
    "    # Get the mean and standard deviation of the ages\n",
    "    mean_age = np.mean(ages)\n",
    "    std_age = np.std(ages)\n",
    "\n",
    "    # Get the minimum and maximum ages\n",
    "    min_age = min(ages)\n",
    "    max_age = max(ages)\n",
    "    \n",
    "    # Plot the ages in a histogram\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.hist(ages, bins=30, color='blue', alpha=0.7)\n",
    "    plt.xlabel('Age (years)', fontsize=14)\n",
    "    plt.ylabel('Count', fontsize=14)\n",
    "    plt.title(f'Age Distribution of Participants\\nN ={n_participants}, Mean={mean_age:.2f}, SD={std_age:.2f}, Min={min_age}, Max={max_age}', fontsize=16)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/participants/age_distribution.png', dpi=dpi / 4)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurement Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_meas_dates_plot:\n",
    "    raw_haemos = load_data('raw_haemos') if len(raw_haemos) == 0 else raw_haemos\n",
    "\n",
    "    # Extract the measurement dates\n",
    "    measurement_dates = [raw_haemo.info['meas_date'] for raw_haemo in raw_haemos]\n",
    "\n",
    "    # Convert to pandas datetime\n",
    "    measurement_dates = pd.to_datetime(measurement_dates)\n",
    "\n",
    "    # Create a plot of the measurement dates\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.plot(measurement_dates, range(1, len(measurement_dates) + 1), 'o-')\n",
    "    plt.xlabel('Measurement date')\n",
    "    plt.ylabel('Participant number')\n",
    "    plt.title('Measurement dates of participants, N = ' + str(len(measurement_dates)))\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/participants/measurement_dates.png', dpi=dpi / 4)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short Distance Channels Distribution Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_short_chns_plot:\n",
    "    raw_haemo_good_recordings = load_data('raw_haemos', 'good') if len(raw_haemo_good_recordings) == 0 else raw_haemo_good_recordings\n",
    "    \n",
    "    total_short_chns = []\n",
    "    for raw_haemo in raw_haemo_good_recordings:\n",
    "        num_short_chns = get_info(raw_haemo)['distance_counts']['short']\n",
    "        total_short_chns.append(num_short_chns)\n",
    "    total_short_chns = np.array(total_short_chns)\n",
    "\n",
    "    # plot a pie chart of the number of short channels\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    plt.pie(Counter(total_short_chns).values(), labels=Counter(total_short_chns).keys(), autopct='%1.1f%%')\n",
    "    plt.title('Distribution of number of short channels across good recordings, N = ' + str(len(total_short_chns)))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/participants/short_channels.png', dpi=dpi / 4)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Behavioural Responses Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_behavioural_responses:\n",
    "    raw_haemos = load_data('raw_haemos') if len(raw_haemos) == 0 else raw_haemos\n",
    "\n",
    "    participant_scores = pd.DataFrame()\n",
    "    for i, raw_haemo in enumerate(raw_haemos, 1):\n",
    "        path = get_info(raw_haemo)['behavioural_data']\n",
    "\n",
    "        # folder_name = (P_(\\d+))\n",
    "        folder_name = re.search(r'P_(\\d+)', path).group(0)\n",
    "\n",
    "        # read the csv file\n",
    "        behavioural_data = pd.read_csv(path)\n",
    "\n",
    "        # only keep the first 6 columns and the column called \"key_resp.keys\"\n",
    "        behavioural_data = behavioural_data.iloc[:, :6].join(behavioural_data[['key_resp.keys', 'key_resp.rt']])\n",
    "\n",
    "        # remove the \"blocknumber\" column\n",
    "        behavioural_data = behavioural_data.drop(columns='blocknumber')\n",
    "\n",
    "        # remove the first row and reset the index\n",
    "        behavioural_data = behavioural_data.iloc[1:].reset_index(drop=True)\n",
    "\n",
    "        # divide the dataframe into rows of 9\n",
    "        behavioural_data = [behavioural_data.iloc[i:i + 9] for i in range(0, len(behavioural_data), 9)]\n",
    "\n",
    "        scores = []\n",
    "        for j, data in enumerate(behavioural_data, 1):\n",
    "            # in the last row, save the \"key_resp.keys\" column\n",
    "            response = data['key_resp.keys'].iloc[-1]\n",
    "\n",
    "            # convert the response to a boolean\n",
    "            response = {'n': False, 'y': True}.get(response, np.nan)\n",
    "            \n",
    "            # in the last row, copy the \"tasknames\" and \"taskmarkers\" value to \"filepaths\" and \"marker\" respectively\n",
    "            data.loc[data.index[-1], [\"filepaths\", \"marker\"]] = [data[\"tasknames\"].iloc[-1], data[\"taskmarker\"].iloc[-1]]\n",
    "\n",
    "            # remove the \"tasknames\" and \"taskmarker\" columns\n",
    "            data = data.drop(columns=[\"tasknames\", \"taskmarker\"])\n",
    "\n",
    "            # check if the (marker - 2000) in the last row was in any of the other rows\n",
    "            correct_response = (data['marker'].iloc[-1] - 2000) in data['marker'].values\n",
    "\n",
    "            # append the data to the scores list\n",
    "            scores.append({\n",
    "                'Participant': i,\n",
    "                'Trial': j,\n",
    "                'Block Name': data['blocknames'].iloc[-1],\n",
    "                'File Path': data['filepaths'].iloc[-1],\n",
    "                'Marker': data['marker'].iloc[-1],\n",
    "                'Response Time': data['key_resp.rt'].iloc[-1],\n",
    "                'Response': response,\n",
    "                'Correct Response': correct_response\n",
    "            })\n",
    "        \n",
    "        # append the scores to the participant_scores dataframe\n",
    "        participant_scores = pd.concat([participant_scores, pd.DataFrame(scores)])\n",
    "\n",
    "    # for each row, if \"Response\" == \"Correct Response\", create a new column called \"Correct\" and set it to True, else False, if the \"Response\" is NaN, set it to NaN, don't get rid of the \"Correct Response\" column\n",
    "    participant_scores['Correct'] = participant_scores.apply(\n",
    "        lambda x: np.nan if pd.isna(x['Response']) else bool(x['Response']) == bool(x['Correct Response']), axis=1\n",
    "    )\n",
    "\n",
    "    # subtract 2000 from the \"Marker\" column\n",
    "    participant_scores['Marker'] = participant_scores['Marker'] - 2000\n",
    "\n",
    "    # for each row in 'Marker' replace it with two columns 'Face Type' and 'Emotion' from the trigger_decoder function, which returns a tuple\n",
    "    participant_scores[['Face Type', 'Emotion']] = participant_scores['Marker'].apply(trigger_decoder).apply(pd.Series)\n",
    "\n",
    "    # remove the \"Block Name\", \"File Path\", \"Marker\", columns\n",
    "    participant_scores = participant_scores.drop(columns=[\"Block Name\", \"File Path\", \"Marker\"])\n",
    "\n",
    "    # move \"Face Type\" and \"Emotion\" to after Participant\n",
    "    participant_scores = participant_scores[['Participant', 'Trial', 'Face Type', 'Emotion', 'Response Time', 'Response', 'Correct Response', 'Correct']]\n",
    "\n",
    "    # save the participant_scores dataframe\n",
    "    participant_scores.to_csv('processed_data/behavioural_responses/participant_scores.csv', index=False)\n",
    "\n",
    "# Load the participant scores\n",
    "participant_scores = pd.read_csv('processed_data/behavioural_responses/participant_scores.csv')\n",
    "\n",
    "# get the number of Correct responses per participant\n",
    "correct_responses = participant_scores.groupby('Participant')['Correct'].sum()\n",
    "\n",
    "# get the indices of the participants with > 60% correct responses\n",
    "good_recordings = correct_responses[correct_responses > 56 * behavioural_accuracy_threshold].index\n",
    "\n",
    "# only keep the participants with > accuracy_threshold correct responses\n",
    "participant_scores = participant_scores[participant_scores['Participant'].isin(good_recordings)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Behavioural Responses Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_behavioural_responses_stats:\n",
    "    # Create a copy of the 'Correct' column and convert it to 1 if True, 0 if False\n",
    "    participant_scores['Correct Numeric'] = np.where(participant_scores['Correct'] == True, 1, 0)\n",
    "\n",
    "    # Create a copy of the relevant columns\n",
    "    data_copy = participant_scores[['Face Type', 'Emotion', 'Correct Numeric']].copy()\n",
    "\n",
    "    # Rename the column for consistency\n",
    "    data_copy = data_copy.rename(columns={'Face Type': 'Face_Type', 'Correct Numeric': 'Correct_Numeric'})\n",
    "\n",
    "    # Build an OLS model including Face Type, Emotion, and their interaction\n",
    "    model = ols(\"Correct_Numeric ~ C(Face_Type) * C(Emotion)\", data=data_copy).fit()\n",
    "\n",
    "    # Perform two-way ANOVA\n",
    "    anova_table = sm.stats.anova_lm(model, typ=3)  # Type 3 ANOVA\n",
    "\n",
    "    # Print the ANOVA table with p-values not in scientific notation\n",
    "    anova_table['PR(>F)'] = anova_table['PR(>F)'].apply(lambda x: f\"{x:.6f}\" if not pd.isnull(x) else x)\n",
    "\n",
    "    # save the anova table\n",
    "    anova_table.to_csv('processed_data/behavioural_responses/anova_table.csv')\n",
    "\n",
    "    p_value_face_type = anova_table.loc['C(Face_Type)', 'PR(>F)']\n",
    "    p_value_emotion = anova_table.loc['C(Emotion)', 'PR(>F)']\n",
    "    p_value_interaction = anova_table.loc['C(Face_Type):C(Emotion)', 'PR(>F)']\n",
    "\n",
    "    # Check if there is a significant correlation between Response Time and Correct\n",
    "    # Ensure both columns have the same length by dropping NaN values in both simultaneously\n",
    "    valid_data = participant_scores[['Response Time', 'Correct Numeric']].dropna()\n",
    "    correlation, p_value = stats.pearsonr(valid_data['Response Time'], valid_data['Correct Numeric'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Behavioural Responses Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_behavioural_responses_plots:\n",
    "    # get the sum of correct responses for each participant\n",
    "    participant_correct_responses = participant_scores.groupby('Participant')['Correct'].sum()\n",
    "\n",
    "    n_blocks = len(participant_scores) / len(participant_scores['Participant'].unique())\n",
    "\n",
    "    # plot this data\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(participant_correct_responses.index, participant_correct_responses.values, width=0.9)\n",
    "    plt.xlabel('Participant')\n",
    "    plt.ylabel('Number of Correct Responses')\n",
    "    plt.title('Number of Correct Responses per Participant, Mean across all ' + str(len(participant_correct_responses)) + ' participants: ' + str(round(participant_correct_responses.mean() / n_blocks * 100, 2)) + '%')\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/behavioural_responses/correct_responses_by_participant.png', dpi=dpi / 4)\n",
    "    plt.close()\n",
    "\n",
    "    # get the percentage of correct responses for each type of face and emotion and the combination of both\n",
    "    face_type_correct_responses = participant_scores.groupby('Face Type')['Correct'].mean() * 100\n",
    "    emotion_correct_responses = participant_scores.groupby('Emotion')['Correct'].mean() * 100\n",
    "    face_type_emotion_correct_responses = participant_scores.groupby(['Face Type', 'Emotion'])['Correct'].mean() * 100\n",
    "\n",
    "    # Create a figure and specify a 2x2 GridSpec\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    gs = fig.add_gridspec(2, 2)  # 2 rows, 2 columns\n",
    "\n",
    "    plt.suptitle('Correct Responses by Face Type, Emotion and Both, N = ' + str(len(participant_correct_responses)), fontsize=16)\n",
    "\n",
    "    # Top-left: Face Type pie chart\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    ax1.pie(face_type_correct_responses,\n",
    "            labels=face_type_correct_responses.index,\n",
    "            autopct='%1.1f%%', startangle=90)\n",
    "    ax1.set_title('Face Type\\np = ' + str(p_value_face_type))\n",
    "\n",
    "    # Top-right: Emotion pie chart\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    ax2.pie(emotion_correct_responses,\n",
    "            labels=emotion_correct_responses.index,\n",
    "            autopct='%1.1f%%', startangle=90)\n",
    "    ax2.set_title('Emotion\\np = ' + str(p_value_emotion))\n",
    "\n",
    "    # Bottom row spanning both columns: Face Type & Emotion pie chart\n",
    "    ax3 = fig.add_subplot(gs[1, :])  # span columns 0 and 1\n",
    "    ax3.pie(face_type_emotion_correct_responses,\n",
    "            labels=[f\"{ft}-{em}\" for ft, em in face_type_emotion_correct_responses.index],\n",
    "            autopct='%1.1f%%', startangle=90)\n",
    "    ax3.set_title('Face Type & Emotion\\np = ' + str(p_value_interaction))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/behavioural_responses/correct_responses_by_face_type_emotion.png', dpi=dpi / 4)\n",
    "    plt.close()\n",
    "\n",
    "    # get the average number of correct responses for each trial\n",
    "    trial_correct_responses = participant_scores.groupby('Trial')['Correct'].mean().to_list()\n",
    "\n",
    "    # calculate the regression line using stats.linregress\n",
    "    x = range(1, len(trial_correct_responses) + 1)\n",
    "    y = trial_correct_responses\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
    "\n",
    "    # plot this data\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(trial_correct_responses) + 1), trial_correct_responses, 'o-')\n",
    "    plt.plot(x, [slope * i + intercept for i in x], 'r--')\n",
    "    plt.xlabel('Trial')\n",
    "    plt.ylabel('Percentage of Correct Responses')\n",
    "    plt.title('Percentage of Correct Responses per Trial, R^2 = ' + str(round(r_value ** 2, 2)) + ', p = ' + str(round(p_value, 4)))\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/behavioural_responses/correct_responses_by_trial.png', dpi=dpi / 4)\n",
    "    plt.close()\n",
    "\n",
    "    # get the average response time for each trial\n",
    "    trial_response_times = participant_scores.groupby('Trial')['Response Time'].mean().to_list()\n",
    "    \n",
    "    # calculate the regression line using stats.linregress\n",
    "    x = range(1, len(trial_response_times) + 1)\n",
    "    y = trial_response_times\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
    "    \n",
    "    # plot this data\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(trial_response_times) + 1), trial_response_times, 'o-')\n",
    "    plt.plot(x, [slope * i + intercept for i in x], 'r--')\n",
    "    plt.xlabel('Trial')\n",
    "    plt.ylabel('Response Time (s)')\n",
    "    plt.title('Response Time per Trial, R^2 = ' + str(round(r_value ** 2, 2)) + ', p = ' + str(p_value) + '\\nCorrelation with Correct answer: ' + str(round(correlation, 4)))\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/behavioural_responses/response_times_by_trial.png', dpi=dpi / 4)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping brain regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_mappings:\n",
    "    raw_haemos = load_data('raw_haemos') if len(raw_haemos) == 0 else raw_haemos\n",
    "\n",
    "    # get the channel names for hbo\n",
    "    ch_names_hbo = [ch_name for ch_name in raw_haemos[0].ch_names if 'hbo' in ch_name]\n",
    "\n",
    "    ch_mapping_hbo = {\n",
    "        \"Left Frontal\": [],\n",
    "        \"Right Frontal\": [],\n",
    "        \"Left Prefrontal\": [],\n",
    "        \"Right Prefrontal\": [],\n",
    "        \"Left Parietal\": [],\n",
    "        \"Right Parietal\": [],\n",
    "        \"Left Occipital\": [],\n",
    "        \"Right Occipital\": []\n",
    "    }\n",
    "\n",
    "    group_boundaries = [0]\n",
    "\n",
    "    ch_mapping_hbo[\"Left Frontal\"].append('S1_D1 hbo')\n",
    "\n",
    "    ch_mapping_hbo[\"Left Frontal\"].append('S1_D2 hbo')\n",
    "\n",
    "    ch_mapping_hbo[\"Left Frontal\"].append('S1_D17 hbo')\n",
    "\n",
    "    # find the channels that have 'S2_', 'S3_', 'S4_', 'S5_' in them\n",
    "    for ch_name in [ch_name for ch_name in ch_names_hbo if 'S2_' in ch_name or 'S3_' in ch_name or 'S4_' in ch_name or 'S5_' in ch_name]:\n",
    "        ch_mapping_hbo[\"Left Frontal\"].append(ch_name)\n",
    "\n",
    "    ch_mapping_hbo[\"Left Frontal\"].append('S6_D2 hbo')\n",
    "\n",
    "    ch_mapping_hbo[\"Left Frontal\"].append('S6_D3 hbo')\n",
    "\n",
    "    ch_mapping_hbo[\"Left Frontal\"].append('S6_D18 hbo')\n",
    "\n",
    "    group_boundaries.append(len(ch_mapping_hbo[\"Left Frontal\"]))\n",
    "\n",
    "    # find the channels that have 'S9_', 'S10_', 'S11_', 'S12_' in them\n",
    "    for ch_name in [ch_name for ch_name in ch_names_hbo if 'S9_' in ch_name or 'S10_' in ch_name or 'S11_' in ch_name or 'S12_' in ch_name]:\n",
    "        ch_mapping_hbo[\"Right Frontal\"].append(ch_name)\n",
    "\n",
    "    group_boundaries.append(len(ch_mapping_hbo[\"Right Frontal\"]) + group_boundaries[-1])\n",
    "\n",
    "    ch_mapping_hbo[\"Left Prefrontal\"].append('S6_D31 hbo')\n",
    "\n",
    "    # find the channels that have 'S7_', 'S8_', 'S25_', 'S26_' in them\n",
    "    for ch_name in [ch_name for ch_name in ch_names_hbo if 'S7_' in ch_name or 'S8_' in ch_name or 'S25_' in ch_name or 'S26_' in ch_name]:\n",
    "        ch_mapping_hbo[\"Left Prefrontal\"].append(ch_name)\n",
    "\n",
    "    group_boundaries.append(len(ch_mapping_hbo[\"Left Prefrontal\"]) + group_boundaries[-1])\n",
    "\n",
    "    # find the channels that have 'S13_', 'S14_', 'S15_', 'S16_' in them\n",
    "    for ch_name in [ch_name for ch_name in ch_names_hbo if 'S13_' in ch_name or 'S14_' in ch_name or 'S15_' in ch_name or 'S16_' in ch_name]:\n",
    "        ch_mapping_hbo[\"Right Prefrontal\"].append(ch_name)\n",
    "\n",
    "    group_boundaries.append(len(ch_mapping_hbo[\"Right Prefrontal\"]) + group_boundaries[-1])\n",
    "\n",
    "    # find the channels that have 'S27_', 'S28_', 'S29_', 'S30_' in them\n",
    "    for ch_name in [ch_name for ch_name in ch_names_hbo if 'S27_' in ch_name or 'S28_' in ch_name or 'S29_' in ch_name or 'S30_' in ch_name]:\n",
    "        ch_mapping_hbo[\"Left Parietal\"].append(ch_name)\n",
    "\n",
    "    group_boundaries.append(len(ch_mapping_hbo[\"Left Parietal\"]) + group_boundaries[-1])\n",
    "\n",
    "    ch_mapping_hbo[\"Right Occipital\"].append('S21_D13 hbo')\n",
    "\n",
    "    ch_mapping_hbo[\"Right Occipital\"].append('S21_D16 hbo')\n",
    "\n",
    "    ch_mapping_hbo[\"Right Occipital\"].append('S23_D15 hbo')\n",
    "\n",
    "    ch_mapping_hbo[\"Right Occipital\"].append('S23_D16 hbo')\n",
    "\n",
    "    # find the channels that have 'S17_', 'S18_', 'S19_', 'S20_' in them\n",
    "    for ch_name in [ch_name for ch_name in ch_names_hbo if 'S17_' in ch_name or 'S18_' in ch_name or 'S19_' in ch_name or 'S20_' in ch_name]:\n",
    "        ch_mapping_hbo[\"Right Parietal\"].append(ch_name)\n",
    "\n",
    "    group_boundaries.append(len(ch_mapping_hbo[\"Right Parietal\"]) + group_boundaries[-1])\n",
    "\n",
    "    # find the channels that have 'S32_', 'S31_' in them\n",
    "    for ch_name in [ch_name for ch_name in ch_names_hbo if 'S32_' in ch_name or 'S31_' in ch_name]:\n",
    "        ch_mapping_hbo[\"Left Occipital\"].append(ch_name)\n",
    "\n",
    "    group_boundaries.append(len(ch_mapping_hbo[\"Left Occipital\"]) + group_boundaries[-1])\n",
    "\n",
    "    # find the channels that have 'S22_', 'S24_' in them\n",
    "    for ch_name in [ch_name for ch_name in ch_names_hbo if 'S22_' in ch_name or 'S24_' in ch_name]:\n",
    "        ch_mapping_hbo[\"Right Occipital\"].append(ch_name)\n",
    "\n",
    "    ch_mapping_hbo[\"Right Occipital\"].append('S21_D28 hbo')\n",
    "\n",
    "    ch_mapping_hbo[\"Right Occipital\"].append('S23_D30 hbo')\n",
    "\n",
    "    ch_mapping_hbr = {region: [channel.replace('hbo', 'hbr') for channel in ch_mapping_hbo[region]] for region in ch_mapping_hbo}\n",
    "\n",
    "    ch_mapping_hbt = {region: [channel.replace('hbo', 'hbt') for channel in ch_mapping_hbo[region]] for region in ch_mapping_hbo}\n",
    "\n",
    "    ch_mapping_all = {region: ch_mapping_hbo[region] + ch_mapping_hbr[region] + ch_mapping_hbt[region] for region in ch_mapping_hbo}\n",
    "\n",
    "    # concatenate the values of the dictionary into a list\n",
    "    all_channels_hbo = [channel for region in ch_mapping_hbo.values() for channel in region]\n",
    "\n",
    "    # duplicate all_channels but replace 'hbo' with 'hbr'\n",
    "    all_channels_hbr = [channel.replace('hbo', 'hbr') for channel in all_channels_hbo]\n",
    "\n",
    "    all_channels_hbt = [channel.replace('hbo', 'hbt') for channel in all_channels_hbo]\n",
    "\n",
    "    # concatenate all_channels_hbo and all_channels_hbr\n",
    "    all_channels = all_channels_hbo + all_channels_hbr + all_channels_hbt\n",
    "\n",
    "    # make a dictionary called ch_mapping_names that has the channel names without the 'hbo' or 'hbr' at the end\n",
    "    ch_mapping_names = {region: [channel[:-4] for channel in ch_mapping_hbo[region]] for region in ch_mapping_hbo}\n",
    "\n",
    "    # make a list of all the channel names without the 'hbo' or 'hbr' at the end\n",
    "    all_channels_names = [channel[:-4] for channel in all_channels_hbo]\n",
    "\n",
    "    ch_names_original = [ch_name[:-4] for ch_name in ch_names_hbo]\n",
    "\n",
    "    # Collect all new variables into a dictionary\n",
    "    mappings = {\n",
    "        \"ch_names_hbo\": ch_names_hbo,\n",
    "        \"ch_mapping_hbo\": ch_mapping_hbo,\n",
    "        \"ch_mapping_hbr\": ch_mapping_hbr,\n",
    "        \"ch_mapping_hbt\": ch_mapping_hbt,\n",
    "        \"ch_mapping_all\": ch_mapping_all,\n",
    "        \"all_channels_hbo\": all_channels_hbo,\n",
    "        \"all_channels_hbr\": all_channels_hbr,\n",
    "        \"all_channels_hbt\": all_channels_hbt,\n",
    "        \"all_channels\": all_channels,\n",
    "        \"ch_mapping_names\": ch_mapping_names,\n",
    "        \"all_channels_names\": all_channels_names,\n",
    "        \"ch_names_original\": ch_names_original,\n",
    "        \"group_boundaries\": group_boundaries\n",
    "    }\n",
    "\n",
    "    # Save the dictionary to a JSON file\n",
    "    with open(\"processed_data//mappings/mappings.json\", \"w\") as json_file:\n",
    "        json.dump(mappings, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalp Coupling Index (SCI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_full_sci:\n",
    "    raw_ods = load_data('raw_ods') if len(raw_ods) == 0 else raw_ods\n",
    "\n",
    "    # for each recording, count the number of channels with a sci greater than good_threshold\n",
    "    good_channels = [sum(scalp_coupling_index(raw_od, verbose=False) >= sci_threshold) for raw_od in raw_ods]\n",
    "    bad_channels = [sum(scalp_coupling_index(raw_od, verbose=False) < sci_threshold) for raw_od in raw_ods]\n",
    "    good_recordings = sum([good_channel >= good_threshold * (good_channel + bad_channel) for good_channel, bad_channel in zip(good_channels, bad_channels)])\n",
    "\n",
    "    # Plot the good vs bad channels for each recording in a dual bar chart\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.bar(range(len(good_channels)), good_channels, label='Good Channels')\n",
    "    ax.bar(range(len(bad_channels)), bad_channels, bottom=good_channels, label='Bad Channels')\n",
    "    ax.set_xlabel('Recording')\n",
    "    ax.set_ylabel('Number of Channels')\n",
    "    ax.axhline(raw_od.info['nchan'] * good_threshold, color='green', linestyle='--')\n",
    "    title = f'Good vs Bad Channels (T = {sci_threshold})\\nGood Recordings: {good_recordings}, N = {len(raw_ods)}, Retention Rate: {good_recordings / len(raw_ods) * 100:.2f}%'\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    plt.savefig(f'plots/signal quality/Signal Quality (SCI).png', dpi=dpi)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coefficient of Variance (CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_full_cv:\n",
    "    raws = load_data('raws') if len(raws) == 0 else raws\n",
    "\n",
    "    # for each recording, count the number of channels with a cv less than cov_threshold\n",
    "    good_channels = [sum(100 * np.std(ch) / np.mean(ch) < cv_threshold for ch in get_long_channels(raw).get_data()) for raw in raws]\n",
    "    bad_channels = [sum(100 * np.std(ch) / np.mean(ch) >= cv_threshold for ch in get_long_channels(raw).get_data()) for raw in raws]\n",
    "    good_recordings = sum([good_channel >= good_threshold * (good_channel + bad_channel) for good_channel, bad_channel in zip(good_channels, bad_channels)])\n",
    "\n",
    "    # Plot the good vs bad channels for each recording in a dual bar chart\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.bar(range(len(good_channels)), good_channels, label='Good Channels')\n",
    "    ax.bar(range(len(bad_channels)), bad_channels, bottom=good_channels, label='Bad Channels')\n",
    "    ax.set_xlabel('Recording')\n",
    "    ax.set_ylabel('Number of Channels')\n",
    "    ax.axhline(raw.info['nchan'] * good_threshold, color='green', linestyle='--')\n",
    "    title = f'Good vs Bad Channels (T = {cv_threshold})\\nGood Recordings: {good_recordings}, N = {len(raws)}, Retention Rate: {good_recordings / len(raws) * 100:.2f}%'\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    plt.savefig(f'plots/signal quality/Signal Quality (CV).png', dpi=dpi)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Peak Power/SCI Sliding Window CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_peak_power_sci_df:\n",
    "    raw_ods = load_data('raw_ods') if len(raw_ods) == 0 else raw_ods\n",
    "\n",
    "    peak_power_df = pd.DataFrame()\n",
    "    sci_df = pd.DataFrame()\n",
    "    for i, raw_od in enumerate(raw_ods, 1):\n",
    "        raw_od_annotated_pp, scores_pp, times_pp = peak_power(raw_od, time_window=5, verbose=False)\n",
    "        raw_od_annotated_sci, scores_sci, times_sci = scalp_coupling_index_windowed(raw_od, time_window=5, verbose=False)\n",
    "\n",
    "        # Convert scores and times to a DataFrame\n",
    "        df_pp = pd.DataFrame(scores_pp.T, columns=[ch_name for ch_name in raw_od.ch_names])\n",
    "        df_sci = pd.DataFrame(scores_sci.T, columns=[ch_name for ch_name in raw_od.ch_names])\n",
    "\n",
    "        # Add time window information\n",
    "        df_pp[\"Start_Time\"] = [t[0] for t in times_pp]\n",
    "        df_pp[\"End_Time\"] = [t[1] for t in times_pp]\n",
    "        df_sci[\"Start_Time\"] = [t[0] for t in times_sci]\n",
    "        df_sci[\"End_Time\"] = [t[1] for t in times_sci]\n",
    "\n",
    "        # Add an index column for window number\n",
    "        df_pp.insert(0, 'Window', range(1, len(df_pp) + 1))\n",
    "        df_sci.insert(0, 'Window', range(1, len(df_sci) + 1))\n",
    "\n",
    "        # Reorder columns so time comes first\n",
    "        df_pp = df_pp[[\"Start_Time\", \"End_Time\"] + list(df_pp.columns[:-2])]\n",
    "        df_sci = df_sci[[\"Start_Time\", \"End_Time\"] + list(df_sci.columns[:-2])]\n",
    "\n",
    "        # remove the columns with '850' in the name\n",
    "        df_pp = df_pp.loc[:, ~df_pp.columns.str.contains('850')]\n",
    "        df_sci = df_sci.loc[:, ~df_sci.columns.str.contains('850')]\n",
    "\n",
    "        # rename the columns to remove the ' 760' at the end if it exists\n",
    "        df_pp.columns = [col[:-4] if col.endswith(' 760') else col for col in df_pp.columns]\n",
    "        df_sci.columns = [col[:-4] if col.endswith(' 760') else col for col in df_sci.columns]\n",
    "\n",
    "        # Add a column for participant number, make it the first column\n",
    "        df_pp.insert(0, 'Participant', i)\n",
    "        df_sci.insert(0, 'Participant', i)\n",
    "\n",
    "        # Append the DataFrame to the list\n",
    "        peak_power_df = pd.concat([peak_power_df, df_pp])\n",
    "        sci_df = pd.concat([sci_df, df_sci])\n",
    "        print(f\"Processed participant {i}\")\n",
    "\n",
    "    # reset the index\n",
    "    peak_power_df.reset_index(drop=True, inplace=True)\n",
    "    sci_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    peak_power_df.to_csv('processed_data/windows/peak_power.csv', index=False)\n",
    "    sci_df.to_csv('processed_data/windows/sci.csv', index=False)\n",
    "\n",
    "# Load the DataFrame\n",
    "peak_power_df = pd.read_csv('processed_data/windows/peak_power.csv')\n",
    "sci_df = pd.read_csv('processed_data/windows/sci.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peak Power/SCI Sliding Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_good_windows_plot:\n",
    "    # Compute the proportion of windows with peak power > peak_power_threshold for each channel\n",
    "    percentage_good_windows_peak_power_df = (\n",
    "        peak_power_df.groupby(\"Participant\")[peak_power_df.columns[4:]]\n",
    "        .apply(lambda df: (df > peak_power_threshold).sum() / len(df))\n",
    "    )\n",
    "\n",
    "    # add a good recordings column\n",
    "    good_recordings = (percentage_good_windows_peak_power_df > good_threshold).sum(axis=1) / len(percentage_good_windows_peak_power_df.columns)\n",
    "    percentage_good_windows_peak_power_df.insert(0, f'Good Recordings (peak_power > {peak_power_threshold} for > {good_threshold * 100}% of channels)', good_recordings)\n",
    "\n",
    "    # Compute the proportion of windows with SCI > good_threshold for each channel\n",
    "    percentage_good_windows_sci_df = (\n",
    "        sci_df.groupby(\"Participant\")[sci_df.columns[4:]]\n",
    "        .apply(lambda df: (df > sci_threshold).sum() / len(df))\n",
    "    )\n",
    "\n",
    "    # add a good recordings column\n",
    "    good_recordings = (percentage_good_windows_sci_df > good_threshold).sum(axis=1) / len(percentage_good_windows_sci_df.columns)\n",
    "    percentage_good_windows_sci_df.insert(0, f'Good Recordings (SCI > {sci_threshold} for > {good_threshold * 100}% of channels)', good_recordings)\n",
    "\n",
    "    # merge the two dataframes on the first 2 columns\n",
    "    percentage_good_windows_df = pd.merge(percentage_good_windows_peak_power_df[percentage_good_windows_peak_power_df.columns[:1]], percentage_good_windows_sci_df[percentage_good_windows_sci_df.columns[:1]], on='Participant')\n",
    "\n",
    "    # create a new column that is true if both columns are greater than good_threshold\n",
    "    percentage_good_windows_df['Good Recording'] = (percentage_good_windows_df.iloc[:, 0] > good_threshold) & (percentage_good_windows_df.iloc[:, 1] > good_threshold)\n",
    "\n",
    "    # save the dataframe to a csv file\n",
    "    percentage_good_windows_df.to_csv('processed_data/windows/percentage_good_windows.csv', index=False)\n",
    "\n",
    "    # Plot a bar chart where only the SCI windows are shown for each participant\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    x = np.arange(len(percentage_good_windows_df))\n",
    "    ax.bar(x, percentage_good_windows_df.iloc[:, 1], label='SCI', color='#FF8C00')  # Darker orange\n",
    "    ax.axhline(good_threshold, color='green', linestyle='--')\n",
    "    ax.set_xlabel('Participant')\n",
    "    ax.set_ylabel('Percentage of Channels (%)')\n",
    "    ax.set_title(f'Percentage of Channels where SCI > ' + str(sci_threshold) + ' for > ' + str(good_threshold * 100) + f'% of windows\\nGood Recordings: {percentage_good_windows_df[\"Good Recording\"].sum()}/{len(percentage_good_windows_df)}, Retention Rate: {percentage_good_windows_df[\"Good Recording\"].sum() / len(percentage_good_windows_df) * 100:.2f}%')\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.savefig(f'plots/signal quality/Percentage of Good Windows.png', dpi=dpi // 2)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peak Power/SCI Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_slope_pp_sci_plot:\n",
    "    pp_channel_slopes = []\n",
    "    sci_channel_slopes = []\n",
    "    for pp_channel, sci_channel in zip(percentage_good_windows_peak_power_df.columns[4:], percentage_good_windows_sci_df.columns[4:]):\n",
    "        pp_slope = []\n",
    "        sci_slope = []\n",
    "        for participant in peak_power_df['Participant'].unique():\n",
    "            # get the data for the channel\n",
    "            pp_array = peak_power_df[peak_power_df['Participant'] == participant][pp_channel]\n",
    "            sci_array = sci_df[sci_df['Participant'] == participant][sci_channel]\n",
    "\n",
    "            # get a line of best fit for the data\n",
    "            x = np.arange(len(pp_array))\n",
    "            pp_m, pp_b = np.polyfit(x, pp_array, 1)\n",
    "            pp_slope.append(pp_m)\n",
    "\n",
    "            sci_m, sci_b = np.polyfit(x, sci_array, 1)\n",
    "            sci_slope.append(sci_m)\n",
    "        pp_channel_slopes.append((pp_channel, np.mean(pp_slope)))\n",
    "        sci_channel_slopes.append((sci_channel, np.mean(sci_slope)))\n",
    "\n",
    "    # plot the slopes\n",
    "    fig, ax = plt.subplots(figsize=(20, 6))\n",
    "    # sort the channels by slope\n",
    "    pp_channel_slopes.sort(key=lambda x: x[1])\n",
    "    sci_channel_slopes.sort(key=lambda x: x[1])\n",
    "    bar_width = 0.45\n",
    "    x = np.arange(len(pp_channel_slopes))\n",
    "    ax.bar(x, [slope for channel, slope in pp_channel_slopes], bar_width, label='Peak Power')\n",
    "    ax.bar(x + bar_width, [slope for channel, slope in sci_channel_slopes], bar_width, label='SCI')\n",
    "    ax.set_xlabel('Channel')\n",
    "    ax.set_ylabel('Slope')\n",
    "    ax.set_title('Slope of Peak Power and SCI over Time')\n",
    "    ax.set_xticks(x + bar_width / 2)\n",
    "    ax.set_xticklabels([channel for channel, slope in pp_channel_slopes])\n",
    "    ax.legend()\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/signal quality/Slope of Peak Power and SCI over Time.png', dpi=dpi)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Head Size vs. SCI Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_head_size_vs_sci_plot:\n",
    "    raw_haemos = load_data('raw_haemos') if len(raw_haemos) == 0 else raw_haemos\n",
    "\n",
    "    cap_size = 58\n",
    "    percentage_good_windows_sci_df_with_head_size = percentage_good_windows_sci_df.copy()\n",
    "\n",
    "    # add the head size to the percentage_good_windows_sci_df as the first column\n",
    "    if 'Head Size (cm)' not in percentage_good_windows_sci_df_with_head_size.columns:\n",
    "        percentage_good_windows_sci_df_with_head_size.insert(0, 'Head Size (cm)', cap_size)\n",
    "\n",
    "    no_head_size = []\n",
    "\n",
    "    for i, raw_haemo in enumerate(raw_haemos, 1):\n",
    "        # get the head size\n",
    "        head_size = get_info(raw_haemo)['remarks']\n",
    "        if head_size:\n",
    "            head_size = float(head_size) * 2.54\n",
    "        else:\n",
    "            # add the participant number to the no_head_size list\n",
    "            no_head_size.append(i)\n",
    "            continue\n",
    "        \n",
    "        # append the head_size to percentage_good_windows_sci_df\n",
    "        percentage_good_windows_sci_df_with_head_size.loc[i, 'Head Size (cm)'] = head_size\n",
    "\n",
    "    # remove the participants with no head size\n",
    "    percentage_good_windows_sci_df_with_head_size = percentage_good_windows_sci_df_with_head_size.drop(no_head_size)\n",
    "\n",
    "    # get the correlation between head size and the channels\n",
    "    correlations = []\n",
    "    for channel in percentage_good_windows_sci_df_with_head_size.columns[2:]:\n",
    "        correlation = percentage_good_windows_sci_df_with_head_size['Head Size (cm)'].corr(percentage_good_windows_sci_df_with_head_size[channel])\n",
    "        correlations.append((channel, correlation))\n",
    "\n",
    "    # get the correlation between head size and the second column\n",
    "    correlation = percentage_good_windows_sci_df_with_head_size['Head Size (cm)'].corr(percentage_good_windows_sci_df_with_head_size.iloc[:, 1])\n",
    "\n",
    "    # plot the correlations\n",
    "    fig, ax = plt.subplots(figsize=(20, 6))\n",
    "    # sort the channels by correlation\n",
    "    correlations.sort(key=lambda x: x[1])\n",
    "    bar_width = 0.45\n",
    "    x = np.arange(len(correlations))\n",
    "    ax.bar(x, [correlation for channel, correlation in correlations], bar_width)\n",
    "    ax.set_xlabel('Channel')\n",
    "    ax.set_ylabel('Correlation')\n",
    "    ax.set_title('Correlation between Head Size and SCI, Correlation with Good Recordings: ' + str(correlation) + ', N = ' + str(len(percentage_good_windows_sci_df_with_head_size)))\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([channel for channel, correlation in correlations])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/signal quality/Correlation between Head Size and SCI.png', dpi=dpi / 3)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average SCI per Channel across Participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_across_participant_sci_plots:\n",
    "    # get a list of participants in percentage_good_windows_sci_df where Good Recording is True\n",
    "    good_participants = percentage_good_windows_df[percentage_good_windows_df['Good Recording'] == True].index\n",
    "\n",
    "    # make a dataframe of the average sci for each channel\n",
    "    avg_sci_df = sci_df.groupby('Participant').mean().drop(columns=['Window', 'Start_Time', 'End_Time'])\n",
    "\n",
    "    # drop the participants that are not in good_participants\n",
    "    avg_sci_df_good = avg_sci_df.loc[good_participants]\n",
    "\n",
    "    # drop the participants that are in good_participants\n",
    "    avg_sci_df_bad = avg_sci_df.drop(index=good_participants)\n",
    "\n",
    "    # make a list of the dataframes\n",
    "    avg_sci_dfs = [avg_sci_df, avg_sci_df_good, avg_sci_df_bad]\n",
    "    df_names = ['All Participants', 'Good Participants', 'Bad Participants']\n",
    "    color_list = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'pink', 'gray']\n",
    "\n",
    "    for df, df_name in zip(avg_sci_dfs, df_names):\n",
    "\n",
    "        # make a violin plot of the average sci for each channel\n",
    "        fig, ax = plt.subplots(figsize=(35, 6))\n",
    "        parts = ax.violinplot(df, showmeans=False, widths=1, showextrema=False)\n",
    "\n",
    "        # match the violin plot colors to the columns in avg_sci_df to the channels in ch_mapping_names\n",
    "        color_i = 0\n",
    "        colors = []\n",
    "        region_labels = []\n",
    "\n",
    "        # for each region in ch_mapping_names, apply the color to the channels in that region\n",
    "        for region, channels in mappings['ch_mapping_names'].items():\n",
    "            for channel in channels:\n",
    "                if channel in df.columns:\n",
    "                    colors.append(color_list[color_i])\n",
    "                    region_labels.append(region)\n",
    "            color_i += 1\n",
    "\n",
    "        # set the colors of the violins\n",
    "        for i, pc in enumerate(parts['bodies']):\n",
    "            pc.set_facecolor(colors[i])\n",
    "            pc.set_edgecolor('black')\n",
    "            pc.set_alpha(1)\n",
    "\n",
    "        # create a legend\n",
    "        handles = [plt.Rectangle((0, 0), 1, 1, color=color) for color in list(dict.fromkeys(colors))]\n",
    "        ax.legend(handles, list(dict.fromkeys(region_labels)), loc='lower left')\n",
    "\n",
    "        # add a white scatter plot of the mean sci for each channel\n",
    "        ax.scatter(np.arange(1, len(df.columns) + 1), df.mean(), color='white', zorder=3)\n",
    "\n",
    "        ax.set_xlabel('Channel')\n",
    "        ax.set_ylabel('Average SCI')\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.axhline(good_threshold, color='green', linestyle='--')\n",
    "        ax.set_title(f'Average SCI per Channel: ({df_name}), N = {len(df)}')\n",
    "        ax.set_xticks(np.arange(1, len(df.columns) + 1))\n",
    "        ax.set_xticklabels(df.columns)\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'plots/signal quality/Average SCI (Windowed) per Channel/{df_name}.png', dpi=dpi / 3)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCI of Windows per Channel for each Participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_participant_sci_plots:\n",
    "    for i in sci_df['Participant'].unique():\n",
    "        df = sci_df[sci_df['Participant'] == i]\n",
    "\n",
    "        # get the good recording status\n",
    "        good_recording = False\n",
    "        if percentage_good_windows_df['Good Recording'][i]:\n",
    "            good_recording = True\n",
    "\n",
    "        # make a violin plot of the sci for each channel\n",
    "        fig, ax = plt.subplots(figsize=(35, 6))\n",
    "        parts = ax.violinplot(df[df.columns[4:]], showmeans=False, widths=1, showextrema=False)\n",
    "\n",
    "        # match the violin plot colors to the columns in sci_df to the channels in ch_mapping_names\n",
    "        color_i = 0\n",
    "        colors = []\n",
    "        region_labels = []\n",
    "\n",
    "        # for each region in ch_mapping_names, apply the color to the channels in that region\n",
    "        for region, channels in mappings['ch_mapping_names'].items():\n",
    "            for channel in channels:\n",
    "                if channel in df.columns:\n",
    "                    colors.append(color_list[color_i])\n",
    "                    region_labels.append(region)\n",
    "            color_i += 1\n",
    "\n",
    "        # set the colors of the violins\n",
    "        for j, pc in enumerate(parts['bodies']):\n",
    "            pc.set_facecolor(colors[j])\n",
    "            pc.set_edgecolor('black')\n",
    "            pc.set_alpha(1)\n",
    "\n",
    "        # create a legend\n",
    "        handles = [plt.Rectangle((0, 0), 1, 1, color=color) for color in list(dict.fromkeys(colors))]\n",
    "        ax.legend(handles, list(dict.fromkeys(region_labels)), loc='lower left')\n",
    "\n",
    "        # add a white scatter plot of the mean sci for each channel\n",
    "        ax.scatter(np.arange(1, len(df.columns) - 3), df[df.columns[4:]].mean(), color='white', zorder=3)\n",
    "\n",
    "        ax.set_xlabel('Channel')\n",
    "        ax.set_ylabel('SCI')\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.axhline(good_threshold, color='green', linestyle='--')\n",
    "        ax.set_title(f'SCI per Channel: Participant {i}, Windows = {len(df)}, Good Recording = {good_recording}')\n",
    "        ax.set_xticks(np.arange(1, len(df.columns) - 3))\n",
    "        ax.set_xticklabels(df.columns[4:])\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'plots/signal quality/Average SCI (Windowed) per Channel/individual/Participant {i}.png', dpi=dpi / 3)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Epoch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = ['face_type', 'emotion']\n",
    "conditions_list = {\n",
    "    'face_type': ['Real', 'Virt'],\n",
    "    'emotion': ['Joy', 'Fear', 'Anger', 'Disgust', 'Sadness', 'Neutral', 'Surprise']\n",
    "}\n",
    "channel_types = ['hbo', 'hbr', 'hbt']\n",
    "\n",
    "if get_epochs:\n",
    "    raw_haemo_good_recordings = load_data('raw_haemos', 'good') if len(raw_haemo_good_recordings) == 0 else raw_haemo_good_recordings\n",
    "    for mode in modes:\n",
    "        for condition in conditions_list[mode]:\n",
    "            participants = []\n",
    "            for raw_haemo in raw_haemo_good_recordings:\n",
    "                ind_epochs = relabel_annotations(raw_haemo.copy(), mode=mode)[condition]\n",
    "                channels = []\n",
    "                for channel_type in channel_types:\n",
    "                    epoch_channel = pick_channels(ind_epochs, channel_type)\n",
    "                    channels.append(epoch_channel.get_data())\n",
    "\n",
    "                participants.append(np.array(channels))\n",
    "            participants = np.array(participants)\n",
    "            participants = np.moveaxis(participants, 1, 2)\n",
    "            np.save(f'processed_data/epochs/{mode}_{condition}.npy', participants)\n",
    "\n",
    "# We now have epochs[mode][condition].shape = (n_participants, n_epochs, n_channel_types, n_channels, n_times)\n",
    "epochs = {\n",
    "    mode: {\n",
    "        condition: np.load(f'processed_data/epochs/{mode}_{condition}.npy') \n",
    "        if os.path.exists(f'processed_data/epochs/{mode}_{condition}.npy') \n",
    "        else None\n",
    "        for condition in conditions_list[mode]\n",
    "    }\n",
    "    for mode in modes\n",
    "}\n",
    "\n",
    "dict_channel_types = {\n",
    "    channel_type: idx for idx, channel_type in enumerate(channel_types)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Type/Emotion Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = ['face_type_emotion']\n",
    "conditions_list = {\n",
    "    'face_type': ['Real', 'Virt'],\n",
    "    'emotion': ['Joy', 'Fear', 'Anger', 'Disgust', 'Sadness', 'Neutral', 'Surprise'],\n",
    "}\n",
    "conditions_list['face_type_emotion'] = [''.join(pair) for pair in itertools.product(conditions_list['face_type'], conditions_list['emotion'])]\n",
    "channel_types = ['hbo', 'hbr', 'hbt']\n",
    "\n",
    "if get_face_type_emotion_epochs:\n",
    "    raw_haemo_good_recordings = load_data('raw_haemos', 'good') if len(raw_haemo_good_recordings) == 0 else raw_haemo_good_recordings\n",
    "    for mode in modes:\n",
    "        for condition in conditions_list[mode]:\n",
    "            all_condition_list = []\n",
    "            for i, raw_haemo in enumerate(raw_haemo_good_recordings, 1):\n",
    "                ind_epochs = relabel_annotations(raw_haemo.copy(), mode=mode)\n",
    "\n",
    "                if condition not in list(set(ind_epochs.annotations.description)):\n",
    "                    print(f'Participant {i} missing {condition}')\n",
    "                    continue\n",
    "\n",
    "                ind_epochs = ind_epochs[condition]\n",
    "\n",
    "                channels = []\n",
    "                for channel_type in channel_types:\n",
    "                    epoch_channel = pick_channels(ind_epochs, channel_type)\n",
    "                    channels.append(epoch_channel.get_data())\n",
    "\n",
    "                all_condition_list.append(np.array(channels))\n",
    "                    \n",
    "            all_conds = None\n",
    "            for cond in all_condition_list:\n",
    "                if all_conds is None:\n",
    "                    all_conds = cond\n",
    "                else:\n",
    "                    all_conds = np.concatenate((all_conds, cond), axis=1)\n",
    "            all_conds = np.moveaxis(all_conds, 0, 1)\n",
    "            np.save(f'processed_data/epochs/{mode}_{condition}.npy', all_conds)\n",
    "\n",
    "# We now have face_emotion_epochs[mode][condition].shape = (n_epochs, n_channel_types, n_channels, n_times)\n",
    "face_emotion_epochs = {\n",
    "    mode: {\n",
    "        condition: np.load(f'processed_data/epochs/{mode}_{condition}.npy') \n",
    "        if os.path.exists(f'processed_data/epochs/{mode}_{condition}.npy') \n",
    "        else None\n",
    "        for condition in conditions_list[mode]\n",
    "    }\n",
    "    for mode in modes\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLM analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = ['all', 'face_type', 'emotion', 'neutral_vs_emotion']\n",
    "conditions_list = {\n",
    "    'all': ['Blck', 'Base'],\n",
    "    'face_type': ['Real', 'Virt', 'Base'],\n",
    "    'emotion': ['Joy', 'Fear', 'Anger', 'Disgust', 'Sadness', 'Neutral', 'Surprise'],\n",
    "    'neutral_vs_emotion': ['Emotion', 'Neutral', 'Base']\n",
    "}\n",
    "channel_types = ['hbo', 'hbr', 'hbt']\n",
    "\n",
    "if get_glm_analysis:\n",
    "    raw_haemo_good_recordings = load_data('raw_haemos', 'good') if len(raw_haemo_good_recordings) == 0 else raw_haemo_good_recordings\n",
    "    for mode in modes:\n",
    "        cha_df = pd.DataFrame()\n",
    "        roi_df = pd.DataFrame()\n",
    "        con_df = pd.DataFrame()\n",
    "        for i, raw_haemo in enumerate(raw_haemo_good_recordings, 1):\n",
    "            raw_haemo_annots = pick_channels(raw_haemo, channel_types)\n",
    "            relabel_annotations(raw_haemo_annots, mode=mode)\n",
    "\n",
    "            # Create a design matrix\n",
    "            design_matrix = make_first_level_design_matrix(\n",
    "                raw_haemo_annots,\n",
    "                drift_model=\"cosine\",\n",
    "                high_pass=0.03125,  # The cutoff period (1/high_pass) should be set as the longest period between two trials of the same condition multiplied by 2\n",
    "                hrf_model=\"spm\",\n",
    "                stim_dur=16.0,\n",
    "            )\n",
    "            \n",
    "            # Run GLM\n",
    "            glm_est = run_glm(raw_haemo_annots, design_matrix, n_jobs=n_jobs)\n",
    "\n",
    "            cha = glm_est.to_dataframe()\n",
    "\n",
    "            # in ch_mapping_all, for each list of channels in the dict, each string is formatted as 'S{number}_D{number} {hbo/hbr}', extract the number from the string and replace the string with [number, number]\n",
    "            groups = {region: [[int(re.findall(r'\\d+', channel)[0]), int(re.findall(r'\\d+', channel)[1])] for channel in mappings['ch_mapping_all'][region]] for region in mappings['ch_mapping_all']}\n",
    "            # apply picks_pair_to_idx to each region in groups\n",
    "            for region in groups:\n",
    "                groups[region] = picks_pair_to_idx(raw_haemo_annots, groups[region], on_missing='ignore')\n",
    "\n",
    "            # Compute region of interest results from channel data\n",
    "            roi = glm_est.to_dataframe_region_of_interest(\n",
    "                groups, design_matrix.columns, demographic_info=True\n",
    "            )\n",
    "\n",
    "            # Define contrasts\n",
    "            contrast_matrix = np.eye(design_matrix.shape[1])\n",
    "            basic_conts = dict(\n",
    "                [(column, contrast_matrix[j]) for j, column in enumerate(design_matrix.columns)]\n",
    "            )\n",
    "            contrasts = []\n",
    "            unique_annots = np.unique(raw_haemo_annots.annotations.description).tolist()\n",
    "            pairs = list(itertools.combinations(unique_annots, 2))\n",
    "            # include the opposite of each pair\n",
    "            pairs = pairs + [(pair[1], pair[0]) for pair in pairs]\n",
    "\n",
    "            # Compute defined contrast pairs\n",
    "            for pair in pairs:\n",
    "                con = glm_est.compute_contrast(basic_conts[pair[0]] - basic_conts[pair[1]]).to_dataframe()\n",
    "                con[\"contrast_pair\"] = f\"{pair[0]} - {pair[1]}\"\n",
    "                contrasts.append(con)\n",
    "\n",
    "            # Add the participant ID to the dataframes\n",
    "            roi[\"Participant\"] = cha[\"Participant\"] = i\n",
    "            for con in contrasts:\n",
    "                con[\"Participant\"] = i\n",
    "\n",
    "            # Convert to uM for nicer plotting below.\n",
    "            cha[\"theta\"] = [t * 1.0e6 for t in cha[\"theta\"]]\n",
    "            roi[\"theta\"] = [t * 1.0e6 for t in roi[\"theta\"]]\n",
    "            for con in contrasts:\n",
    "                con[\"effect\"] = [t * 1.0e6 for t in con[\"effect\"]]\n",
    "\n",
    "            # Append the dataframes to the main dataframes\n",
    "            cha_df = pd.concat([cha_df, cha])\n",
    "            roi_df = pd.concat([roi_df, roi])\n",
    "            for con in contrasts:\n",
    "                con_df = pd.concat([con_df, con])\n",
    "\n",
    "        # Apply FDR correction to all dataframes\n",
    "        for df in [cha_df, roi_df, con_df]:\n",
    "            if 'p_value' in df.columns:\n",
    "                _, df['p_value_fdr'] = fdrcorrection(df['p_value'])\n",
    "\n",
    "        cha_df.to_csv('processed_data/glm/cha/cha_df_' + mode + '.csv', index=False)\n",
    "        roi_df.to_csv('processed_data/glm/roi/roi_df_' + mode + '.csv', index=False)\n",
    "        con_df.to_csv('processed_data/glm/cons/con_df_' + mode + '.csv', index=False)\n",
    "\n",
    "# load the dataframes\n",
    "glm = {\n",
    "    mode: {\n",
    "        'cha': pd.read_csv(f'processed_data/glm/cha/cha_df_{mode}.csv'),\n",
    "        'roi': pd.read_csv(f'processed_data/glm/roi/roi_df_{mode}.csv'),\n",
    "        'con': pd.read_csv(f'processed_data/glm/cons/con_df_{mode}.csv')\n",
    "    }\n",
    "    for mode in modes\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual GLM Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_ind_glm_plots:\n",
    "    channel_type = ['hbt']\n",
    "    for mode in modes:\n",
    "        grp_results = glm[mode]['roi'].query(f\"Condition in {conditions_list[mode]}\")\n",
    "        grp_results = grp_results.query(f\"Chroma in {channel_types}\")\n",
    "\n",
    "        theta_min = grp_results['theta'].min()\n",
    "        theta_max = grp_results['theta'].max()\n",
    "\n",
    "        # clear any files in the plots/glm/individual folder\n",
    "        for f in os.listdir('plots/glm/individual_' + mode):\n",
    "            os.remove(os.path.join('plots/glm/individual_' + mode, f))\n",
    "\n",
    "        for i in grp_results['Participant'].unique():\n",
    "            fig, ax = plt.subplots(figsize=(12, 6))\n",
    "            fig.suptitle(f'GLM Results for Participant {i}')\n",
    "\n",
    "            sns.swarmplot(data=grp_results.query(f\"Participant == {i} and Chroma == '{channel_type[0]}'\"),\n",
    "                  x='Condition', y='theta', hue='ROI', ax=ax, dodge=False)\n",
    "            ax.set_title(f'{channel_type[0]}')\n",
    "            ax.set_ylabel('Theta (uM)')\n",
    "            ax.set_ylim(theta_min, theta_max)\n",
    "            ax.set_xlabel('Condition')\n",
    "            ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "            plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "            plt.savefig(f'plots/glm/individual_{mode}/Participant {i}.png', dpi=dpi / 4)\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group GLM Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_group_glm_plots:\n",
    "    channel_type = ['hbt']\n",
    "    for mode in modes:\n",
    "        grp_results = glm[mode]['roi'].query(f\"Condition in {conditions_list[mode]}\")\n",
    "        grp_results = grp_results.query(f\"Chroma in {channel_type}\")\n",
    "\n",
    "        # Run a GLM model\n",
    "        roi_model = mixedlm(\"theta ~ -1 + ROI:Condition:Chroma\", grp_results, groups=grp_results[\"Participant\"]).fit(method=\"nm\")\n",
    "\n",
    "        # Get the results of the model and put it in a csv file\n",
    "        roi_model_results = statsmodels_to_results(roi_model)\n",
    "\n",
    "        # Apply FDR correction to the p-values in roi_model_results\n",
    "        _, roi_model_results['P>|z|_fdr'] = fdrcorrection(roi_model_results['P>|z|'])\n",
    "\n",
    "        # plot the results of the model for 'hbt' channel type\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        fig.suptitle(f'GLM Results for Group - {mode}')\n",
    "\n",
    "        sns.swarmplot(data=roi_model_results.query(f\"Chroma == '{channel_type[0]}'\"),\n",
    "              x='Condition', y='Coef.', hue='ROI', ax=ax, dodge=False)\n",
    "        ax.set_title(f'{channel_type[0]}')\n",
    "        ax.set_ylabel('Theta (uM)')\n",
    "        ax.set_xlabel('Condition')\n",
    "        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        plt.savefig(f'plots/glm/group_results/results_{mode}.png', dpi=dpi / 4)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Contrasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_group_contrast_plots:\n",
    "\n",
    "    # pick channel type\n",
    "    channel_type = 'hbt'\n",
    "\n",
    "    for mode in modes:\n",
    "        pairs = list(itertools.combinations(conditions_list[mode], 2))\n",
    "        for pair in pairs:\n",
    "            con_summary = glm[mode]['con'].query(f\"contrast_pair == '{pair[0]} - {pair[1]}'\")\n",
    "\n",
    "            if len(con_summary) == 0:\n",
    "                print(f\"No data for contrast {pair[0]} - {pair[1]}\")\n",
    "                continue\n",
    "\n",
    "            raw_haemos = load_data('raw_haemos') if len(raw_haemos) == 0 else raw_haemos\n",
    "            raw_haemo = pick_channels(raw_haemos[0].copy(), 'hbo')\n",
    "            raw_haemo_chs = pick_channels(raw_haemos[0].copy(), channel_type).ch_names\n",
    "\n",
    "            con_summary_channel = con_summary.query(f\"Chroma in {[channel_type]}\")\n",
    "\n",
    "            # Run group level model and convert to dataframe\n",
    "            con_model = mixedlm(\"effect ~ -1 + ch_name:Chroma\", con_summary_channel, groups=con_summary_channel[\"Participant\"]).fit(method=\"nm\")\n",
    "\n",
    "            # Get the results of the model\n",
    "            con_model_df = statsmodels_to_results(con_model, raw_haemo_chs)\n",
    "\n",
    "            \n",
    "            # Apply FDR correction to the group-level p-values from the model\n",
    "            _, con_model_df['p_value_fdr'] = fdrcorrection(con_model_df['P>|z|'])\n",
    "\n",
    "            # Plot the topographic map\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(10, 6))\n",
    "            fig.suptitle(f'Contrast: {pair[0]} > {pair[1]}, {channel_type}')\n",
    "\n",
    "            # set the \"Coef.\" value to near 0 for channels that are not significant\n",
    "            coef = con_model_df.copy()\n",
    "            coef.loc[coef['p_value_fdr'] > 0.05, 'Coef.'] /= 15\n",
    "\n",
    "            norm = mpl.colors.Normalize(vmin=-3, vmax=3)\n",
    "\n",
    "            # plot the topomap\n",
    "            mne.viz.plot_topomap(coef[\"Coef.\"].to_numpy(), raw_haemo.info, extrapolate='head', show=False, axes=axes[0], sensors='ok', cnorm=norm)\n",
    "\n",
    "            # make a plot of a colorbar without using plt.colorbar\n",
    "            cb1 = mpl.colorbar.ColorbarBase(axes[1], cmap=mpl.cm.RdBu_r, norm=norm, orientation='vertical')\n",
    "            cb1.set_label('Coefficient Value', fontsize=10)\n",
    "            cb1.ax.set_aspect(3)\n",
    "\n",
    "            plt.tight_layout(pad=0, rect=[0, 0, 1, 0.95])\n",
    "            if \"Neutral\" in pair:\n",
    "                plt.savefig(f'plots/glm/contrasts/differences_neutral/Contrast_{pair[0]}-{pair[1]}.png', dpi=dpi / 2)\n",
    "            else:\n",
    "                plt.savefig(f'plots/glm/contrasts/differences/Contrast_{pair[0]}-{pair[1]}.png', dpi=dpi / 2)\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROI Timeseries Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_roi_timeseries_activity:\n",
    "    raw_haemo_good_recordings = load_data('raw_haemos', 'good') if len(raw_haemo_good_recordings) == 0 else raw_haemo_good_recordings\n",
    "\n",
    "    channel_types = ['hbo', 'hbr', 'hbt']\n",
    "    \n",
    "    # create an empty dataframe\n",
    "    roi_timeseries_activity = pd.DataFrame()\n",
    "\n",
    "    tmin = 4\n",
    "    tmax = 16\n",
    "\n",
    "    i = 1\n",
    "    for raw_haemo in raw_haemo_good_recordings:\n",
    "        face_epochs = relabel_annotations(raw_haemo.copy(), mode='face_type')\n",
    "        emotion_epochs = relabel_annotations(raw_haemo.copy(), mode='emotion')\n",
    "\n",
    "        # crop the epochs to tmin-tmax\n",
    "        face_epochs.crop(tmin=tmin, tmax=tmax)\n",
    "\n",
    "        # convert the epochs to a dataframe\n",
    "        face_epochs_df = face_epochs.to_data_frame()\n",
    "\n",
    "        # remove the baseline condition\n",
    "        face_epochs_df = face_epochs_df.where(face_epochs_df['condition'] != 'Base').dropna()\n",
    "\n",
    "        # average these columns: column_names[3:].tolist(), by the epoch column and condition column\n",
    "        face_epochs_df = face_epochs_df.groupby(['epoch', 'condition']).mean().reset_index()\n",
    "\n",
    "        # crop the epochs to tmix-tmax\n",
    "        emotion_epochs.crop(tmin=tmin, tmax=tmax)\n",
    "\n",
    "        # convert the epochs to a dataframe\n",
    "        emotion_epochs_df = emotion_epochs.to_data_frame()\n",
    "\n",
    "        # remove the baseline condition\n",
    "        emotion_epochs_df = emotion_epochs_df.where(emotion_epochs_df['condition'] != 'Base').dropna()\n",
    "\n",
    "        # average these columns: column_names[3:].tolist(), by the epoch column and condition column\n",
    "        emotion_epochs_df = emotion_epochs_df.groupby(['epoch', 'condition']).mean().reset_index()\n",
    "\n",
    "        # add the condition column from the emotion_epochs_df to the face_epochs_df and line it up with the epoch column\n",
    "        face_epochs_df['emotion'] = emotion_epochs_df['condition']\n",
    "\n",
    "        # put the emotion column in the third column\n",
    "        all_epochs_df = face_epochs_df[['epoch', 'condition', 'emotion'] + face_epochs_df.columns[2:-1].tolist()]\n",
    "\n",
    "        # rename the condition column to face type\n",
    "        all_epochs_df.rename(columns={'condition': 'face type'}, inplace=True)\n",
    "\n",
    "        # divide the epoch column by 2 and floor it and convert it to an integer\n",
    "        all_epochs_df['epoch'] = (all_epochs_df['epoch'] // 2).astype(int)\n",
    "\n",
    "        # remove the time column\n",
    "        all_epochs_df.drop(columns='time', inplace=True)\n",
    "\n",
    "        if 'hbo' in channel_types:\n",
    "            for region, channels in mappings['ch_mapping_hbo'].items():\n",
    "                # Ensure the channels exist in the dataframe to avoid errors\n",
    "                valid_channels = [channel for channel in channels if channel in all_epochs_df.columns]\n",
    "                if valid_channels:\n",
    "                    # Create a new column for the region's average\n",
    "                    all_epochs_df[region + ' Average Hbo'] = all_epochs_df[valid_channels].mean(axis=1)\n",
    "\n",
    "        if 'hbr' in channel_types:\n",
    "            for region, channels in mappings['ch_mapping_hbr'].items():\n",
    "                # Ensure the channels exist in the dataframe to avoid errors\n",
    "                valid_channels = [channel for channel in channels if channel in all_epochs_df.columns]\n",
    "                if valid_channels:\n",
    "                    # Create a new column for the region's average\n",
    "                    all_epochs_df[region + ' Average Hbr'] = all_epochs_df[valid_channels].mean(axis=1)\n",
    "\n",
    "        if 'hbt' in channel_types:\n",
    "            for region, channels in mappings['ch_mapping_hbt'].items():\n",
    "                # Ensure the channels exist in the dataframe to avoid errors\n",
    "                valid_channels = [channel for channel in channels if channel in all_epochs_df.columns]\n",
    "                if valid_channels:\n",
    "                    # Create a new column for the region's average\n",
    "                    all_epochs_df[region + ' Average Hbt'] = all_epochs_df[valid_channels].mean(axis=1)\n",
    "\n",
    "        # drop all the channel columns\n",
    "        all_epochs_df.drop(columns=mappings['all_channels'], inplace=True)\n",
    "\n",
    "        # add participant number column\n",
    "        all_epochs_df['Participant'] = i\n",
    "\n",
    "        # make the participant number the first column\n",
    "        all_epochs_df = all_epochs_df[['Participant'] + all_epochs_df.columns[:-1].tolist()]\n",
    "\n",
    "        # add measurement date column\n",
    "        #all_epochs_df['Measurement Date'] = raw_haemo.info['meas_date']\n",
    "\n",
    "        # add an empty column for repetition and put it after the emotion column\n",
    "        all_epochs_df.insert(4, 'Repetition', '')\n",
    "\n",
    "        conditions = defaultdict(int)\n",
    "        for index, row in all_epochs_df.iterrows():\n",
    "            # add the condition-emotion pair to the conditions dictionary and increment the count\n",
    "            conditions[f\"{row['face type']}-{row['emotion']}\"] += 1\n",
    "\n",
    "            # add the count to the repetition column\n",
    "            all_epochs_df.at[index, 'Repetition'] = conditions[f\"{row['face type']}-{row['emotion']}\"]\n",
    "\n",
    "        # put it after the repetition column\n",
    "        all_epochs_df.insert(5, 'Sex', '')\n",
    "\n",
    "        # add the sex column\n",
    "        all_epochs_df['Sex'] = get_info(raw_haemo)['gender']\n",
    "\n",
    "        # add the dataframe to the average_timeseries_activity datafram\n",
    "        roi_timeseries_activity = pd.concat([roi_timeseries_activity, all_epochs_df])\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    # reset the index\n",
    "    roi_timeseries_activity.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # name the index column to 'observation'\n",
    "    roi_timeseries_activity.index.name = 'Observation'\n",
    "\n",
    "    # replace any spaces in the column names with underscores\n",
    "    roi_timeseries_activity.columns = roi_timeseries_activity.columns.str.replace(' ', '_')\n",
    "\n",
    "    # capitalize the column names\n",
    "    roi_timeseries_activity.columns = roi_timeseries_activity.columns.str.capitalize()\n",
    "\n",
    "    mappings = {}\n",
    "    for col in roi_timeseries_activity.select_dtypes(include=['object']).columns:\n",
    "        # Create a mapping dictionary for the column\n",
    "        unique_values = roi_timeseries_activity[col].unique()\n",
    "        col_mapping = {val: idx for idx, val in enumerate(unique_values)}\n",
    "        mappings[col] = col_mapping\n",
    "        \n",
    "        # Replace the column values in the DataFrame with numeric values\n",
    "        roi_timeseries_activity[col] = roi_timeseries_activity[col].map(col_mapping)\n",
    "\n",
    "    # Save mappings to a JSON file\n",
    "    with open('processed_data/roi_timeseries_activity/mappings.json', 'w') as json_file:\n",
    "        json.dump(mappings, json_file, indent=4)\n",
    "\n",
    "    # Get the unique number of participants\n",
    "    num_participants = roi_timeseries_activity['Participant'].nunique()\n",
    "\n",
    "    # save the dataframe to a csv file\n",
    "    roi_timeseries_activity.to_csv(f'processed_data/roi_timeseries_activity/roi_timeseries_activity_sci{sci_threshold}_n{num_participants}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERP Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = ['face_type', 'emotion']\n",
    "conditions_list = {\n",
    "    'face_type': ['Real', 'Virt'],\n",
    "    'emotion': ['Joy', 'Fear', 'Anger', 'Disgust', 'Sadness', 'Neutral', 'Surprise']\n",
    "}\n",
    "channel_types = ['hbo', 'hbr', 'hbt']\n",
    "\n",
    "if get_erp_plots:\n",
    "    # make a color list for the different channel types\n",
    "    colors = {\n",
    "        'hbo': 'red',\n",
    "        'hbr': 'blue',\n",
    "        'hbt': 'purple'\n",
    "    }\n",
    "\n",
    "    # pick a channel type to plot the difference between regions\n",
    "    difference_channel_type = 'hbt'\n",
    "\n",
    "    for mode in modes:\n",
    "        for condition in conditions_list[mode]:\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "            for channel_type in channel_types:\n",
    "                # get epoch data, shape is now (n_participants, n_times)\n",
    "                epoch_data = np.mean(epochs[mode][condition][:, :, dict_channel_types[channel_type]], axis=(1, 2))\n",
    "                epoch_data_mean = np.mean(epoch_data, axis=0)\n",
    "                \n",
    "                # get the standard error of the mean\n",
    "                epoch_data_sem = np.std(epoch_data, axis=0) / np.sqrt(epoch_data.shape[0])\n",
    "\n",
    "                # plot the mean\n",
    "                ax.plot(epoch_data_mean, label=channel_type, color=colors[channel_type])\n",
    "\n",
    "                # plot the standard error of the mean as a shaded area\n",
    "                ax.fill_between(np.arange(epoch_data_mean.shape[0]), epoch_data_mean - epoch_data_sem, epoch_data_mean + epoch_data_sem, color=colors[channel_type], alpha=0.3)\n",
    "            \n",
    "            ax.set_xlabel('Time (s)')\n",
    "            ax.set_ylabel('uM')\n",
    "            ax.set_title(f'ERP Plot for {mode}: {condition}, N = {len(epochs[mode][condition])}, Shaded Area: SEM')\n",
    "            ax.legend()\n",
    "            ax.set_ylim(-3e-6, 3e-6) # set the ylims to -+3 * 10^-6\n",
    "            \n",
    "            # set the xticks to the time points, 0-16 seconds, instead of 99 time points\n",
    "            ax.set_xticks(np.arange(0, 99, 6))\n",
    "            ax.set_xticklabels(np.arange(0, 17, 1))\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'plots/erp/erp_conditions/{condition}.png', dpi=dpi / 2)\n",
    "            plt.close()\n",
    "\n",
    "        for cond1, cond2 in itertools.combinations(conditions_list[mode], 2):\n",
    "            # create a 8x1 subplot that shares the x-axis and i can iterate over\n",
    "            fig, axes = plt.subplots(8, 1, figsize=(12, 24), sharex=True)        \n",
    "\n",
    "            # in ch_mapping_all, for each list of channels in the dict, filter for the channels with difference_channel_type in the string\n",
    "            groups = {region: [channel for channel in channels if difference_channel_type in channel] for region, channels in mappings['ch_mapping_all'].items()}\n",
    "            group_i = 0\n",
    "            for region, ch_name in groups.items():\n",
    "                # Get the indices for the current region from group_boundaries\n",
    "                keys_list = list(groups.keys())\n",
    "                start_idx = mappings['group_boundaries'][keys_list.index(region)]\n",
    "                end_idx = mappings['group_boundaries'][keys_list.index(region) + 1] if keys_list.index(region) + 1 < len(mappings['group_boundaries']) else None\n",
    "\n",
    "                # get the data for the current region\n",
    "                epoch_data_region1 = np.mean(epochs[mode][cond1][:, :, dict_channel_types[difference_channel_type], start_idx:end_idx], axis=(0, 1))\n",
    "                epoch_data_region2 = np.mean(epochs[mode][cond2][:, :, dict_channel_types[difference_channel_type], start_idx:end_idx], axis=(0, 1))\n",
    "\n",
    "                # get the mean of the data\n",
    "                epoch_data_region1_mean = np.mean(epoch_data_region1, axis=0)\n",
    "                epoch_data_region2_mean = np.mean(epoch_data_region2, axis=0)\n",
    "\n",
    "                # get the standard error of the mean\n",
    "                epoch_data_region1_sem = np.std(epoch_data_region1, axis=0) / np.sqrt(epoch_data_region1.shape[0])\n",
    "                epoch_data_region2_sem = np.std(epoch_data_region2, axis=0) / np.sqrt(epoch_data_region2.shape[0])\n",
    "\n",
    "                # plot the mean\n",
    "                axes[group_i].plot(epoch_data_region1_mean, label=cond1, color='red')\n",
    "                axes[group_i].plot(epoch_data_region2_mean, label=cond2, color='blue')\n",
    "\n",
    "                # plot the standard error of the mean as a shaded area\n",
    "                axes[group_i].fill_between(np.arange(epoch_data_region1_mean.shape[0]), epoch_data_region1_mean - epoch_data_region1_sem, epoch_data_region1_mean + epoch_data_region1_sem, color='red', alpha=0.3)\n",
    "                axes[group_i].fill_between(np.arange(epoch_data_region2_mean.shape[0]), epoch_data_region2_mean - epoch_data_region2_sem, epoch_data_region2_mean + epoch_data_region2_sem, color='blue', alpha=0.3)\n",
    "\n",
    "                # set the title of the subplot to the region\n",
    "                if group_i == 0:\n",
    "                    axes[group_i].set_title(f'ERP Plot for {mode}: {cond1} - {cond2}, N = {len(epochs[mode][cond1])}, Shaded Area: SEM, {difference_channel_type}\\n{region}')\n",
    "                else:\n",
    "                    axes[group_i].set_title(region)\n",
    "                \n",
    "                # set the ylims to -+7 * 10^-6\n",
    "                axes[group_i].set_ylim(-7e-6, 7e-6)\n",
    "\n",
    "                # add a legend to the subplot\n",
    "                axes[group_i].legend()\n",
    "\n",
    "                group_i += 1\n",
    "\n",
    "            axes[-1].set_xlabel('Time (s)')\n",
    "            axes[-1].set_xticks(np.arange(0, 99, 6))\n",
    "            axes[-1].set_xticklabels(np.arange(0, 17, 1))\n",
    "            axes[-1].set_ylabel('uM')\n",
    "            plt.tight_layout()\n",
    "            if \"Neutral\" == cond1 or \"Neutral\" == cond2:\n",
    "                plt.savefig(f'plots/erp/erp_differences_neutral/{cond1}_{cond2}.png', dpi=dpi / 2)\n",
    "            else:\n",
    "                plt.savefig(f'plots/erp/erp_differences/{cond1}_{cond2}.png', dpi=dpi / 2)\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connectivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = ['all', 'face_type', 'emotion']\n",
    "conditions_list = {\n",
    "    'all': ['Blck', 'Base'],\n",
    "    'face_type': ['Real', 'Virt', 'Base'],\n",
    "    'emotion': ['Joy', 'Fear', 'Anger', 'Disgust', 'Sadness', 'Neutral', 'Surprise']\n",
    "}\n",
    "\n",
    "connectivity_method = [spectral_connectivity_time, spectral_connectivity_epochs]\n",
    "\n",
    "# pick the channels\n",
    "channel_types = ['hbo', 'hbr', 'hbt']\n",
    "\n",
    "# main channel type\n",
    "channel_type = 'hbt'\n",
    "\n",
    "# pick the connectivity method\n",
    "method = \"coh\"\n",
    "\n",
    "# pick the mode\n",
    "con_mode = \"cwt_morlet\"\n",
    "\n",
    "# pick the frequency range\n",
    "cwt_freqs = np.linspace(0.1, 0.5, 5)\n",
    "\n",
    "# pick the number of cycles\n",
    "cwt_n_cycles = 1\n",
    "\n",
    "# average the connectivity matrices across frequencies\n",
    "faverage = True\n",
    "\n",
    "if run_ind_connectivity:\n",
    "    raw_haemo_good_recordings = load_data('raw_haemos', 'good') if len(raw_haemo_good_recordings) == 0 else raw_haemo_good_recordings\n",
    "    for func in connectivity_method:\n",
    "        for mode in modes:\n",
    "            for condition in conditions_list[mode]:\n",
    "                # create an empty list to store the connectivity for each participant\n",
    "                participant_cons = []\n",
    "\n",
    "                # for each raw_haemo in raw_haemo_good_recordings, compute the connectivity\n",
    "                for i, raw_haemo in enumerate(raw_haemo_good_recordings, 1):\n",
    "\n",
    "                    # relabel the annotations and pick the condition\n",
    "                    ind_epochs = relabel_annotations(raw_haemo.copy(), mode=mode)[condition]\n",
    "\n",
    "                    # create an empty list to store each channel type's connectivity\n",
    "                    cons = []\n",
    "\n",
    "                    # for each channel type in channel_types\n",
    "                    for channel_type in channel_types:\n",
    "                        # pick the channels\n",
    "                        epochs_channel = pick_channels(ind_epochs, channel_type)\n",
    "                    \n",
    "                        # use func to compute the connectivity\n",
    "                        if func.__name__ == 'spectral_connectivity_epochs':\n",
    "                            con = func(\n",
    "                                epochs_channel,\n",
    "                                method=method,\n",
    "                                mode=con_mode,\n",
    "                                cwt_freqs=cwt_freqs,\n",
    "                                cwt_n_cycles=cwt_n_cycles,\n",
    "                                faverage=faverage,\n",
    "                                n_jobs=n_jobs,\n",
    "                                verbose=False\n",
    "                            )\n",
    "\n",
    "                            data = con.get_data()\n",
    "                            data = np.moveaxis(np.moveaxis(data, 1, 2), 0, 1)\n",
    "                        elif func.__name__ == 'spectral_connectivity_time':\n",
    "                            con = func(\n",
    "                                epochs_channel,\n",
    "                                method=method,\n",
    "                                mode=con_mode,\n",
    "                                freqs=cwt_freqs,\n",
    "                                n_cycles=cwt_n_cycles,\n",
    "                                faverage=faverage,\n",
    "                                n_jobs=n_jobs,\n",
    "                                verbose=False\n",
    "                            )\n",
    "\n",
    "                            data = con.get_data()\n",
    "                        else:\n",
    "                            raise ValueError('func must be spectral_connectivity_epochs or spectral_connectivity_time')\n",
    "\n",
    "                        # append the connectivity to the cons list\n",
    "                        cons.append(data)\n",
    "\n",
    "                    participant_cons.append(np.array(cons))\n",
    "                    \n",
    "                # save the connectivity to disk\n",
    "                np.save(f'processed_data\\\\{func.__name__}\\\\individual_cons\\\\{mode}_{condition}_con.npy', np.array(participant_cons))\n",
    "\n",
    "        # make a dictionary to store the connectivity parameters\n",
    "        ind_connectivity_params = {\n",
    "            \"func\": func.__name__,\n",
    "            \"channel_types\": channel_types,\n",
    "            \"method\": method,\n",
    "            \"con_mode\": con_mode,\n",
    "            \"cwt_freqs\": cwt_freqs.tolist(),\n",
    "            \"cwt_n_cycles\": cwt_n_cycles,\n",
    "            \"faverage\": faverage,\n",
    "            \"ch_names\": pick_channels(raw_haemo, channel_types).ch_names\n",
    "        }\n",
    "\n",
    "        # save the connectivity parameters to disk in preprocessed_data\\connectivity\n",
    "        with open(f\"processed_data\\\\{func.__name__}\\\\ind_connectivity_params.json\", \"w\") as f:\n",
    "            json.dump(ind_connectivity_params, f)\n",
    "\n",
    "# load the numpy files from disk so we have ind_con[func_name][mode][condition].shape = (n_participants, n_channel_types, n_epochs OR n_times, n_channels, n_freqs\n",
    "ind_con = {\n",
    "    func.__name__: {\n",
    "        mode: {\n",
    "            condition: np.load(f'processed_data\\\\{func.__name__}\\\\individual_cons\\\\{mode}_{condition}_con.npy')\n",
    "            for condition in conditions_list[mode]\n",
    "        }\n",
    "        for mode in modes\n",
    "    }\n",
    "    for func in connectivity_method\n",
    "}\n",
    "\n",
    "ind_connectivity_params = {\n",
    "    func.__name__: json.load(open(f\"processed_data\\\\{func.__name__}\\\\ind_connectivity_params.json\", \"r\"))\n",
    "    for func in connectivity_method\n",
    "}\n",
    "\n",
    "dict_channel_types = {\n",
    "    channel_type: idx for idx, channel_type in enumerate(channel_types)\n",
    "}\n",
    "\n",
    "participants = ind_con[connectivity_method[0].__name__][modes[0]][conditions_list[modes[0]][0]].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Condition Connectivity Heatmap/Chord Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = ['face_type', 'emotion']\n",
    "conditions_list = {\n",
    "    'face_type': ['Real', 'Virt', 'Base'],\n",
    "    'emotion': ['Joy', 'Fear', 'Anger', 'Disgust', 'Sadness', 'Neutral', 'Surprise']\n",
    "}\n",
    "\n",
    "if get_condition_con_plots:\n",
    "    # make a color scheme\n",
    "    colorscheme = dict(\n",
    "        facecolor='white',\n",
    "        textcolor='black',\n",
    "        colormap='hot',\n",
    "        facecolor2='black',\n",
    "        textcolor2='white',\n",
    "    )\n",
    "\n",
    "    # get the node angles\n",
    "    node_angles = circular_layout(\n",
    "        mappings['ch_names_original'], mappings['all_channels_names'], start_pos=90, group_boundaries=mappings['group_boundaries']\n",
    "    )\n",
    "\n",
    "    for func in connectivity_method:\n",
    "        for mode in modes:\n",
    "            for condition in conditions_list[mode]:\n",
    "                averaged_data = np.mean(ind_con[func.__name__][mode][condition][:, dict_channel_types[channel_type]], axis=(0, 1, 3))\n",
    "                \n",
    "                # Get the grid size\n",
    "                grid_size = int(np.sqrt(averaged_data.size))\n",
    "\n",
    "                # Reshape the data into a 2D grid\n",
    "                heatmap_data = averaged_data.reshape((grid_size, grid_size))\n",
    "\n",
    "                # Make the matrix symmetric\n",
    "                symmetric_data = heatmap_data + heatmap_data.T\n",
    "\n",
    "                # Set the diagonal to the highest value\n",
    "                np.fill_diagonal(symmetric_data, np.max(symmetric_data))\n",
    "\n",
    "                # Plot the heatmap\n",
    "                fig, ax = plt.subplots(figsize=(25, 25))\n",
    "                im = ax.imshow(symmetric_data, cmap='viridis')\n",
    "                ax.set_title(f'Connectivity Heatmap for {func.__name__}, Mode: {mode}, Condition: {condition}, Channel Type: {channel_type}')\n",
    "                ax.set_xlabel('Channel')\n",
    "                ax.set_ylabel('Channel')\n",
    "                ax.set_xticks(np.arange(grid_size))\n",
    "                ax.set_yticks(np.arange(grid_size))\n",
    "                ch_names = [ch_name for ch_name in ind_connectivity_params[func.__name__]['ch_names'] if channel_type in ch_name]\n",
    "                ax.set_xticklabels(ch_names)\n",
    "                ax.set_yticklabels(ch_names)\n",
    "                plt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "                cbar = fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "                cbar.set_label('Connectivity Value')\n",
    "                plt.savefig(f'plots/{func.__name__}/heatmaps/conditions/{mode}_{condition}_con.png', dpi=dpi / 4)\n",
    "                plt.close()\n",
    "\n",
    "                # find the min of the averaged_data, when dropping all the 0s\n",
    "                min_val = np.min(averaged_data[np.nonzero(averaged_data)])\n",
    "\n",
    "                # find the max of the averaged_data, when dropping all the 0s\n",
    "                max_val = np.max(averaged_data[np.nonzero(averaged_data)])\n",
    "\n",
    "                # Plot the connectivity circle\n",
    "                plot_connectivity_circle(\n",
    "                    heatmap_data,\n",
    "                    node_names=mappings['ch_names_original'],\n",
    "                    node_angles=node_angles,\n",
    "                    n_lines=10000,\n",
    "                    title=f'{func.__name__}, {condition}, {channel_type}',\n",
    "                    colorbar_size=1,\n",
    "                    fontsize_colorbar=12,\n",
    "                    facecolor=colorscheme['facecolor'],\n",
    "                    textcolor=colorscheme['textcolor'],\n",
    "                    colormap=colorscheme['colormap'],\n",
    "                    padding=3,\n",
    "                    vmin=min_val,\n",
    "                    vmax=max_val,\n",
    "                    fontsize_title=12,\n",
    "                    colorbar=True,\n",
    "                    show=False\n",
    "                )\n",
    "                plt.savefig(f'plots/{func.__name__}/chord_plots/conditions/{mode}_{condition}_con.png', dpi=dpi / 4)\n",
    "                plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Condition Connectivity Heatmaps Across Time GIFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_condition_con_gif_plots:\n",
    "    for mode in modes:\n",
    "        for condition in conditions_list[mode]:\n",
    "            # the shape of averaged_data is (n_times, n_channels)\n",
    "            averaged_data = ind_con['spectral_connectivity_epochs'][mode][condition][:, dict_channel_types[channel_type], :, :, 0].mean(axis=0)\n",
    "\n",
    "            # reshape the data to (n_times, sqrt(n_channels), sqrt(n_channels))\n",
    "            averaged_data = averaged_data.reshape(averaged_data.shape[0], int(np.sqrt(averaged_data.shape[1])), int(np.sqrt(averaged_data.shape[1])))\n",
    "\n",
    "            # make the matrix symmetric for the (sqrt(n_channels), sqrt(n_channels)) part\n",
    "            for matrix in averaged_data:\n",
    "                matrix += matrix.T\n",
    "                np.fill_diagonal(matrix, np.max(matrix))\n",
    "\n",
    "            # create a funcanimation object to animate the connectivity across time\n",
    "            fig, ax = plt.subplots(figsize=(12, 10))\n",
    "            im = ax.imshow(averaged_data[0], cmap='viridis', animated=True)\n",
    "            ax.set_title(f'{mode} {condition} {channel_type} - Frame 1/{averaged_data.shape[0]}')\n",
    "            ax.set_xlabel('Channel')\n",
    "            ax.set_ylabel('Channel')\n",
    "\n",
    "            plt.colorbar(im)\n",
    "            plt.tight_layout()\n",
    "\n",
    "            def update(frame):\n",
    "                im.set_array(averaged_data[frame])\n",
    "                ax.set_title(f'{mode} {condition} {channel_type} - Time Point: {frame + 1}/{averaged_data.shape[0]}')\n",
    "                return im,\n",
    "\n",
    "            ani = FuncAnimation(fig, update, frames=averaged_data.shape[0], blit=True)\n",
    "            ani.save(f'plots/spectral_connectivity_epochs/heatmaps/gifs/{mode}_{condition}.gif', writer='pillow', fps=8, dpi=dpi / 4)\n",
    "            plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Condition Connectivity Variance Across Participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_variance_con_plots:\n",
    "    for mode in modes:\n",
    "        vmax = None\n",
    "        for condition in conditions_list[mode]:\n",
    "            # the shape of averaged_data is (n_participants, n_channels)\n",
    "            averaged_data = ind_con['spectral_connectivity_time'][mode][condition][:, dict_channel_types[channel_type], :, :, 0].mean(axis=1)\n",
    "\n",
    "            # reshape the data to (n_participants, n_channels, n_channels)\n",
    "            averaged_data = averaged_data.reshape(averaged_data.shape[0], int(np.sqrt(averaged_data.shape[1])), int(np.sqrt(averaged_data.shape[1])))\n",
    "\n",
    "            # make the matrix symmetric for the (sqrt(n_channels), sqrt(n_channels)) part\n",
    "            for matrix in averaged_data:\n",
    "                matrix += matrix.T\n",
    "                np.fill_diagonal(matrix, np.max(matrix))\n",
    "\n",
    "            # create a matrix of the variance of each channel pair over all participants\n",
    "            variance_matrix = np.var(averaged_data, axis=0)\n",
    "\n",
    "            # Plot the heatmap\n",
    "            fig, ax = plt.subplots(figsize=(10, 10))\n",
    "            im = ax.imshow(variance_matrix, cmap='viridis')\n",
    "            ax.set_title(f'Variance Heatmap for Mode: {mode}, Condition: {condition}, Channel Type: {channel_type}')\n",
    "            ax.set_xlabel('Channel')\n",
    "            ax.set_ylabel('Channel')\n",
    "            \n",
    "            cbar = plt.colorbar(im, ax=ax)\n",
    "            cbar.set_label('Variance Value')\n",
    "            if vmax is None:\n",
    "                vmax = variance_matrix.max()\n",
    "            \n",
    "            im.set_clim(0, vmax)\n",
    "            plt.tight_layout()\n",
    "\n",
    "            plt.savefig(f'plots/spectral_connectivity_time/heatmaps/variance/{mode}_{condition}_con.png', dpi=dpi / 4)\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Level Connectivity t-tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "connectivity_method = [spectral_connectivity_time, spectral_connectivity_epochs]\n",
    "\n",
    "modes = ['face_type', 'emotion']\n",
    "conditions_list = {\n",
    "    'face_type': ['Real', 'Virt', 'Base'],\n",
    "    'emotion': ['Joy', 'Fear', 'Anger', 'Disgust', 'Sadness', 'Neutral', 'Surprise']\n",
    "}\n",
    "\n",
    "if run_group_level_t_tests:\n",
    "    for func in connectivity_method:\n",
    "        for mode in modes:\n",
    "            for condition1, condition2 in itertools.combinations(conditions_list[mode], 2):\n",
    "                # Get the data for the two conditions\n",
    "                data1 = np.mean(ind_con[func.__name__][mode][condition1][:, dict_channel_types[channel_type]], axis=(1, 3))\n",
    "                data2 = np.mean(ind_con[func.__name__][mode][condition2][:, dict_channel_types[channel_type]], axis=(1, 3))\n",
    "\n",
    "                # reshape the data to (num_participants, int(np.sqrt(num_channel_connections)), int(np.sqrt(num_channel_connections)))\n",
    "                data1 = data1.reshape(data1.shape[0], int(np.sqrt(data1.shape[1])), int(np.sqrt(data1.shape[1])))\n",
    "                data2 = data2.reshape(data2.shape[0], int(np.sqrt(data2.shape[1])), int(np.sqrt(data2.shape[1])))\n",
    "\n",
    "                # ensure the data shapes are the same\n",
    "                assert data1.shape == data2.shape\n",
    "\n",
    "                # ensure the data is not empty or contains NaNs\n",
    "                assert not np.isnan(data1).any()\n",
    "                assert not np.isnan(data2).any()\n",
    "\n",
    "                # apply fisher z transform to the data\n",
    "                data1 = np.arctanh(data1)\n",
    "                data2 = np.arctanh(data2)\n",
    "\n",
    "                # Initialize matrices to store t and p-values\n",
    "                t_vals = np.zeros(data1.shape[1:])\n",
    "                p_values = np.zeros(data1.shape[1:])\n",
    "\n",
    "                # Iterate over each pair of channels\n",
    "                for i in range(data1.shape[1]):\n",
    "                    for j in range(i): # because the matrix is lower triangular\n",
    "                        # Run the t-test\n",
    "                        t_val, p_val = stats.ttest_rel(data1[:, i, j], data2[:, i, j])\n",
    "\n",
    "                        # Store the results\n",
    "                        t_vals[i, j] = t_val\n",
    "                        p_values[i, j] = p_val\n",
    "\n",
    "                # Flatten the lower triangular valid p-values (non-nan) for FDR correction\n",
    "                valid_idx = ~np.isnan(p_values)\n",
    "                p_values_flat = p_values[valid_idx]\n",
    "\n",
    "                # Correct p-values for multiple comparisons\n",
    "                _, p_values_flat_fdr = fdrcorrection(p_values_flat)\n",
    "\n",
    "                # Reshape the corrected p-values back to matrix form\n",
    "                p_values_fdr = p_values_flat_fdr.reshape(p_values.shape)\n",
    "\n",
    "                # Save the p-values to disk\n",
    "                np.save(f'processed_data\\\\{func.__name__}\\\\paired_t_tests\\\\{mode}_{condition1}_{condition2}_p_values.npy', p_values_fdr)\n",
    "\n",
    "# Load the p-values from disk\n",
    "group_level_t_tests_p_values = {\n",
    "    func.__name__: {\n",
    "        mode: {\n",
    "            f\"{condition1}_{condition2}\": np.load(f'processed_data\\\\{func.__name__}\\\\paired_t_tests\\\\{mode}_{condition1}_{condition2}_p_values.npy')\n",
    "            for condition1, condition2 in itertools.combinations(conditions_list[mode], 2)\n",
    "        }\n",
    "        for mode in modes\n",
    "    }\n",
    "    for func in connectivity_method\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Level Connectivity t-test Chord Plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_26972\\3809924952.py:30: RuntimeWarning: divide by zero encountered in log10\n",
      "  p_values = -np.log10(p_values)\n"
     ]
    }
   ],
   "source": [
    "connectivity_method = [spectral_connectivity_time, spectral_connectivity_epochs]\n",
    "modes = ['face_type', 'emotion']\n",
    "conditions_list = {\n",
    "    'face_type': ['Real', 'Virt', 'Base'],\n",
    "    'emotion': ['Joy', 'Fear', 'Anger', 'Disgust', 'Sadness', 'Neutral', 'Surprise']\n",
    "}\n",
    "\n",
    "if get_group_level_t_tests_chord_plots:\n",
    "\n",
    "    # make a color scheme\n",
    "    colorscheme = dict(\n",
    "        facecolor='white',\n",
    "        textcolor='black',\n",
    "        colormap='hot',\n",
    "        facecolor2='black',\n",
    "        textcolor2='white',\n",
    "    )\n",
    "\n",
    "    # get the node angles\n",
    "    node_angles = circular_layout(\n",
    "        mappings['ch_names_original'], mappings['all_channels_names'], start_pos=90, group_boundaries=mappings['group_boundaries']\n",
    "    )\n",
    "    \n",
    "    for func in connectivity_method:\n",
    "        for mode in modes:\n",
    "            for condition1, condition2 in itertools.combinations(conditions_list[mode], 2):\n",
    "                p_values = group_level_t_tests_p_values[func.__name__][mode][f\"{condition1}_{condition2}\"]\n",
    "\n",
    "                # apply log(p) to the p-values\n",
    "                p_values = -np.log10(p_values)\n",
    "\n",
    "                # set any p-values that are < -log(0.05) to 0\n",
    "                p_values = np.where(p_values < -np.log10(0.05), 0, p_values)\n",
    "\n",
    "                # Set the diagonal to 0\n",
    "                np.fill_diagonal(p_values, 0)\n",
    "\n",
    "                plot_connectivity_circle(\n",
    "                    p_values,\n",
    "                    node_names=mappings['ch_names_original'],\n",
    "                    node_angles=node_angles,\n",
    "                    n_lines=10000,\n",
    "                    title=f'{func.__name__}, {condition1} - {condition2}, {channel_type}, -log(p)',\n",
    "                    colorbar_size=1,\n",
    "                    fontsize_colorbar=12,\n",
    "                    facecolor=colorscheme['facecolor'],\n",
    "                    textcolor=colorscheme['textcolor'],\n",
    "                    colormap=colorscheme['colormap'],\n",
    "                    padding=3,\n",
    "                    vmin=0,\n",
    "                    vmax=-np.log10(0.0005),\n",
    "                    fontsize_title=16,\n",
    "                    colorbar=True,\n",
    "                    show=False\n",
    "                )\n",
    "                plt.savefig(f'plots/{func.__name__}/chord_plots/group_level_t_tests/{mode}_{condition1}_{condition2}_p_values.png', dpi=dpi / 4)\n",
    "                plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Level Connectivity t-test ROI Chord Plots\n",
    "Remember to edit the plot_connectivity_circle function to include the diagonals: \\mne\\viz\\circle.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_26972\\3747612962.py:75: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  discrete_cmap = cm.get_cmap('Reds', np.nanmax(matrix) + 1)\n"
     ]
    }
   ],
   "source": [
    "modes = ['face_type', 'emotion']\n",
    "conditions_list = {\n",
    "    'face_type': ['Real', 'Virt', 'Base'],\n",
    "    'emotion': ['Joy', 'Fear', 'Anger', 'Disgust', 'Sadness', 'Neutral', 'Surprise']\n",
    "}\n",
    "\n",
    "if get_group_level_t_tests_roi_chord_plots:\n",
    "    # Get the names of the regions\n",
    "    regions = list(mappings['ch_mapping_names'].keys())\n",
    "\n",
    "    colorscheme = dict(\n",
    "        facecolor='white',\n",
    "        textcolor='black',\n",
    "        colormap='Reds',\n",
    "        facecolor2='black',\n",
    "        textcolor2='white',\n",
    "    )\n",
    "\n",
    "    # get the node angles\n",
    "    node_angles = circular_layout(\n",
    "        regions, regions, start_pos=90\n",
    "    )\n",
    "\n",
    "    # First, flatten the dictionary into a list of tuples (channel_name, region)\n",
    "    flat_list = [(channel, region) for region, channels in mappings['ch_mapping_names'].items() for channel in channels]\n",
    "\n",
    "    # Now, replace the channel name with its enumeration (starting from 0)\n",
    "    reshaped_list = [(i, region) for i, (_, region) in enumerate(flat_list)]\n",
    "\n",
    "    for func in connectivity_method:\n",
    "        for mode in modes:\n",
    "            for condition1, condition2 in itertools.combinations(conditions_list[mode], 2):\n",
    "                p_values_by_roi = {}\n",
    "\n",
    "                for i, row in enumerate(group_level_t_tests_p_values[func.__name__][mode][f'{condition1}_{condition2}']):\n",
    "                    region_i = reshaped_list[i][1]\n",
    "                    for j, element in enumerate(row):\n",
    "                        region_j = reshaped_list[j][1]\n",
    "\n",
    "                        # skip the zeros\n",
    "                        if element == 0:\n",
    "                            continue\n",
    "\n",
    "                        # Store the significant p-values for each ROI pair\n",
    "                        if f'{region_i}_{region_j}' not in p_values_by_roi:\n",
    "                            p_values_by_roi[f'{region_i}_{region_j}'] = [element]\n",
    "                        else:\n",
    "                            p_values_by_roi[f'{region_i}_{region_j}'].append(element)\n",
    "\n",
    "                for roi_pair, p_values in p_values_by_roi.items():\n",
    "                    # filter out the p-values that are > 0.05\n",
    "                    p_values_by_roi[roi_pair] = [p_value for p_value in p_values if p_value < 0.05]\n",
    "\n",
    "                    # Get the count of p-values for each ROI pair\n",
    "                    p_values_by_roi[roi_pair] = len(p_values_by_roi[roi_pair])\n",
    "\n",
    "                # Create a matrix filled with NaNs\n",
    "                matrix = np.full((len(regions), len(regions)), np.nan)\n",
    "\n",
    "                # Fill the matrix using the dictionary values\n",
    "                for key, value in p_values_by_roi.items():\n",
    "                    r1, r2 = key.split('_')\n",
    "                    i = regions.index(r1)\n",
    "                    j = regions.index(r2)\n",
    "                    # Fill the lower triangular part of the matrix\n",
    "                    if i > j:\n",
    "                        matrix[i, j] = value\n",
    "                    else:\n",
    "                        matrix[j, i] = value\n",
    "\n",
    "                # replace any 0s with NaNs\n",
    "                matrix[matrix == 0] = np.nan\n",
    "\n",
    "                # Set the number of discrete colors to the highest number of significant p-values\n",
    "                discrete_cmap = cm.get_cmap('Reds', np.nanmax(matrix) + 1)\n",
    "\n",
    "                # Plot the connectivity circle\n",
    "                plot_connectivity_circle(\n",
    "                    matrix,\n",
    "                    node_names=regions,\n",
    "                    node_angles=node_angles,\n",
    "                    n_lines=10000,\n",
    "                    title=f'Number of significant p_values for {func.__name__}\\n{mode}, {condition1} - {condition2}',\n",
    "                    colorbar_size=1,\n",
    "                    fontsize_colorbar=12,\n",
    "                    node_width=40,\n",
    "                    linewidth=4.5,\n",
    "                    facecolor=colorscheme['facecolor'],\n",
    "                    textcolor=colorscheme['textcolor'],\n",
    "                    colormap=discrete_cmap,\n",
    "                    padding=3,\n",
    "                    fontsize_title=12,\n",
    "                    fontsize_names=10,\n",
    "                    colorbar=True,\n",
    "                    show=False\n",
    "                )\n",
    "                plt.savefig(f'plots/{func.__name__}/chord_plots/group_level_t_tests_roi/{mode}_{condition1}_{condition2}.png', dpi=dpi / 4)\n",
    "                plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = ['all', 'face_type', 'emotion']\n",
    "\n",
    "conditions_list = {\n",
    "    'all': ['Blck', 'Base'],\n",
    "    'face_type': ['Real', 'Virt'],\n",
    "    'emotion': ['Joy', 'Fear', 'Anger', 'Disgust', 'Sadness', 'Neutral', 'Surprise']\n",
    "}\n",
    "\n",
    "channel_types = ['hbo', 'hbr', 'hbt']\n",
    "\n",
    "dict_channel_types = {\n",
    "    channel_type: idx for idx, channel_type in enumerate(channel_types)\n",
    "}\n",
    "\n",
    "channel_type = 'hbo'\n",
    "\n",
    "raw_haemo_good_recordings = load_data('raw_haemos', 'good') if len(raw_haemo_good_recordings) == 0 else raw_haemo_good_recordings\n",
    "\n",
    "func = spectral_connectivity_time\n",
    "\n",
    "# load the numpy files from disk so we have ind_con[func_name][mode][condition].shape = (n_participants, n_channel_types, n_epochs, n_channels)\n",
    "ind_con = {\n",
    "    mode: {\n",
    "        condition: np.load(f'processed_data\\\\{func.__name__}\\\\individual_cons\\\\{mode}_{condition}_con.npy')\n",
    "        for condition in conditions_list[mode]\n",
    "    }\n",
    "    for mode in modes\n",
    "}\n",
    "\n",
    "# Example hyperparameter grids for each model\n",
    "param_grids = {\n",
    "    'RandomForestClassifier': {\n",
    "        'randomforestclassifier__n_estimators': [100, 200, 300, 400, 500],\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        'logisticregression__C': [0.01, 0.1, 1, 10, 100]\n",
    "    },\n",
    "    'LGBMClassifier': {\n",
    "        'lgbmclassifier__n_estimators': [100, 200, 300, 400, 500],\n",
    "    },\n",
    "    'KNeighborsClassifier': {\n",
    "        'kneighborsclassifier__n_neighbors': [3, 5, 7, 9, 11],\n",
    "    },\n",
    "    'MLPClassifier': {\n",
    "        'mlpclassifier__hidden_layer_sizes': [(50, 50), (100, 50), (100, 100)],\n",
    "    },\n",
    "    'QuadraticDiscriminantAnalysis': {\n",
    "        'quadraticdiscriminantanalysis__reg_param': [0.0, 0.1, 0.5, 1.0],\n",
    "    },\n",
    "    'LinearDiscriminantAnalysis': {\n",
    "        'lineardiscriminantanalysis__shrinkage': [None, 'auto', 0.1, 0.5, 1.0]\n",
    "    },\n",
    "    'GaussianNB': {\n",
    "        'gaussiannb__var_smoothing': [1e-09, 1e-08, 1e-07, 1e-06, 1e-05]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Within Participants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traditional Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_traditional_raw_within_decoding:\n",
    "    models_to_run = [\n",
    "        RandomForestClassifier(random_state=42, n_jobs=n_jobs),\n",
    "        LogisticRegression(random_state=42, n_jobs=n_jobs),\n",
    "        LGBMClassifier(random_state=42, n_jobs=n_jobs),\n",
    "        KNeighborsClassifier(n_jobs=n_jobs),\n",
    "        MLPClassifier(random_state=42, verbose=False),\n",
    "        QuadraticDiscriminantAnalysis(),\n",
    "        LinearDiscriminantAnalysis(),\n",
    "        GaussianNB()\n",
    "    ]\n",
    "\n",
    "    for model in models_to_run:\n",
    "        model_score = {mode: None for mode in modes}\n",
    "        for mode in modes:\n",
    "            st_scores = []\n",
    "            for raw_haemo in raw_haemo_good_recordings:\n",
    "                # Relabel the annotations\n",
    "                ind_epochs = relabel_annotations(raw_haemo.copy(), mode=mode)[conditions_list[mode]]\n",
    "\n",
    "                # pick the channels\n",
    "                ind_epochs = pick_channels(ind_epochs, channel_type)\n",
    "\n",
    "                # Get the data and labels\n",
    "                X = ind_epochs.get_data(copy=False)\n",
    "                y = ind_epochs.events[:, 2]\n",
    "\n",
    "                # Reshape the data to (n_epochs, n_channels * n_times)\n",
    "                X = X.reshape(X.shape[0], -1)\n",
    "\n",
    "                # Standardize the data\n",
    "                X = StandardScaler().fit_transform(X)\n",
    "                \n",
    "                # Compute the cross-validated scores for the model\n",
    "                # Use StratifiedKFold with 5 splits\n",
    "                scores = 100 * cross_val_score(\n",
    "                    model, \n",
    "                    X,\n",
    "                    y, \n",
    "                    cv=5,\n",
    "                    n_jobs=n_jobs,\n",
    "                    scoring='accuracy',\n",
    "                    verbose=0\n",
    "                )\n",
    "\n",
    "                # Append the scores to the list\n",
    "                st_scores.append(np.mean(scores))\n",
    "\n",
    "            # Return a dictionary with each score, and the mean score\n",
    "            model_score[mode] = {\n",
    "                'participant_scores': st_scores,\n",
    "                'mean_score': np.mean(st_scores)\n",
    "            }\n",
    "\n",
    "        # Save the model scores to disk\n",
    "        with open(f'processed_data/models/raw_within_scores/{model.__class__.__name__}_scores.json', 'w') as f:\n",
    "            json.dump(model_score, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Connectivity Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traditional Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_traditional_con_within_decoding:\n",
    "    models_to_run = [\n",
    "        RandomForestClassifier(random_state=42, n_jobs=n_jobs),\n",
    "        LogisticRegression(random_state=42, n_jobs=n_jobs),\n",
    "        LGBMClassifier(random_state=42, n_jobs=n_jobs),\n",
    "        KNeighborsClassifier(n_jobs=n_jobs),\n",
    "        MLPClassifier(random_state=42, verbose=False),\n",
    "        QuadraticDiscriminantAnalysis(),\n",
    "        LinearDiscriminantAnalysis(),\n",
    "        GaussianNB()\n",
    "    ]\n",
    "\n",
    "    for model in models_to_run:\n",
    "        model_score = {mode: None for mode in modes}\n",
    "        for mode in modes:\n",
    "            st_scores = []\n",
    "            for i in range(ind_con[modes[0]][conditions_list[modes[0]][0]].shape[0]):\n",
    "                participant_conds = []\n",
    "                labels = []\n",
    "\n",
    "                for condition in conditions_list[mode]:\n",
    "                    x = ind_con[mode][condition][i, dict_channel_types[channel_type], :, :, 0]\n",
    "                    label = np.array([condition] * x.shape[0])\n",
    "                    participant_conds.append(x)\n",
    "                    labels.append(label)\n",
    "                \n",
    "                # stack the data by the first axis\n",
    "                X = np.vstack(participant_conds)\n",
    "                \n",
    "                # stack the labels by the first axis\n",
    "                y = np.hstack(labels)\n",
    "\n",
    "                # map the labels to integers\n",
    "                label_mapping = {label: idx for idx, label in enumerate(np.unique(y))}\n",
    "                y = np.array([label_mapping[label] for label in y])\n",
    "\n",
    "                # Standardize the data\n",
    "                X = StandardScaler().fit_transform(X)\n",
    "\n",
    "                # Compute the cross-validated scores for the model\n",
    "                # Use StratifiedKFold with 5 splits\n",
    "                scores = 100 * cross_val_score(\n",
    "                    model, \n",
    "                    X,\n",
    "                    y, \n",
    "                    cv=5,\n",
    "                    n_jobs=n_jobs,\n",
    "                    scoring='accuracy',\n",
    "                    verbose=0\n",
    "                )\n",
    "            \n",
    "                # Append the scores to the list\n",
    "                st_scores.append(np.mean(scores, axis=0))\n",
    "            \n",
    "            # Return a dictionary with each score, and the mean score\n",
    "            model_score[mode] = {\n",
    "                'participant_scores': st_scores,\n",
    "                'mean_score': np.mean(st_scores)\n",
    "            }\n",
    "\n",
    "        # save the model scores to disk\n",
    "        with open(f'processed_data/models/con_within_scores/{model.__class__.__name__}_scores.json', 'w') as f:\n",
    "            json.dump(model_score, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Across Participants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traditional Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_traditional_raw_across_decoding:\n",
    "    models_to_run = [\n",
    "        RandomForestClassifier(random_state=42, n_jobs=n_jobs),\n",
    "        LogisticRegression(random_state=42, n_jobs=n_jobs),\n",
    "        LGBMClassifier(random_state=42, n_jobs=n_jobs),\n",
    "        KNeighborsClassifier(n_jobs=n_jobs),\n",
    "        MLPClassifier(random_state=42, verbose=False),\n",
    "        QuadraticDiscriminantAnalysis(),\n",
    "        LinearDiscriminantAnalysis(),\n",
    "        GaussianNB()\n",
    "    ]\n",
    "\n",
    "    # Outer cross-validation: leave-one-group-out\n",
    "    logo = LeaveOneGroupOut()\n",
    "\n",
    "    for model in models_to_run:\n",
    "        model_score = {mode: {} for mode in modes}\n",
    "        model_name = model.__class__.__name__\n",
    "        param_grid = param_grids.get(model_name, {})\n",
    "\n",
    "        for mode in modes:\n",
    "            X_list, y_list, groups_list = [], [], []\n",
    "            for i, raw_haemo in enumerate(raw_haemo_good_recordings, 1):\n",
    "                # Relabel annotations and select the channel according to mode\n",
    "                ind_epochs = relabel_annotations(raw_haemo.copy(), mode=mode)[conditions_list[mode]]\n",
    "                ind_epochs = pick_channels(ind_epochs, channel_type)\n",
    "                # Get data (n_epochs x n_channels x n_times) and labels\n",
    "                X = ind_epochs.get_data(copy=False)\n",
    "                y = ind_epochs.events[:, 2]\n",
    "                X_list.append(X)\n",
    "                y_list.append(y)\n",
    "                # Create a group array indicating participant index for each epoch\n",
    "                groups_list.append(np.full(len(y), i))\n",
    "\n",
    "            # Concatenate data across participants and reshape to (n_epochs, n_channels * n_times)\n",
    "            X = np.concatenate(X_list, axis=0)\n",
    "            y = np.concatenate(y_list, axis=0)\n",
    "            groups = np.concatenate(groups_list, axis=0)\n",
    "            X = X.reshape(X.shape[0], -1)\n",
    "\n",
    "            outer_scores = []\n",
    "            outer_best_params = []\n",
    "\n",
    "            # Outer CV loop\n",
    "            for train_index, test_index in logo.split(X, y, groups):\n",
    "                X_train, X_test = X[train_index], X[test_index]\n",
    "                y_train, y_test = y[train_index], y[test_index]\n",
    "                groups_train = groups[train_index]  # Use groups for inner CV\n",
    "\n",
    "                # Pipeline: scale features then fit model\n",
    "                pipe = make_pipeline(StandardScaler(), model)\n",
    "\n",
    "                # Inner CV using GroupKFold (note: no shuffling available)\n",
    "                inner_cv = GroupKFold(n_splits=5)\n",
    "\n",
    "                # Hyperparameter search using GridSearchCV\n",
    "                rs = GridSearchCV(\n",
    "                    estimator=pipe,\n",
    "                    param_grid=param_grid,\n",
    "                    cv=inner_cv,\n",
    "                    scoring='accuracy',\n",
    "                    n_jobs=n_jobs,\n",
    "                    verbose=0\n",
    "                )\n",
    "                # IMPORTANT: pass groups for the inner CV\n",
    "                rs.fit(X_train, y_train, groups=groups_train)\n",
    "\n",
    "                # Evaluate best estimator on the outer test fold\n",
    "                test_score = 100 * rs.best_estimator_.score(X_test, y_test)\n",
    "                outer_scores.append(test_score)\n",
    "                outer_best_params.append(rs.best_params_)\n",
    "\n",
    "            # Save per-mode results\n",
    "            model_score[mode] = {\n",
    "                'logo_scores': outer_scores,\n",
    "                'mean_score': np.mean(outer_scores),\n",
    "                'fold_best_params': outer_best_params\n",
    "            }\n",
    "\n",
    "        # Write results for the current model to disk\n",
    "        with open(f'processed_data/models/raw_across_scores/{model_name}_scores.json', 'w') as f:\n",
    "            json.dump(model_score, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = ['face_type']\n",
    "raw_haemos = load_data('raw_haemos') if len(raw_haemos) == 0 else raw_haemos\n",
    "\n",
    "if run_dl_raw_across_decoding:\n",
    "\n",
    "    logo = LeaveOneGroupOut()\n",
    "\n",
    "    model_score = {mode: None for mode in modes}\n",
    "    for mode in modes:\n",
    "        X_list, y_list, groups_list = [], [], []\n",
    "        for i, raw_haemo in enumerate(raw_haemos, 1):\n",
    "            # Relabel the annotations\n",
    "            ind_epochs = relabel_annotations(raw_haemo.copy(), mode=mode)[conditions_list[mode]]\n",
    "            \n",
    "            # Select the channel(s)\n",
    "            ind_epochs = pick_channels(ind_epochs, channel_type)\n",
    "            \n",
    "            # Get the data and labels:\n",
    "            # X shape: (n_epochs, n_channels, n_times)\n",
    "            X = ind_epochs.get_data()\n",
    "            y = ind_epochs.events[:, 2]\n",
    "            \n",
    "            X_list.append(X)\n",
    "            y_list.append(y)\n",
    "            # Assign a group ID (participant index) for each epoch\n",
    "            groups_list.append(np.full(len(y), i))\n",
    "        \n",
    "        # Concatenate data across participants\n",
    "        X = np.concatenate(X_list, axis=0)\n",
    "        y = np.concatenate(y_list, axis=0)\n",
    "\n",
    "        # Map labels to integers\n",
    "        label_mapping = {label: idx for idx, label in enumerate(np.unique(y))}\n",
    "        y = np.array([label_mapping[label] for label in y])\n",
    "\n",
    "        # Create a group array identifying the participant index for each epoch\n",
    "        groups = np.concatenate(groups_list, axis=0)\n",
    "\n",
    "        # New shape: (n_epochs, n_times, n_channels)\n",
    "        X = np.transpose(X, (0, 2, 1))\n",
    "\n",
    "        # Flatten epochs and time for scaling, then reshape back.\n",
    "        X_reshaped = X.reshape(-1, X.shape[-1])\n",
    "        X_reshaped = StandardScaler().fit_transform(X_reshaped)\n",
    "\n",
    "        # Reshape back to 3D\n",
    "        X = X_reshaped.reshape(X.shape)\n",
    "\n",
    "        print(X.shape)\n",
    "\n",
    "        # Leave-One-Group-Out Cross Validation:\n",
    "        fold_scores = []\n",
    "        for train_idx, test_idx in logo.split(X, y, groups=groups):\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "            # Define a CNN model for the current fold\n",
    "            model = tf.keras.Sequential([\n",
    "                tf.keras.layers.Input(shape=(X_train.shape[1], X_train.shape[2])),\n",
    "                tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)),\n",
    "                tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)),\n",
    "                tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, dropout=0.2, recurrent_dropout=0.2)),\n",
    "                tf.keras.layers.Dense(128, activation='relu'),\n",
    "                tf.keras.layers.Dropout(0.5),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dropout(0.5),\n",
    "                tf.keras.layers.Dense(len(np.unique(y)), activation='softmax')\n",
    "            ])\n",
    "\n",
    "            model.compile(\n",
    "                optimizer='sgd',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "\n",
    "            # Train the model on the training fold\n",
    "            history = model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                epochs=100,\n",
    "                batch_size=32,\n",
    "                validation_split=0.2,\n",
    "                callbacks=[\n",
    "                    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
    "                    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5)\n",
    "                ],\n",
    "                verbose=1\n",
    "            )\n",
    "\n",
    "            # Evaluate on the test fold (the left-out group)\n",
    "            score = model.evaluate(X_test, y_test, verbose=0)\n",
    "            plot_history(history, mode, score[1])\n",
    "            break\n",
    "    #         fold_scores.append(100 * score[1])\n",
    "\n",
    "    #     # Return a dictionary with each score, and the mean score\n",
    "    #     model_score[mode] = {\n",
    "    #         'logo_scores': fold_scores,\n",
    "    #         'mean_score': np.mean(fold_scores)\n",
    "    #     }\n",
    "\n",
    "    # # Save the model scores to disk\n",
    "    # with open(f'processed_data/models/raw_across_scores/{model.__class__.__name__}_scores.json', 'w') as f:\n",
    "    #     json.dump(model_score, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Connectivity Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traditional Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_traditional_con_across_decoding:\n",
    "    models_to_run = [\n",
    "        RandomForestClassifier(random_state=42, n_jobs=n_jobs),\n",
    "        LogisticRegression(random_state=42, n_jobs=n_jobs),\n",
    "        LGBMClassifier(random_state=42, n_jobs=n_jobs),\n",
    "        KNeighborsClassifier(n_jobs=n_jobs),\n",
    "        MLPClassifier(random_state=42, verbose=False),\n",
    "        QuadraticDiscriminantAnalysis(),\n",
    "        LinearDiscriminantAnalysis(),\n",
    "        GaussianNB()\n",
    "    ]\n",
    "\n",
    "    # Outer CV: leave-one-group-out (e.g. leave-one-participant-out)\n",
    "    logo = LeaveOneGroupOut()\n",
    "\n",
    "    # Loop over models\n",
    "    for model in models_to_run:\n",
    "        model_score = {mode: {} for mode in modes}\n",
    "        model_name = model.__class__.__name__\n",
    "        # Get hyperparameter grid (if available) for this model\n",
    "        param_grid = param_grids.get(model_name, {})\n",
    "\n",
    "        # Loop over decoding modes\n",
    "        for mode in modes:\n",
    "            X_list, y_list, groups_list = [], [], []\n",
    "            # Build feature matrices from connectivity data per participant\n",
    "            for i in range(ind_con[modes[0]][conditions_list[modes[0]][0]].shape[0]):\n",
    "                participant_conds = []\n",
    "                labels = []\n",
    "                for condition in conditions_list[mode]:\n",
    "                    # Here we select the connectivity data for a specific channel type and condition.\n",
    "                    # The shape of x is assumed to be: n_epochs x ... (we take the first connectivity matrix)\n",
    "                    x = ind_con[mode][condition][i, dict_channel_types[channel_type], :, :, 0]\n",
    "                    # Create labels for each epoch corresponding to this condition\n",
    "                    label = np.array([condition] * x.shape[0])\n",
    "                    participant_conds.append(x)\n",
    "                    labels.append(label)\n",
    "                # Stack epochs for the current participant\n",
    "                X_part = np.vstack(participant_conds)\n",
    "                y_part = np.hstack(labels)\n",
    "                # Map string labels to integers consistently across participants\n",
    "                label_mapping = {label: idx for idx, label in enumerate(np.unique(y_part))}\n",
    "                y_part = np.array([label_mapping[label] for label in y_part])\n",
    "                X_list.append(X_part)\n",
    "                y_list.append(y_part)\n",
    "                groups_list.append(np.full(len(y_part), i))\n",
    "            \n",
    "            # Concatenate data across all participants\n",
    "            X = np.concatenate(X_list, axis=0)\n",
    "            y = np.concatenate(y_list, axis=0)\n",
    "            groups = np.concatenate(groups_list, axis=0)\n",
    "            \n",
    "            # Create a pipeline: scaling is performed inside each CV fold to avoid leakage\n",
    "            pipe = make_pipeline(StandardScaler(), model)\n",
    "\n",
    "            outer_scores = []\n",
    "            outer_best_params = []\n",
    "\n",
    "            # Outer CV loop: leave-one-group-out\n",
    "            for train_index, test_index in logo.split(X, y, groups):\n",
    "                X_train, X_test = X[train_index], X[test_index]\n",
    "                y_train, y_test = y[train_index], y[test_index]\n",
    "                groups_train = groups[train_index]\n",
    "\n",
    "                # Inner CV for hyperparameter tuning using GroupKFold\n",
    "                inner_cv = GroupKFold(n_splits=5)\n",
    "                rs = GridSearchCV(\n",
    "                    estimator=pipe,\n",
    "                    param_grid=param_grid,\n",
    "                    cv=inner_cv,\n",
    "                    scoring='accuracy',\n",
    "                    n_jobs=n_jobs,\n",
    "                    verbose=0\n",
    "                )\n",
    "                # Pass the groups for inner CV\n",
    "                rs.fit(X_train, y_train, groups=groups_train)\n",
    "                # Evaluate on the outer test fold\n",
    "                test_score = 100 * rs.best_estimator_.score(X_test, y_test)\n",
    "                outer_scores.append(test_score)\n",
    "                outer_best_params.append(rs.best_params_)\n",
    "\n",
    "            # Save results for the current mode\n",
    "            model_score[mode] = {\n",
    "                'logo_scores': outer_scores,\n",
    "                'mean_score': np.mean(outer_scores),\n",
    "                'fold_best_params': outer_best_params\n",
    "            }\n",
    "\n",
    "        # Save model scores to disk\n",
    "        with open(f'processed_data/models/con_across_scores/{model_name}_scores.json', 'w') as f:\n",
    "            json.dump(model_score, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = ['face_type']\n",
    "\n",
    "if run_dl_con_across_decoding:\n",
    "\n",
    "    logo = LeaveOneGroupOut()\n",
    "\n",
    "    model_score = {mode: None for mode in modes}\n",
    "    for mode in modes:\n",
    "        X_list, y_list, groups_list = [], [], []\n",
    "        for i in range(ind_con[modes[0]][conditions_list[modes[0]][0]].shape[0]):\n",
    "            participant_conds = []\n",
    "            labels = []\n",
    "            for condition in conditions_list[mode]:\n",
    "                x = ind_con[mode][condition][i, dict_channel_types[channel_type], :, :, 0]\n",
    "                label = np.array([condition] * x.shape[0])\n",
    "                participant_conds.append(x)\n",
    "                labels.append(label)\n",
    "\n",
    "            # Stack the epochs and labels for this participant\n",
    "            X_part = np.vstack(participant_conds)\n",
    "            y_part = np.hstack(labels)\n",
    "\n",
    "            # Standardize the data\n",
    "            X_part = StandardScaler().fit_transform(X_part)\n",
    "            \n",
    "            # Map the labels to integers consistently\n",
    "            label_mapping = {label: idx for idx, label in enumerate(np.unique(y_part))}\n",
    "            y_part = np.array([label_mapping[label] for label in y_part])\n",
    "            X_list.append(X_part)\n",
    "            y_list.append(y_part)\n",
    "            groups_list.append(np.full(len(y_part), i))\n",
    "        \n",
    "        # Concatenate data across all participants\n",
    "        X = np.concatenate(X_list, axis=0)\n",
    "        y = np.concatenate(y_list, axis=0)\n",
    "        groups = np.concatenate(groups_list, axis=0)\n",
    "        \n",
    "        # Reshape the data to (n_samples, sqrt(n_features), sqrt(n_features))\n",
    "        # This assumes that X.shape[1] is a perfect square.\n",
    "        side_length = int(np.sqrt(X.shape[1]))\n",
    "        X = X.reshape(X.shape[0], side_length, side_length)\n",
    "\n",
    "        # Perform Leave-One-Group-Out CV; for each fold, reinitialize and train a new CNN model.\n",
    "        fold_scores = []\n",
    "        for train_idx, test_idx in logo.split(X, y, groups=groups):\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "            # Define a CNN model for the current fold\n",
    "            model = tf.keras.Sequential([\n",
    "                tf.keras.layers.Input(shape=(X_train.shape[1], X_train.shape[2])),\n",
    "                tf.keras.layers.Flatten(),\n",
    "\n",
    "                tf.keras.layers.Dense(1024),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Activation('relu'),\n",
    "                tf.keras.layers.Dropout(0.5),\n",
    "\n",
    "                tf.keras.layers.Dense(512),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Activation('relu'),\n",
    "                tf.keras.layers.Dropout(0.5),\n",
    "\n",
    "                tf.keras.layers.Dense(128),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Activation('relu'),\n",
    "                tf.keras.layers.Dropout(0.5),\n",
    "\n",
    "                tf.keras.layers.Dense(64),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Activation('relu'),\n",
    "                tf.keras.layers.Dropout(0.5),\n",
    "\n",
    "                tf.keras.layers.Dense(len(np.unique(y)), activation='softmax')\n",
    "            ])\n",
    "\n",
    "            model.compile(\n",
    "                optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "\n",
    "            # Train the model using a validation split for early stopping.\n",
    "            history = model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                epochs=100,\n",
    "                batch_size=8,\n",
    "                validation_split=0.2,\n",
    "                callbacks=[\n",
    "                    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "                    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5)\n",
    "                ],\n",
    "                verbose=1\n",
    "            )\n",
    "\n",
    "            # Evaluate on the test fold\n",
    "            score = model.evaluate(X_test, y_test, verbose=0)\n",
    "            plot_history(history, mode, score[1])\n",
    "            break\n",
    "    #         fold_scores.append(100 * score[1])  # score[1] is the accuracy\n",
    "\n",
    "    #     # Compute the mean score for this mode\n",
    "    #     model_score[mode] = {\n",
    "    #         'logo_scores': fold_scores,\n",
    "    #         'mean_score': np.mean(fold_scores)\n",
    "    #     }\n",
    "\n",
    "    # # Save the model scores to disk\n",
    "    # with open(f'processed_data/models/con_across_scores/{model.__class__.__name__}_scores.json', 'w') as f:\n",
    "    #     json.dump(model_score, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding Analysis Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_types = ['raw_within_scores', 'con_within_scores', 'raw_across_scores', 'con_across_scores']\n",
    "\n",
    "if get_decoding_table_scores_plots:\n",
    "    for score_type in score_types:\n",
    "        # load data from 'processed_data/models' folder\n",
    "        model_scores = {}\n",
    "        # for any csv file in the folder\n",
    "        for file in os.listdir(f'processed_data/models/{score_type}'):\n",
    "            # if the file is a json file\n",
    "            if file.endswith('.json'):\n",
    "                # open the file\n",
    "                with open(f'processed_data/models/{score_type}/{file}', 'r') as f:\n",
    "                    # load the data from the file\n",
    "                    model_scores[file.split('_')[0]] = json.load(f)\n",
    "\n",
    "        # split score_type by '_'\n",
    "        score_type_split = score_type.split('_')\n",
    "        if score_type_split[0] == 'con':\n",
    "            score_type_split[0] = 'Connectivity'\n",
    "        elif score_type_split[0] == 'raw':\n",
    "            score_type_split[0] = 'Raw'\n",
    "\n",
    "        score_type_split[1] = score_type_split[1].capitalize()\n",
    "\n",
    "        # Create a DataFrame from the model scores\n",
    "        model_scores_df = pd.DataFrame([\n",
    "            {'Model': model_name, 'Mode': mode, 'Mean Score': score['mean_score']}\n",
    "            for model_name, scores in model_scores.items()\n",
    "            for mode, score in scores.items()\n",
    "        ])\n",
    "\n",
    "        # Pivot the DataFrame to make 'Model' the first column and each 'Mode' a separate column\n",
    "        model_scores_df = model_scores_df.pivot(index='Model', columns='Mode', values='Mean Score').reset_index()\n",
    "\n",
    "        # Fill NaN values with 0 or any other value if needed\n",
    "        model_scores_df.fillna(np.nan, inplace=True)\n",
    "\n",
    "        # Remove the Mode column name\n",
    "        model_scores_df.columns.name = None\n",
    "\n",
    "        # Reorder the columns to have 'Model', 'all', 'face_type', 'emotion'\n",
    "        model_scores_df = model_scores_df[['Model', 'all', 'face_type', 'emotion']]\n",
    "\n",
    "        # Round the scores to 2 decimal places\n",
    "        model_scores_df = model_scores_df.round(2)\n",
    "\n",
    "        # Create a matplotlib figure and add a table\n",
    "        fig, ax = plt.subplots(figsize=(6, len(model_scores_df) / 4))\n",
    "        ax.axis('tight')\n",
    "        ax.axis('off')\n",
    "        table = ax.table(cellText=model_scores_df.values, colLabels=model_scores_df.columns, loc='center')\n",
    "\n",
    "        # Set the biggest number in each column to be bold\n",
    "        for i, col in enumerate(model_scores_df.columns):\n",
    "            if col == 'Model':\n",
    "                continue\n",
    "            max_val = model_scores_df[col].max()\n",
    "            for j, val in enumerate(model_scores_df[col]):\n",
    "                if val == max_val:\n",
    "                    table[(j + 1, i)].set_text_props(weight='bold')\n",
    "\n",
    "        # Add a title to the plot\n",
    "        ax.set_title(f'{score_type_split[0]} {score_type_split[1]} Scores (Accuracy%)')\n",
    "\n",
    "        # Save the figure as a PNG file, bbox_inches='tight'\n",
    "        plt.savefig(f'plots/models/tables/{score_type}.png', dpi=dpi, bbox_inches='tight')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Participant/LOGO Scores Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_decoding_individual_scores_plots:\n",
    "    for score_type in score_types:\n",
    "        # load data from 'processed_data/models' folder\n",
    "        model_scores = {}\n",
    "        # for any csv file in the folder\n",
    "        for file in os.listdir(f'processed_data/models/{score_type}'):\n",
    "            # if the file is a json file\n",
    "            if file.endswith('.json'):\n",
    "                # open the file\n",
    "                with open(f'processed_data/models/{score_type}/{file}', 'r') as f:\n",
    "                    # load the data from the file\n",
    "                    model_scores[file.split('_')[0]] = json.load(f)\n",
    "\n",
    "        # split score_type by '_'\n",
    "        score_type_split = score_type.split('_')\n",
    "        if score_type_split[0] == 'con':\n",
    "            score_type_split[0] = 'Connectivity'\n",
    "        elif score_type_split[0] == 'raw':\n",
    "            score_type_split[0] = 'Raw'\n",
    "\n",
    "        score_type_split[1] = score_type_split[1].capitalize()\n",
    "\n",
    "        models = list(model_scores.keys())\n",
    "        modes = list(model_scores[list(model_scores.keys())[0]].keys())\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        colors = plt.cm.tab10.colors  # Use a colormap for distinct colors\n",
    "\n",
    "        # Assign a consistent color to each model\n",
    "        model_colors = {model: colors[idx % len(colors)] for idx, model in enumerate(models)}\n",
    "\n",
    "        for mode_idx, mode in enumerate(modes):\n",
    "            for model_idx, model in enumerate(models):\n",
    "                scores = list(model_scores[model][mode].keys())[0]\n",
    "                vp = plt.violinplot(\n",
    "                    model_scores[model][mode][scores],\n",
    "                    positions=[model_idx + mode_idx * (len(models) + 1)],\n",
    "                    showmeans=True,\n",
    "                    showextrema=True,\n",
    "                    widths=0.8,\n",
    "                    bw_method=0.5,\n",
    "                )\n",
    "                # Set the color for the violin parts\n",
    "                for part in vp['bodies']:\n",
    "                    part.set_facecolor(model_colors[model])\n",
    "                    part.set_edgecolor('black')\n",
    "                    part.set_alpha(0.7)\n",
    "                vp['cmeans'].set_color('black')  # Median line color\n",
    "                vp['cbars'].set_color('black')    # Whisker line color\n",
    "                vp['cmaxes'].set_color('black')   # Max line color\n",
    "                vp['cmins'].set_color('black')    # Min line color\n",
    "\n",
    "        # Set x-ticks and labels\n",
    "        x_ticks = [(len(models) + 1) * i + len(models) / 2 - 0.5 for i in range(len(modes))]\n",
    "        ax.set_xticks(x_ticks)\n",
    "        ax.set_xticklabels(modes)\n",
    "        ax.set_xlabel('Modes')\n",
    "        ax.set_ylabel('Scores')\n",
    "        ax.set_title(f'{score_type_split[0]} {score_type_split[1]} Scores (Accuracy%)')\n",
    "        ax.set_ylim(0, 100)\n",
    "        ax.set_yticks(np.arange(0, 101, 10))\n",
    "        ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        ax.legend(\n",
    "            handles=[mpatches.Patch(color=model_colors[model], label=model) for model in models],\n",
    "            title='Models',\n",
    "            loc='upper right'\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'plots/models/violin_plots/{score_type}.png', dpi=dpi / 2)\n",
    "        plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
