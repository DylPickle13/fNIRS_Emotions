{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import mne\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import re\n",
    "import seaborn as sns\n",
    "from statsmodels.formula.api import rlm, mixedlm\n",
    "\n",
    "from preprocess_nirs import *\n",
    "\n",
    "from mne_nirs.channels import picks_pair_to_idx, get_long_channels\n",
    "from mne_nirs.preprocessing import peak_power, scalp_coupling_index_windowed\n",
    "from mne.preprocessing.nirs import source_detector_distances, scalp_coupling_index\n",
    "from mne_connectivity import spectral_connectivity_epochs\n",
    "from mne_connectivity.viz import plot_connectivity_circle\n",
    "from mne.viz import circular_layout\n",
    "from mne_nirs.experimental_design import make_first_level_design_matrix\n",
    "from mne_nirs.statistics import run_glm, statsmodels_to_results\n",
    "from mne_nirs.visualisation import plot_glm_group_topo\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, LeaveOneGroupOut, cross_val_predict\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Set the image dpi\n",
    "dpi = 600\n",
    "\n",
    "# Set the number of cores to use\n",
    "n_jobs = 1\n",
    "if os.cpu_count() is not None:\n",
    "    n_jobs = int(os.cpu_count() * 0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pick Analyses to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "process_data = False\n",
    "good_threshold = 0.7\n",
    "cov_threshold = 15\n",
    "\n",
    "# Get the non windowed SCI and CV measure\n",
    "get_full_sci = False\n",
    "get_full_cv = False\n",
    "\n",
    "# Get the peak power/scalp coupling index dataframes\n",
    "get_peak_power_sci_df = False\n",
    "peak_power_threshold = 0.1\n",
    "\n",
    "# Get the SCI violin plots per participant\n",
    "get_participant_sci_plots = False\n",
    "\n",
    "# Get the GLM data\n",
    "get_glm_analysis = False\n",
    "get_ind_glm_plots = False\n",
    "get_group_glm_plots = False\n",
    "get_group_contrast_plots = False\n",
    "\n",
    "# Get the average timeseries activity\n",
    "get_roi_timeseries_activity = False\n",
    "\n",
    "# Get the ERP plots\n",
    "get_erp_mean_regions_plots = False\n",
    "get_erp_per_region_plots = False\n",
    "\n",
    "# Get the topographic plots\n",
    "get_topo_condition_plots = False\n",
    "get_topo_diff_plots = False\n",
    "\n",
    "# Get the connectivity data\n",
    "run_ind_connectivity = False\n",
    "\n",
    "# Get the individual connectivity plots\n",
    "get_ind_con_plots = False\n",
    "\n",
    "# Get the average connectivity over time over all participants\n",
    "get_avg_ind_con_plot_avg = False\n",
    "\n",
    "# Get the connectivity data for each condition\n",
    "run_condition_connectivity = False\n",
    "\n",
    "# Get the average connectivity data over time\n",
    "get_avg_condition_con_plot = False\n",
    "\n",
    "# Get the histogram connectivity plots\n",
    "get_hist_con_plots = False\n",
    "get_hist_diff_face_plots = False\n",
    "get_hist_diff_emotion_plots = False\n",
    "\n",
    "# Get the heatmap connectivity plots\n",
    "get_heatmap_con_dist_plots = False\n",
    "get_heatmap_diff_face_plots = False\n",
    "get_heatmap_diff_emotion_plots = False\n",
    "\n",
    "# Get the chord connectivity plots\n",
    "get_chord_con_plots = False\n",
    "get_chord_diff_face_plots = False\n",
    "get_chord_diff_emotion_plots = False\n",
    "\n",
    "# Get the time series data\n",
    "get_time_series = False\n",
    "run_models = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder path\n",
    "data_path = r\"\"\"C:\\Users\\super\\OneDrive - Ontario Tech University\\fNIRS_Emotions\\data\"\"\"\n",
    "\n",
    "# Get a list of paths of all the subfolders of the folders labeled 'P_1', 'P_2', etc.\n",
    "participants = [os.path.join(data_path, f) for f in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, f))]\n",
    "\n",
    "participants_with_same_order = []\n",
    "\n",
    "# remove participants P_1 to P_11 but keep P_10, and P_12 onwards\n",
    "# P_1 to P_11 have the same order of faces, and P_86 and P_87 have the same order of faces\n",
    "for i in range(1, 12):\n",
    "    if i != 10:\n",
    "        participants_with_same_order.append(os.path.join(data_path, f'P_{i}'))\n",
    "        #participants.remove(os.path.join(data_path, f'P_{i}'))\n",
    "\n",
    "participants_with_same_order.append(os.path.join(data_path, f'P_87'))\n",
    "#participants.remove(os.path.join(data_path, f'P_87'))\n",
    "\n",
    "# remove participants P_13 due to not recording\n",
    "participants.remove(os.path.join(data_path, f'P_13'))\n",
    "\n",
    "# remove participants P_50 due to ending early\n",
    "participants.remove(os.path.join(data_path, f'P_50'))\n",
    "\n",
    "# participant P_54 used their phone\n",
    "participants.remove(os.path.join(data_path, f'P_54'))\n",
    "\n",
    "# Search recursively for the folder with the .snirf extension\n",
    "fnirs_folders = []\n",
    "for participant in participants:\n",
    "    for root, dirs, files in os.walk(participant):\n",
    "        for file in files:\n",
    "            if file.endswith('.snirf'):\n",
    "                fnirs_folders.append(root)\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    raws = []\n",
    "    raw_ods = []\n",
    "    raw_haemos = []\n",
    "\n",
    "    if process_data:\n",
    "        # Load the snirf files\n",
    "        for folder in fnirs_folders:\n",
    "            # find all the .snirf files in the folder but get the full path\n",
    "            snirf_files = [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith('.snirf')]\n",
    "            if len(snirf_files) == 0:\n",
    "                print(f\"No .snirf files found in {folder}\")\n",
    "                continue\n",
    "            elif len(snirf_files) > 1:\n",
    "                raise Exception(f\"Multiple .snirf files found in {folder}\")\n",
    "            else:\n",
    "                raw = mne.io.read_raw_snirf(snirf_files[0], optode_frame='mri', preload=True, verbose=False)\n",
    "            \n",
    "            # find all the 'description.json' files in the folder but get the full path\n",
    "            description_files = [f for f in os.listdir(folder) if 'description.json' in f]\n",
    "            if len(description_files) == 0:\n",
    "                print(f\"No description.json files found in {folder}\")\n",
    "                continue\n",
    "            elif len(description_files) > 1:\n",
    "                raise Exception(f\"Multiple description.json files found in {folder}\")\n",
    "            else:\n",
    "                description = json.load(open(os.path.join(folder, description_files[0])))\n",
    "\n",
    "            # add the description to the raw object\n",
    "            raw.info['description'] = str(description)\n",
    "            raws.append(raw)\n",
    "\n",
    "        # sort the raws by the measurement date\n",
    "        raws = sorted(raws, key=lambda x: x.info['meas_date'])\n",
    "\n",
    "        i = 1\n",
    "        for raw in raws:\n",
    "            raw_od, raw_haemo = preprocess_data(raw)\n",
    "            raw_ods.append(raw_od)\n",
    "            raw_haemos.append(raw_haemo)\n",
    "            i += 1\n",
    "\n",
    "        # clear any files in each folder\n",
    "        for folder in ['processed_data/raws', 'processed_data/raw_ods', 'processed_data/raw haemos']:\n",
    "            for f in os.listdir(folder):\n",
    "                os.remove(os.path.join(folder, f))\n",
    "\n",
    "        for i, (raw, raw_od, raw_haemo) in enumerate(zip(raws, raw_ods, raw_haemos), 1):\n",
    "            # save raw as a fif file\n",
    "            raw.save(f'processed_data/raws/raw{i}.fif', overwrite=True, verbose=False)\n",
    "\n",
    "            # save raw_od as a fif file\n",
    "            raw_od.save(f'processed_data/raw_ods/raw_od{i}.fif', overwrite=True, verbose=False)\n",
    "\n",
    "            # save raw_haemo as a fif file\n",
    "            raw_haemo.save(f'processed_data/raw haemos/raw_haemo{i}.fif', overwrite=True, verbose=False)\n",
    "        \n",
    "        raws = []\n",
    "        raw_ods = []\n",
    "        raw_haemos = []\n",
    "\n",
    "    # count how many files are in the processed_data/raw_haemos folder\n",
    "    processed_data_count = len([f for f in os.listdir('processed_data/raw haemos') if f.endswith('.fif')])\n",
    "\n",
    "    # Load the processed data\n",
    "    for i in range(1, processed_data_count + 1):\n",
    "        raw = mne.io.read_raw_fif(f'processed_data/raws/raw{i}.fif', preload=True, verbose=False)\n",
    "        raws.append(raw)\n",
    "\n",
    "        raw_od = mne.io.read_raw_fif(f'processed_data/raw_ods/raw_od{i}.fif', preload=True, verbose=False)\n",
    "        raw_ods.append(raw_od)\n",
    "\n",
    "        raw_haemo = mne.io.read_raw_fif(f'processed_data/raw haemos/raw_haemo{i}.fif', preload=True, verbose=False)\n",
    "        raw_haemos.append(raw_haemo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Participant Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Participant Age/Head Size Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the file with description.json as a substring in each subfolder\n",
    "description_files = [os.path.join(subfolder, f) for subfolder in fnirs_folders for f in os.listdir(subfolder) if 'description.json' in f]\n",
    "\n",
    "# Load the description files\n",
    "descriptions = [json.load(open(description_file)) for description_file in description_files]\n",
    "\n",
    "# Get the average age of the participants and convert it to a float\n",
    "ages = [float(description['age']) for description in descriptions]\n",
    "average_age = sum(ages) / len(ages)\n",
    "min_age = min(ages)\n",
    "max_age = max(ages)\n",
    "std_age = np.std(ages)\n",
    "\n",
    "# Convert the remarks to a float and get the average\n",
    "remarks = [description['remarks'] for description in descriptions]\n",
    "# if remark is '', replace it with None\n",
    "remarks = [float(remark) if remark != '' else None for remark in remarks]\n",
    "average_head_circumference = sum(remark for remark in remarks if remark is not None) / len([remark for remark in remarks if remark is not None])\n",
    "std_head_circumference = np.std([remark for remark in remarks if remark is not None])\n",
    "# Create subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Age histogram\n",
    "axs[0].hist(ages, bins=40, edgecolor='black')\n",
    "axs[0].set_xlabel('Age')\n",
    "axs[0].set_ylabel('Number of participants')\n",
    "axs[0].set_title(f'Age distribution of participants\\n{len(ages)} participants, Mean: {round(average_age, 2)}, Std: {round(std_age, 2)}')\n",
    "\n",
    "# Head circumference histogram\n",
    "axs[1].hist([remark * 2.54 for remark in remarks if remark is not None], bins=40, edgecolor='black')\n",
    "axs[1].set_xlabel('Head circumference (cm)')\n",
    "axs[1].set_ylabel('Number of participants')\n",
    "axs[1].set_title(f'Head circumference distribution of participants\\nMean: {round(average_head_circumference * 2.54, 2)} cm, Std: {round(std_head_circumference * 2.54, 2)} cm')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/participants/participant_info.png', dpi=dpi)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurement Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the measurement dates\n",
    "measurement_dates = [raw_haemo.info['meas_date'] for raw_haemo in raw_haemos]\n",
    "\n",
    "# Convert to pandas datetime\n",
    "measurement_dates = pd.to_datetime(measurement_dates)\n",
    "\n",
    "# Create a plot of the measurement dates\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.plot(measurement_dates, range(1, len(measurement_dates) + 1), 'o-')\n",
    "plt.xlabel('Measurement date')\n",
    "plt.ylabel('Participant number')\n",
    "plt.title('Measurement dates of participants, N = ' + str(len(measurement_dates)))\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/participants/measurement_dates.png', dpi=dpi / 4)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short Distance Channels Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 participants with 4 short channels (< 1 cm) and 206 long channels (>= 1 cm)\n",
      "39 participants with 16 short channels (< 1 cm) and 206 long channels (>= 1 cm)\n"
     ]
    }
   ],
   "source": [
    "distance_counts = [\n",
    "    (\n",
    "        sum(distances < 0.01),\n",
    "        sum((distances >= 0.01))\n",
    "    )\n",
    "    for raw in raws\n",
    "    for distances in [np.array(source_detector_distances(raw.info))]\n",
    "]\n",
    "\n",
    "# Count unique tuples\n",
    "unique_distance_counts = Counter(distance_counts)\n",
    "\n",
    "# Display the results\n",
    "for (short_count, long_count), participant_count in unique_distance_counts.items():\n",
    "    print(f\"{participant_count} participants with {short_count} short channels (< 1 cm) and {long_count} long channels (>= 1 cm)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping brain regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the channel names for hbo\n",
    "ch_names_hbo = [ch_name for ch_name in raw_haemos[0].ch_names if 'hbo' in ch_name]\n",
    "\n",
    "ch_mapping_hbo = {\n",
    "    \"Left Frontal\": [],\n",
    "    \"Right Frontal\": [],\n",
    "    \"Left Prefrontal\": [],\n",
    "    \"Right Prefrontal\": [],\n",
    "    \"Left Parietal\": [],\n",
    "    \"Right Parietal\": [],\n",
    "    \"Left Occipital\": [],\n",
    "    \"Right Occipital\": []\n",
    "}\n",
    "\n",
    "group_boundaries = [0]\n",
    "\n",
    "ch_mapping_hbo[\"Left Frontal\"].append('S1_D1 hbo')\n",
    "\n",
    "ch_mapping_hbo[\"Left Frontal\"].append('S1_D2 hbo')\n",
    "\n",
    "ch_mapping_hbo[\"Left Frontal\"].append('S1_D17 hbo')\n",
    "\n",
    "# find the channels that have 'S2_', 'S3_', 'S4_', 'S5_' in them\n",
    "for ch_name in [ch_name for ch_name in ch_names_hbo if 'S2_' in ch_name or 'S3_' in ch_name or 'S4_' in ch_name or 'S5_' in ch_name]:\n",
    "    ch_mapping_hbo[\"Left Frontal\"].append(ch_name)\n",
    "\n",
    "ch_mapping_hbo[\"Left Frontal\"].append('S6_D2 hbo')\n",
    "\n",
    "ch_mapping_hbo[\"Left Frontal\"].append('S6_D3 hbo')\n",
    "\n",
    "ch_mapping_hbo[\"Left Frontal\"].append('S6_D18 hbo')\n",
    "\n",
    "group_boundaries.append(len(ch_mapping_hbo[\"Left Frontal\"]))\n",
    "\n",
    "# find the channels that have 'S9_', 'S10_', 'S11_', 'S12_' in them\n",
    "for ch_name in [ch_name for ch_name in ch_names_hbo if 'S9_' in ch_name or 'S10_' in ch_name or 'S11_' in ch_name or 'S12_' in ch_name]:\n",
    "    ch_mapping_hbo[\"Right Frontal\"].append(ch_name)\n",
    "\n",
    "group_boundaries.append(len(ch_mapping_hbo[\"Right Frontal\"]) + group_boundaries[-1])\n",
    "\n",
    "ch_mapping_hbo[\"Left Prefrontal\"].append('S6_D31 hbo')\n",
    "\n",
    "# find the channels that have 'S7_', 'S8_', 'S25_', 'S26_' in them\n",
    "for ch_name in [ch_name for ch_name in ch_names_hbo if 'S7_' in ch_name or 'S8_' in ch_name or 'S25_' in ch_name or 'S26_' in ch_name]:\n",
    "    ch_mapping_hbo[\"Left Prefrontal\"].append(ch_name)\n",
    "\n",
    "group_boundaries.append(len(ch_mapping_hbo[\"Left Prefrontal\"]) + group_boundaries[-1])\n",
    "\n",
    "# find the channels that have 'S13_', 'S14_', 'S15_', 'S16_' in them\n",
    "for ch_name in [ch_name for ch_name in ch_names_hbo if 'S13_' in ch_name or 'S14_' in ch_name or 'S15_' in ch_name or 'S16_' in ch_name]:\n",
    "    ch_mapping_hbo[\"Right Prefrontal\"].append(ch_name)\n",
    "\n",
    "group_boundaries.append(len(ch_mapping_hbo[\"Right Prefrontal\"]) + group_boundaries[-1])\n",
    "\n",
    "# find the channels that have 'S27_', 'S28_', 'S29_', 'S30_' in them\n",
    "for ch_name in [ch_name for ch_name in ch_names_hbo if 'S27_' in ch_name or 'S28_' in ch_name or 'S29_' in ch_name or 'S30_' in ch_name]:\n",
    "    ch_mapping_hbo[\"Left Parietal\"].append(ch_name)\n",
    "\n",
    "group_boundaries.append(len(ch_mapping_hbo[\"Left Parietal\"]) + group_boundaries[-1])\n",
    "\n",
    "ch_mapping_hbo[\"Right Occipital\"].append('S21_D13 hbo')\n",
    "\n",
    "ch_mapping_hbo[\"Right Occipital\"].append('S21_D16 hbo')\n",
    "\n",
    "ch_mapping_hbo[\"Right Occipital\"].append('S23_D15 hbo')\n",
    "\n",
    "ch_mapping_hbo[\"Right Occipital\"].append('S23_D16 hbo')\n",
    "\n",
    "# find the channels that have 'S17_', 'S18_', 'S19_', 'S20_' in them\n",
    "for ch_name in [ch_name for ch_name in ch_names_hbo if 'S17_' in ch_name or 'S18_' in ch_name or 'S19_' in ch_name or 'S20_' in ch_name]:\n",
    "    ch_mapping_hbo[\"Right Parietal\"].append(ch_name)\n",
    "\n",
    "group_boundaries.append(len(ch_mapping_hbo[\"Right Parietal\"]) + group_boundaries[-1])\n",
    "\n",
    "# find the channels that have 'S32_', 'S31_' in them\n",
    "for ch_name in [ch_name for ch_name in ch_names_hbo if 'S32_' in ch_name or 'S31_' in ch_name]:\n",
    "    ch_mapping_hbo[\"Left Occipital\"].append(ch_name)\n",
    "\n",
    "group_boundaries.append(len(ch_mapping_hbo[\"Left Occipital\"]) + group_boundaries[-1])\n",
    "\n",
    "# find the channels that have 'S22_', 'S24_' in them\n",
    "for ch_name in [ch_name for ch_name in ch_names_hbo if 'S22_' in ch_name or 'S24_' in ch_name]:\n",
    "    ch_mapping_hbo[\"Right Occipital\"].append(ch_name)\n",
    "\n",
    "ch_mapping_hbo[\"Right Occipital\"].append('S21_D28 hbo')\n",
    "\n",
    "ch_mapping_hbo[\"Right Occipital\"].append('S23_D30 hbo')\n",
    "\n",
    "ch_mapping_hbr = {region: [channel.replace('hbo', 'hbr') for channel in ch_mapping_hbo[region]] for region in ch_mapping_hbo}\n",
    "\n",
    "ch_mapping_hbt = {region: [channel.replace('hbo', 'hbt') for channel in ch_mapping_hbo[region]] for region in ch_mapping_hbo}\n",
    "\n",
    "ch_mapping_all = {region: ch_mapping_hbo[region] + ch_mapping_hbr[region] + ch_mapping_hbt[region] for region in ch_mapping_hbo}\n",
    "\n",
    "# concatenate the values of the dictionary into a list\n",
    "all_channels_hbo = [channel for region in ch_mapping_hbo.values() for channel in region]\n",
    "\n",
    "# duplicate all_channels but replace 'hbo' with 'hbr'\n",
    "all_channels_hbr = [channel.replace('hbo', 'hbr') for channel in all_channels_hbo]\n",
    "\n",
    "all_channels_hbt = [channel.replace('hbo', 'hbt') for channel in all_channels_hbo]\n",
    "\n",
    "# concatenate all_channels_hbo and all_channels_hbr\n",
    "all_channels = all_channels_hbo + all_channels_hbr + all_channels_hbt\n",
    "\n",
    "# make a dictionary called ch_mapping_names that has the channel names without the 'hbo' or 'hbr' at the end\n",
    "ch_mapping_names = {region: [channel[:-4] for channel in ch_mapping_hbo[region]] for region in ch_mapping_hbo}\n",
    "\n",
    "# make a list of all the channel names without the 'hbo' or 'hbr' at the end\n",
    "all_channels_names = [channel[:-4] for channel in all_channels_hbo]\n",
    "\n",
    "ch_names_original = [ch_name[:-4] for ch_name in ch_names_hbo]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalp Coupling Index (SCI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_full_sci:\n",
    "    # for each recording, count the number of channels with a sci greater than good_threshold\n",
    "    good_channels = [sum(scalp_coupling_index(raw_od, verbose=False) >= good_threshold) for raw_od in raw_ods]\n",
    "    bad_channels = [sum(scalp_coupling_index(raw_od, verbose=False) < good_threshold) for raw_od in raw_ods]\n",
    "    good_recordings = sum([good_channel >= good_threshold * (good_channel + bad_channel) for good_channel, bad_channel in zip(good_channels, bad_channels)])\n",
    "\n",
    "    # Plot the good vs bad channels for each recording in a dual bar chart\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.bar(range(len(good_channels)), good_channels, label='Good Channels')\n",
    "    ax.bar(range(len(bad_channels)), bad_channels, bottom=good_channels, label='Bad Channels')\n",
    "    ax.set_xlabel('Recording')\n",
    "    ax.set_ylabel('Number of Channels')\n",
    "    ax.axhline(raw_od.info['nchan'] * good_threshold, color='green', linestyle='--')\n",
    "    title = f'Good vs Bad Channels (T = {good_threshold})\\nGood Recordings: {good_recordings}, N = {len(raw_ods)}, Retention Rate: {good_recordings / len(raw_ods) * 100:.2f}%'\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    plt.savefig(f'plots/signal quality/Signal Quality (SCI).png', dpi=dpi)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coefficient of Variance (CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_full_cv:\n",
    "    # for each recording, count the number of channels with a cv less than cov_threshold\n",
    "    good_channels = [sum(100 * np.std(ch) / np.mean(ch) < cov_threshold for ch in get_long_channels(raw).get_data()) for raw in raws]\n",
    "    bad_channels = [sum(100 * np.std(ch) / np.mean(ch) >= cov_threshold for ch in get_long_channels(raw).get_data()) for raw in raws]\n",
    "    good_recordings = sum([good_channel >= good_threshold * (good_channel + bad_channel) for good_channel, bad_channel in zip(good_channels, bad_channels)])\n",
    "\n",
    "    # Plot the good vs bad channels for each recording in a dual bar chart\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.bar(range(len(good_channels)), good_channels, label='Good Channels')\n",
    "    ax.bar(range(len(bad_channels)), bad_channels, bottom=good_channels, label='Bad Channels')\n",
    "    ax.set_xlabel('Recording')\n",
    "    ax.set_ylabel('Number of Channels')\n",
    "    ax.axhline(raw.info['nchan'] * good_threshold, color='green', linestyle='--')\n",
    "    title = f'Good vs Bad Channels (T = {cov_threshold})\\nGood Recordings: {good_recordings}, N = {len(raws)}, Retention Rate: {good_recordings / len(raws) * 100:.2f}%'\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    plt.savefig(f'plots/signal quality/Signal Quality (CV).png', dpi=dpi)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Peak Power/SCI Sliding Window CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_peak_power_sci_df:\n",
    "    peak_power_df = pd.DataFrame()\n",
    "    sci_df = pd.DataFrame()\n",
    "    for i, raw_od in enumerate(raw_ods, 1):\n",
    "        raw_od_annotated_pp, scores_pp, times_pp = peak_power(raw_od, time_window=5, threshold=peak_power_threshold, verbose=False)\n",
    "        raw_od_annotated_sci, scores_sci, times_sci = scalp_coupling_index_windowed(raw_od, time_window=5, threshold=good_threshold, verbose=False)\n",
    "\n",
    "        # Convert scores and times to a DataFrame\n",
    "        df_pp = pd.DataFrame(scores_pp.T, columns=[ch_name for ch_name in raw_od.ch_names])\n",
    "        df_sci = pd.DataFrame(scores_sci.T, columns=[ch_name for ch_name in raw_od.ch_names])\n",
    "\n",
    "        # Add time window information\n",
    "        df_pp[\"Start_Time\"] = [t[0] for t in times_pp]\n",
    "        df_pp[\"End_Time\"] = [t[1] for t in times_pp]\n",
    "        df_sci[\"Start_Time\"] = [t[0] for t in times_sci]\n",
    "        df_sci[\"End_Time\"] = [t[1] for t in times_sci]\n",
    "\n",
    "        # Add an index column for window number\n",
    "        df_pp.insert(0, 'Window', range(1, len(df_pp) + 1))\n",
    "        df_sci.insert(0, 'Window', range(1, len(df_sci) + 1))\n",
    "\n",
    "        # Reorder columns so time comes first\n",
    "        df_pp = df_pp[[\"Start_Time\", \"End_Time\"] + list(df_pp.columns[:-2])]\n",
    "        df_sci = df_sci[[\"Start_Time\", \"End_Time\"] + list(df_sci.columns[:-2])]\n",
    "\n",
    "        # remove the columns with '850' in the name\n",
    "        df_pp = df_pp.loc[:, ~df_pp.columns.str.contains('850')]\n",
    "        df_sci = df_sci.loc[:, ~df_sci.columns.str.contains('850')]\n",
    "\n",
    "        # rename the columns to remove the ' 760' at the end if it exists\n",
    "        df_pp.columns = [col[:-4] if col.endswith(' 760') else col for col in df_pp.columns]\n",
    "        df_sci.columns = [col[:-4] if col.endswith(' 760') else col for col in df_sci.columns]\n",
    "\n",
    "        # Add a column for participant number, make it the first column\n",
    "        df_pp.insert(0, 'Participant', i)\n",
    "        df_sci.insert(0, 'Participant', i)\n",
    "\n",
    "        # Append the DataFrame to the list\n",
    "        peak_power_df = pd.concat([peak_power_df, df_pp])\n",
    "        sci_df = pd.concat([sci_df, df_sci])\n",
    "        print(f\"Processed participant {i}\")\n",
    "\n",
    "    # reset the index\n",
    "    peak_power_df.reset_index(drop=True, inplace=True)\n",
    "    sci_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    peak_power_df.to_csv('processed_data/windows/peak_power.csv', index=False)\n",
    "    sci_df.to_csv('processed_data/windows/sci.csv', index=False)\n",
    "\n",
    "# Load the DataFrame\n",
    "peak_power_df = pd.read_csv('processed_data/windows/peak_power.csv')\n",
    "sci_df = pd.read_csv('processed_data/windows/sci.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peak Power/SCI Sliding Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the proportion of windows with peak power > peak_power_threshold for each channel\n",
    "percentage_good_windows_peak_power_df = (\n",
    "    peak_power_df.groupby(\"Participant\")[peak_power_df.columns[4:]]\n",
    "    .apply(lambda df: (df > peak_power_threshold).sum() / len(df))\n",
    ")\n",
    "\n",
    "# add a good recordings column\n",
    "good_recordings = (percentage_good_windows_peak_power_df > good_threshold).sum(axis=1) / len(percentage_good_windows_peak_power_df.columns)\n",
    "percentage_good_windows_peak_power_df.insert(0, f'Good Recordings (peak_power > {peak_power_threshold} for > {good_threshold * 100}% of channels)', good_recordings)\n",
    "\n",
    "# Compute the proportion of windows with SCI > good_threshold for each channel\n",
    "percentage_good_windows_sci_df = (\n",
    "    sci_df.groupby(\"Participant\")[sci_df.columns[4:]]\n",
    "    .apply(lambda df: (df >= good_threshold).sum() / len(df))\n",
    ")\n",
    "\n",
    "# add a good recordings column\n",
    "good_recordings = (percentage_good_windows_sci_df > good_threshold).sum(axis=1) / len(percentage_good_windows_sci_df.columns)\n",
    "percentage_good_windows_sci_df.insert(0, f'Good Recordings (SCI > {good_threshold} for > {good_threshold * 100}% of channels)', good_recordings)\n",
    "\n",
    "# merge the two dataframes on the first 2 columns\n",
    "percentage_good_windows_df = pd.merge(percentage_good_windows_peak_power_df[percentage_good_windows_peak_power_df.columns[:1]], percentage_good_windows_sci_df[percentage_good_windows_sci_df.columns[:1]], on='Participant')\n",
    "\n",
    "# create a new column that is true if both columns are greater than good_threshold\n",
    "percentage_good_windows_df['Good Recording'] = (percentage_good_windows_df.iloc[:, 0] > good_threshold) & (percentage_good_windows_df.iloc[:, 1] > good_threshold)\n",
    "\n",
    "# Plot a bar chart where the SCI and peak power windows are compared, two bars next to each other for each participant\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "bar_width = 0.45\n",
    "x = np.arange(len(percentage_good_windows_df))\n",
    "ax.bar(x, percentage_good_windows_df.iloc[:, 0], bar_width, label='Peak Power')\n",
    "ax.bar(x + bar_width, percentage_good_windows_df.iloc[:, 1], bar_width, label='SCI')\n",
    "ax.axhline(good_threshold, color='green', linestyle='--')\n",
    "ax.set_xlabel('Participant')\n",
    "ax.set_ylabel('Percentage of Good Windows')\n",
    "title = f'Percentage of Good Windows: peak_power > {peak_power_threshold}, SCI > {good_threshold}\\nGood Recordings: {percentage_good_windows_df[\"Good Recording\"].sum()}, N = {len(percentage_good_windows_df)}, Retention Rate: {percentage_good_windows_df[\"Good Recording\"].sum() / len(percentage_good_windows_df) * 100:.2f}%'\n",
    "ax.set_title(title)\n",
    "ax.set_xticks(x + bar_width / 2)\n",
    "ax.set_xticklabels(percentage_good_windows_df.index)\n",
    "ax.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'plots/signal quality/Percentage of Good Windows.png', dpi=dpi / 2)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peak Power/SCI Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_channel_slopes = []\n",
    "sci_channel_slopes = []\n",
    "for pp_channel, sci_channel in zip(percentage_good_windows_peak_power_df.columns[4:], percentage_good_windows_sci_df.columns[4:]):\n",
    "    pp_slope = []\n",
    "    sci_slope = []\n",
    "    for participant in peak_power_df['Participant'].unique():\n",
    "        # get the data for the channel\n",
    "        pp_array = peak_power_df[peak_power_df['Participant'] == participant][pp_channel]\n",
    "        sci_array = sci_df[sci_df['Participant'] == participant][sci_channel]\n",
    "\n",
    "        # get a line of best fit for the data\n",
    "        x = np.arange(len(pp_array))\n",
    "        pp_m, pp_b = np.polyfit(x, pp_array, 1)\n",
    "        pp_slope.append(pp_m)\n",
    "\n",
    "        sci_m, sci_b = np.polyfit(x, sci_array, 1)\n",
    "        sci_slope.append(sci_m)\n",
    "    pp_channel_slopes.append((pp_channel, np.mean(pp_slope)))\n",
    "    sci_channel_slopes.append((sci_channel, np.mean(sci_slope)))\n",
    "\n",
    "# plot the slopes\n",
    "fig, ax = plt.subplots(figsize=(20, 6))\n",
    "# sort the channels by slope\n",
    "pp_channel_slopes.sort(key=lambda x: x[1])\n",
    "sci_channel_slopes.sort(key=lambda x: x[1])\n",
    "bar_width = 0.45\n",
    "x = np.arange(len(pp_channel_slopes))\n",
    "ax.bar(x, [slope for channel, slope in pp_channel_slopes], bar_width, label='Peak Power')\n",
    "ax.bar(x + bar_width, [slope for channel, slope in sci_channel_slopes], bar_width, label='SCI')\n",
    "ax.set_xlabel('Channel')\n",
    "ax.set_ylabel('Slope')\n",
    "ax.set_title('Slope of Peak Power and SCI over Time')\n",
    "ax.set_xticks(x + bar_width / 2)\n",
    "ax.set_xticklabels([channel for channel, slope in pp_channel_slopes])\n",
    "ax.legend()\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'plots/signal quality/Slope of Peak Power and SCI over Time.png', dpi=dpi)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Head Size vs. SCI Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_35796\\3840856922.py:21: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '60.96' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  percentage_good_windows_sci_df_with_head_size.loc[i, 'Head Size (cm)'] = head_size\n"
     ]
    }
   ],
   "source": [
    "cap_size = 58\n",
    "percentage_good_windows_sci_df_with_head_size = percentage_good_windows_sci_df.copy()\n",
    "\n",
    "# add the head size to the percentage_good_windows_sci_df as the first column\n",
    "if 'Head Size (cm)' not in percentage_good_windows_sci_df_with_head_size.columns:\n",
    "    percentage_good_windows_sci_df_with_head_size.insert(0, 'Head Size (cm)', cap_size)\n",
    "\n",
    "no_head_size = []\n",
    "\n",
    "for i, raw_haemo in enumerate(raw_haemos, 1):\n",
    "    # get the head size\n",
    "    head_size = get_info(raw_haemo)['remarks']\n",
    "    if head_size:\n",
    "        head_size = float(head_size) * 2.54\n",
    "    else:\n",
    "        # add the participant number to the no_head_size list\n",
    "        no_head_size.append(i)\n",
    "        continue\n",
    "    \n",
    "    # append the head_size to percentage_good_windows_sci_df\n",
    "    percentage_good_windows_sci_df_with_head_size.loc[i, 'Head Size (cm)'] = head_size\n",
    "\n",
    "# remove the participants with no head size\n",
    "percentage_good_windows_sci_df_with_head_size = percentage_good_windows_sci_df_with_head_size.drop(no_head_size)\n",
    "\n",
    "# get the correlation between head size and the channels\n",
    "correlations = []\n",
    "for channel in percentage_good_windows_sci_df_with_head_size.columns[2:]:\n",
    "    correlation = percentage_good_windows_sci_df_with_head_size['Head Size (cm)'].corr(percentage_good_windows_sci_df_with_head_size[channel])\n",
    "    correlations.append((channel, correlation))\n",
    "\n",
    "# get the correlation between head size and the second column\n",
    "correlation = percentage_good_windows_sci_df_with_head_size['Head Size (cm)'].corr(percentage_good_windows_sci_df_with_head_size.iloc[:, 1])\n",
    "\n",
    "# plot the correlations\n",
    "fig, ax = plt.subplots(figsize=(20, 6))\n",
    "# sort the channels by correlation\n",
    "correlations.sort(key=lambda x: x[1])\n",
    "bar_width = 0.45\n",
    "x = np.arange(len(correlations))\n",
    "ax.bar(x, [correlation for channel, correlation in correlations], bar_width)\n",
    "ax.set_xlabel('Channel')\n",
    "ax.set_ylabel('Correlation')\n",
    "ax.set_title('Correlation between Head Size and SCI, Correlation with Good Recordings: ' + str(correlation) + ', N = ' + str(len(percentage_good_windows_sci_df_with_head_size)))\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([channel for channel, correlation in correlations])\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'plots/signal quality/Correlation between Head Size and SCI.png', dpi=dpi / 3)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average SCI per Channel across Participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of participants in percentage_good_windows_sci_df where Good Recording is True\n",
    "good_participants = percentage_good_windows_df[percentage_good_windows_df['Good Recording'] == True].index\n",
    "\n",
    "# make a dataframe of the average sci for each channel\n",
    "avg_sci_df = sci_df.groupby('Participant').mean().drop(columns=['Window', 'Start_Time', 'End_Time'])\n",
    "\n",
    "# drop the participants that are not in good_participants\n",
    "avg_sci_df_good = avg_sci_df.loc[good_participants]\n",
    "\n",
    "# drop the participants that are in good_participants\n",
    "avg_sci_df_bad = avg_sci_df.drop(index=good_participants)\n",
    "\n",
    "# make a list of the dataframes\n",
    "avg_sci_dfs = [avg_sci_df, avg_sci_df_good, avg_sci_df_bad]\n",
    "df_names = ['All Participants', 'Good Participants', 'Bad Participants']\n",
    "color_list = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'pink', 'gray']\n",
    "\n",
    "for df, df_name in zip(avg_sci_dfs, df_names):\n",
    "\n",
    "    # make a violin plot of the average sci for each channel\n",
    "    fig, ax = plt.subplots(figsize=(35, 6))\n",
    "    parts = ax.violinplot(df, showmeans=False, widths=1, showextrema=False)\n",
    "\n",
    "    # match the violin plot colors to the columns in avg_sci_df to the channels in ch_mapping_names\n",
    "    color_i = 0\n",
    "    colors = []\n",
    "    region_labels = []\n",
    "\n",
    "    # for each region in ch_mapping_names, apply the color to the channels in that region\n",
    "    for region, channels in ch_mapping_names.items():\n",
    "        for channel in channels:\n",
    "            if channel in df.columns:\n",
    "                colors.append(color_list[color_i])\n",
    "                region_labels.append(region)\n",
    "        color_i += 1\n",
    "\n",
    "    # set the colors of the violins\n",
    "    for i, pc in enumerate(parts['bodies']):\n",
    "        pc.set_facecolor(colors[i])\n",
    "        pc.set_edgecolor('black')\n",
    "        pc.set_alpha(1)\n",
    "\n",
    "    # create a legend\n",
    "    handles = [plt.Rectangle((0, 0), 1, 1, color=color) for color in list(dict.fromkeys(colors))]\n",
    "    ax.legend(handles, list(dict.fromkeys(region_labels)), loc='lower left')\n",
    "\n",
    "    # add a white scatter plot of the mean sci for each channel\n",
    "    ax.scatter(np.arange(1, len(df.columns) + 1), df.mean(), color='white', zorder=3)\n",
    "\n",
    "    ax.set_xlabel('Channel')\n",
    "    ax.set_ylabel('Average SCI')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axhline(good_threshold, color='green', linestyle='--')\n",
    "    ax.set_title(f'Average SCI per Channel: ({df_name}), N = {len(df)}')\n",
    "    ax.set_xticks(np.arange(1, len(df.columns) + 1))\n",
    "    ax.set_xticklabels(df.columns)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/signal quality/Average SCI (Windowed) per Channel/{df_name}.png', dpi=dpi / 3)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCI of Windows per Channel for each Participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_participant_sci_plots:\n",
    "    for i in sci_df['Participant'].unique():\n",
    "        df = sci_df[sci_df['Participant'] == i]\n",
    "\n",
    "        # get the good recording status\n",
    "        good_recording = False\n",
    "        if percentage_good_windows_df['Good Recording'][i]:\n",
    "            good_recording = True\n",
    "\n",
    "        # make a violin plot of the sci for each channel\n",
    "        fig, ax = plt.subplots(figsize=(35, 6))\n",
    "        parts = ax.violinplot(df[df.columns[4:]], showmeans=False, widths=1, showextrema=False)\n",
    "\n",
    "        # match the violin plot colors to the columns in sci_df to the channels in ch_mapping_names\n",
    "        color_i = 0\n",
    "        colors = []\n",
    "        region_labels = []\n",
    "\n",
    "        # for each region in ch_mapping_names, apply the color to the channels in that region\n",
    "        for region, channels in ch_mapping_names.items():\n",
    "            for channel in channels:\n",
    "                if channel in df.columns:\n",
    "                    colors.append(color_list[color_i])\n",
    "                    region_labels.append(region)\n",
    "            color_i += 1\n",
    "\n",
    "        # set the colors of the violins\n",
    "        for j, pc in enumerate(parts['bodies']):\n",
    "            pc.set_facecolor(colors[j])\n",
    "            pc.set_edgecolor('black')\n",
    "            pc.set_alpha(1)\n",
    "\n",
    "        # create a legend\n",
    "        handles = [plt.Rectangle((0, 0), 1, 1, color=color) for color in list(dict.fromkeys(colors))]\n",
    "        ax.legend(handles, list(dict.fromkeys(region_labels)), loc='lower left')\n",
    "\n",
    "        # add a white scatter plot of the mean sci for each channel\n",
    "        ax.scatter(np.arange(1, len(df.columns) - 3), df[df.columns[4:]].mean(), color='white', zorder=3)\n",
    "\n",
    "        ax.set_xlabel('Channel')\n",
    "        ax.set_ylabel('SCI')\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.axhline(good_threshold, color='green', linestyle='--')\n",
    "        ax.set_title(f'SCI per Channel: Participant {i}, Windows = {len(df)}, Good Recording = {good_recording}')\n",
    "        ax.set_xticks(np.arange(1, len(df.columns) - 3))\n",
    "        ax.set_xticklabels(df.columns[4:])\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'plots/signal quality/Average SCI (Windowed) per Channel/individual/Participant {i}.png', dpi=dpi / 3)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Good Recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_haemo_good_recordings = []\n",
    "for i, raw_haemo in enumerate(raw_haemos, 1):\n",
    "    if len(percentage_good_windows_df) >= i:\n",
    "        if percentage_good_windows_df.iloc[i - 1]['Good Recording']:\n",
    "            raw_haemo_good_recordings.append(raw_haemo.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLM analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = ['face_type', 'emotion', 'neutral_vs_emotion']\n",
    "conditions_list = {\n",
    "    'face_type': ['Real', 'Virt', 'Base'],\n",
    "    'emotion': ['Joy', 'Fear', 'Anger', 'Disgust', 'Sadness', 'Neutral', 'Surprise'],\n",
    "    'neutral_vs_emotion': ['Emotion', 'Neutral', 'Base']\n",
    "}\n",
    "\n",
    "if get_glm_analysis:\n",
    "    channel_types = ['hbo', 'hbr', 'hbt']\n",
    "    for mode in modes:\n",
    "        cha_df = pd.DataFrame()\n",
    "        roi_df = pd.DataFrame()\n",
    "        con_df = pd.DataFrame()\n",
    "        for i, raw_haemo in enumerate(raw_haemo_good_recordings, 1):\n",
    "            raw_haemo_annots = pick_channels(raw_haemo, channel_types)\n",
    "            relabel_annotations(raw_haemo_annots, mode=mode)\n",
    "\n",
    "            # Create a design matrix\n",
    "            design_matrix = make_first_level_design_matrix(\n",
    "                raw_haemo_annots,\n",
    "                drift_model=\"cosine\",\n",
    "                high_pass=0.03125,  # The cutoff period (1/high_pass) should be set as the longest period between two trials of the same condition multiplied by 2\n",
    "                hrf_model=\"spm\",\n",
    "                stim_dur=16.0,\n",
    "            )\n",
    "            \n",
    "            # Run GLM\n",
    "            glm_est = run_glm(raw_haemo_annots, design_matrix)\n",
    "\n",
    "            cha = glm_est.to_dataframe()\n",
    "\n",
    "            # in ch_mapping_all, for each list of channels in the dict, each string is formatted as 'S{number}_D{number} {hbo/hbr}', extract the number from the string and replace the string with [number, number]\n",
    "            groups = {region: [[int(re.findall(r'\\d+', channel)[0]), int(re.findall(r'\\d+', channel)[1])] for channel in ch_mapping_all[region]] for region in ch_mapping_all}\n",
    "            # apply picks_pair_to_idx to each region in groups\n",
    "            for region in groups:\n",
    "                groups[region] = picks_pair_to_idx(raw_haemo_annots, groups[region], on_missing='ignore')\n",
    "\n",
    "            # Compute region of interest results from channel data\n",
    "            roi = glm_est.to_dataframe_region_of_interest(\n",
    "                groups, design_matrix.columns, demographic_info=True\n",
    "            )\n",
    "\n",
    "            # Define contrasts\n",
    "            contrast_matrix = np.eye(design_matrix.shape[1])\n",
    "            basic_conts = dict(\n",
    "                [(column, contrast_matrix[j]) for j, column in enumerate(design_matrix.columns)]\n",
    "            )\n",
    "            contrasts = []\n",
    "            unique_annots = np.unique(raw_haemo_annots.annotations.description).tolist()\n",
    "            pairs = list(itertools.combinations(unique_annots, 2))\n",
    "            # include the opposite of each pair\n",
    "            pairs = pairs + [(pair[1], pair[0]) for pair in pairs]\n",
    "\n",
    "            # Compute defined contrast pairs\n",
    "            for pair in pairs:\n",
    "                con = glm_est.compute_contrast(basic_conts[pair[0]] - basic_conts[pair[1]]).to_dataframe()\n",
    "                con[\"contrast_pair\"] = f\"{pair[0]} - {pair[1]}\"\n",
    "                contrasts.append(con)\n",
    "\n",
    "            # Add the participant ID to the dataframes\n",
    "            roi[\"Participant\"] = cha[\"Participant\"] = i\n",
    "            for con in contrasts:\n",
    "                con[\"Participant\"] = i\n",
    "\n",
    "            # Convert to uM for nicer plotting below.\n",
    "            cha[\"theta\"] = [t * 1.0e6 for t in cha[\"theta\"]]\n",
    "            roi[\"theta\"] = [t * 1.0e6 for t in roi[\"theta\"]]\n",
    "            for con in contrasts:\n",
    "                con[\"effect\"] = [t * 1.0e6 for t in con[\"effect\"]]\n",
    "\n",
    "            # Append the dataframes to the main dataframes\n",
    "            cha_df = pd.concat([cha_df, cha])\n",
    "            roi_df = pd.concat([roi_df, roi])\n",
    "            for con in contrasts:\n",
    "                con_df = pd.concat([con_df, con])\n",
    "\n",
    "        cha_df.to_csv('processed_data/glm/cha/cha_df_' + mode + '.csv', index=False)\n",
    "        roi_df.to_csv('processed_data/glm/roi/roi_df_' + mode + '.csv', index=False)\n",
    "        con_df.to_csv('processed_data/glm/cons/con_df_' + mode + '.csv', index=False)\n",
    "\n",
    "# load the dataframes\n",
    "glm = {\n",
    "    mode: {\n",
    "        'cha': pd.read_csv(f'processed_data/glm/cha/cha_df_{mode}.csv'),\n",
    "        'roi': pd.read_csv(f'processed_data/glm/roi/roi_df_{mode}.csv'),\n",
    "        'con': pd.read_csv(f'processed_data/glm/cons/con_df_{mode}.csv')\n",
    "    }\n",
    "    for mode in modes\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual GLM Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_ind_glm_plots:\n",
    "    channel_types = ['hbo', 'hbr', 'hbt']\n",
    "    for mode in modes:\n",
    "        grp_results = glm[mode]['roi'].query(f\"Condition in {conditions_list[mode]}\")\n",
    "        grp_results = grp_results.query(f\"Chroma in {channel_types}\")\n",
    "\n",
    "        theta_min = grp_results['theta'].min()\n",
    "        theta_max = grp_results['theta'].max()\n",
    "\n",
    "        # clear any files in the plots/glm/individual folder\n",
    "        for f in os.listdir('plots/glm/individual_' + mode):\n",
    "            os.remove(os.path.join('plots/glm/individual_' + mode, f))\n",
    "\n",
    "        for i in grp_results['Participant'].unique():\n",
    "            fig, axes = plt.subplots(1, len(channel_types), figsize=(18, 6), sharey=True)\n",
    "            fig.suptitle(f'GLM Results for Participant {i}')\n",
    "\n",
    "            for ax, channel_type in zip(axes, channel_types):\n",
    "                sns.swarmplot(data=grp_results.query(f\"Participant == {i} and Chroma == '{channel_type}'\"), \n",
    "                              x='Condition', y='theta', hue='ROI', ax=ax, dodge=False)\n",
    "                ax.set_title(f'{channel_type}')\n",
    "                ax.set_ylabel('Theta (uM)')\n",
    "                ax.set_ylim(theta_min, theta_max)\n",
    "                ax.set_xlabel('Condition')\n",
    "                ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "            plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "            plt.savefig(f'plots/glm/individual_{mode}/Participant {i}.png', dpi=dpi / 4)\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group GLM Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_group_glm_plots:\n",
    "    channel_types = ['hbo', 'hbr', 'hbt']\n",
    "    for mode in modes:\n",
    "        grp_results = glm[mode]['roi'].query(f\"Condition in {conditions_list[mode]}\")\n",
    "        grp_results = grp_results.query(f\"Chroma in {channel_types}\")\n",
    "\n",
    "        # Run a GLM model\n",
    "        roi_model = mixedlm(\"theta ~ -1 + ROI:Condition:Chroma\", grp_results, groups=grp_results[\"Participant\"]).fit(method=\"nm\")\n",
    "\n",
    "        # Get the results of the model and put it in a csv file\n",
    "        roi_model_results = statsmodels_to_results(roi_model)\n",
    "\n",
    "        # plot the results of the model\n",
    "        fig, axes = plt.subplots(1, len(channel_types), figsize=(18, 6), sharey=True)\n",
    "        fig.suptitle(f'GLM Results for Group')\n",
    "\n",
    "        for ax, channel_type in zip(axes, channel_types):\n",
    "            sns.swarmplot(data=roi_model_results.query(f\"Chroma == '{channel_type}'\"), x='Condition', y='Coef.', hue='ROI', ax=ax, dodge=False)\n",
    "            ax.set_title(f'{channel_type}')\n",
    "            ax.set_ylabel('Theta (uM)')\n",
    "            ax.set_xlabel('Condition')\n",
    "            ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        \n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        plt.savefig(f'plots/glm/group_results/results_{mode}.png', dpi=dpi / 4)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Contrasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add hbt as plot_glm_group_topo does not support hbt\n",
    "\n",
    "if get_group_contrast_plots:\n",
    "    channel_types = ['hbo', 'hbr']\n",
    "    for mode in modes:\n",
    "        pairs = list(itertools.combinations(conditions_list[mode], 2))\n",
    "        for pair in pairs:\n",
    "            con_summary = glm[mode]['con'].query(f\"contrast_pair == '{pair[0]} - {pair[1]}'\")\n",
    "\n",
    "            if len(con_summary) == 0:\n",
    "                print(f\"No data for contrast {pair[0]} - {pair[1]}\")\n",
    "                continue\n",
    "\n",
    "            fig, axes = plt.subplots(1, len(channel_types), figsize=(18, 6))\n",
    "            fig.suptitle(f\"Contrast: {pair[0]} - {pair[1]}\")\n",
    "\n",
    "            for ax, channel_type in zip(axes, channel_types):\n",
    "                raw_haemo_channel = pick_channels(raw_haemo, channel_type)\n",
    "\n",
    "                con_summary_channel = con_summary.query(f\"Chroma in {[channel_type]}\")\n",
    "\n",
    "                # Run group level model and convert to dataframe\n",
    "                con_model = mixedlm(\"effect ~ -1 + ch_name:Chroma\", con_summary_channel, groups=con_summary_channel[\"Participant\"]).fit(method=\"nm\")\n",
    "\n",
    "                con_model_df = statsmodels_to_results(con_model, order=raw_haemo_channel.ch_names)\n",
    "                vlim = max(abs(con_model_df['Coef.'].min()), abs(con_model_df['Coef.'].max()))\n",
    "                vlim = (-vlim, vlim)\n",
    "\n",
    "                plot_glm_group_topo(\n",
    "                    raw_haemo_channel, con_model_df.query(f\"Chroma == '{channel_type}'\"), colorbar=True, extrapolate=\"head\", threshold=True, vlim=vlim, axes=ax\n",
    "                )\n",
    "                ax.set_title(f'{channel_type}')\n",
    "\n",
    "            plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "            if \"Neutral\" in pair:\n",
    "                plt.savefig(f'plots/glm/contrasts/differences_neutral/Contrast_{pair[0]}-{pair[1]}.png', dpi=dpi / 4)\n",
    "            else:\n",
    "                plt.savefig(f'plots/glm/contrasts/differences/Contrast_{pair[0]}-{pair[1]}.png', dpi=dpi / 4)\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROI Timeseries Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_roi_timeseries_activity:\n",
    "    channel_types = ['hbo', 'hbr', 'hbt']\n",
    "    \n",
    "    # create an empty dataframe\n",
    "    roi_timeseries_activity = pd.DataFrame()\n",
    "\n",
    "    tmin = 4\n",
    "    tmax = 16\n",
    "\n",
    "    i = 1\n",
    "    for raw_haemo in raw_haemo_good_recordings:\n",
    "        face_epochs = relabel_annotations(raw_haemo.copy(), mode='face_type')\n",
    "        emotion_epochs = relabel_annotations(raw_haemo.copy(), mode='emotion')\n",
    "\n",
    "        # crop the epochs to tmin-tmax\n",
    "        face_epochs.crop(tmin=tmin, tmax=tmax)\n",
    "\n",
    "        # convert the epochs to a dataframe\n",
    "        face_epochs_df = face_epochs.to_data_frame()\n",
    "\n",
    "        # remove the baseline condition\n",
    "        face_epochs_df = face_epochs_df.where(face_epochs_df['condition'] != 'Base').dropna()\n",
    "\n",
    "        # average these columns: column_names[3:].tolist(), by the epoch column and condition column\n",
    "        face_epochs_df = face_epochs_df.groupby(['epoch', 'condition']).mean().reset_index()\n",
    "\n",
    "        # crop the epochs to tmix-tmax\n",
    "        emotion_epochs.crop(tmin=tmin, tmax=tmax)\n",
    "\n",
    "        # convert the epochs to a dataframe\n",
    "        emotion_epochs_df = emotion_epochs.to_data_frame()\n",
    "\n",
    "        # remove the baseline condition\n",
    "        emotion_epochs_df = emotion_epochs_df.where(emotion_epochs_df['condition'] != 'Base').dropna()\n",
    "\n",
    "        # average these columns: column_names[3:].tolist(), by the epoch column and condition column\n",
    "        emotion_epochs_df = emotion_epochs_df.groupby(['epoch', 'condition']).mean().reset_index()\n",
    "\n",
    "        # add the condition column from the emotion_epochs_df to the face_epochs_df and line it up with the epoch column\n",
    "        face_epochs_df['emotion'] = emotion_epochs_df['condition']\n",
    "\n",
    "        # put the emotion column in the third column\n",
    "        all_epochs_df = face_epochs_df[['epoch', 'condition', 'emotion'] + face_epochs_df.columns[2:-1].tolist()]\n",
    "\n",
    "        # rename the condition column to face type\n",
    "        all_epochs_df.rename(columns={'condition': 'face type'}, inplace=True)\n",
    "\n",
    "        # divide the epoch column by 2 and floor it and convert it to an integer\n",
    "        all_epochs_df['epoch'] = (all_epochs_df['epoch'] // 2).astype(int)\n",
    "\n",
    "        # remove the time column\n",
    "        all_epochs_df.drop(columns='time', inplace=True)\n",
    "\n",
    "        if 'hbo' in channel_types:\n",
    "            for region, channels in ch_mapping_hbo.items():\n",
    "                # Ensure the channels exist in the dataframe to avoid errors\n",
    "                valid_channels = [channel for channel in channels if channel in all_epochs_df.columns]\n",
    "                if valid_channels:\n",
    "                    # Create a new column for the region's average\n",
    "                    all_epochs_df[region + ' Average Hbo'] = all_epochs_df[valid_channels].mean(axis=1)\n",
    "\n",
    "        if 'hbr' in channel_types:\n",
    "            for region, channels in ch_mapping_hbr.items():\n",
    "                # Ensure the channels exist in the dataframe to avoid errors\n",
    "                valid_channels = [channel for channel in channels if channel in all_epochs_df.columns]\n",
    "                if valid_channels:\n",
    "                    # Create a new column for the region's average\n",
    "                    all_epochs_df[region + ' Average Hbr'] = all_epochs_df[valid_channels].mean(axis=1)\n",
    "\n",
    "        if 'hbt' in channel_types:\n",
    "            for region, channels in ch_mapping_hbt.items():\n",
    "                # Ensure the channels exist in the dataframe to avoid errors\n",
    "                valid_channels = [channel for channel in channels if channel in all_epochs_df.columns]\n",
    "                if valid_channels:\n",
    "                    # Create a new column for the region's average\n",
    "                    all_epochs_df[region + ' Average Hbt'] = all_epochs_df[valid_channels].mean(axis=1)\n",
    "\n",
    "        # drop all the channel columns\n",
    "        all_epochs_df.drop(columns=all_channels, inplace=True)\n",
    "\n",
    "        # add participant number column\n",
    "        all_epochs_df['Participant'] = i\n",
    "\n",
    "        # make the participant number the first column\n",
    "        all_epochs_df = all_epochs_df[['Participant'] + all_epochs_df.columns[:-1].tolist()]\n",
    "\n",
    "        # add measurement date column\n",
    "        #all_epochs_df['Measurement Date'] = raw_haemo.info['meas_date']\n",
    "\n",
    "        # add an empty column for repetition and put it after the emotion column\n",
    "        all_epochs_df.insert(4, 'Repetition', '')\n",
    "\n",
    "        conditions = defaultdict(int)\n",
    "        for index, row in all_epochs_df.iterrows():\n",
    "            # add the condition-emotion pair to the conditions dictionary and increment the count\n",
    "            conditions[f\"{row['face type']}-{row['emotion']}\"] += 1\n",
    "\n",
    "            # add the count to the repetition column\n",
    "            all_epochs_df.at[index, 'Repetition'] = conditions[f\"{row['face type']}-{row['emotion']}\"]\n",
    "\n",
    "        # put it after the repetition column\n",
    "        all_epochs_df.insert(5, 'Sex', '')\n",
    "\n",
    "        # add the sex column\n",
    "        all_epochs_df['Sex'] = get_info(raw_haemo)['gender']\n",
    "\n",
    "        # add the dataframe to the average_timeseries_activity datafram\n",
    "        roi_timeseries_activity = pd.concat([roi_timeseries_activity, all_epochs_df])\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    # reset the index\n",
    "    roi_timeseries_activity.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # name the index column to 'observation'\n",
    "    roi_timeseries_activity.index.name = 'Observation'\n",
    "\n",
    "    # replace any spaces in the column names with underscores\n",
    "    roi_timeseries_activity.columns = roi_timeseries_activity.columns.str.replace(' ', '_')\n",
    "\n",
    "    # capitalize the column names\n",
    "    roi_timeseries_activity.columns = roi_timeseries_activity.columns.str.capitalize()\n",
    "\n",
    "    mappings = {}\n",
    "    for col in roi_timeseries_activity.select_dtypes(include=['object']).columns:\n",
    "        # Create a mapping dictionary for the column\n",
    "        unique_values = roi_timeseries_activity[col].unique()\n",
    "        col_mapping = {val: idx for idx, val in enumerate(unique_values)}\n",
    "        mappings[col] = col_mapping\n",
    "        \n",
    "        # Replace the column values in the DataFrame with numeric values\n",
    "        roi_timeseries_activity[col] = roi_timeseries_activity[col].map(col_mapping)\n",
    "\n",
    "    # Save mappings to a JSON file\n",
    "    with open('processed_data/roi_timeseries_activity/mappings.json', 'w') as json_file:\n",
    "        json.dump(mappings, json_file, indent=4)\n",
    "\n",
    "    # save the dataframe to a csv file\n",
    "    roi_timeseries_activity.to_csv('processed_data/roi_timeseries_activity/roi_timeseries_activity.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Epochs for all Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_35796\\939854204.py:12: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  \"Real\": mne.concatenate_epochs([epochs['Real'] for epochs in all_epochs_faces]),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "1092 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_35796\\939854204.py:13: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  \"Virtual\": mne.concatenate_epochs([epochs['Virt'] for epochs in all_epochs_faces]),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "1092 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_35796\\939854204.py:14: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  \"Joy\": mne.concatenate_epochs([epochs['Joy'] for epochs in all_epochs_emotions]),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "312 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_35796\\939854204.py:15: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  \"Fear\": mne.concatenate_epochs([epochs['Fear'] for epochs in all_epochs_emotions]),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "312 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_35796\\939854204.py:16: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  \"Anger\": mne.concatenate_epochs([epochs['Anger'] for epochs in all_epochs_emotions]),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "312 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_35796\\939854204.py:17: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  \"Disgust\": mne.concatenate_epochs([epochs['Disgust'] for epochs in all_epochs_emotions]),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "312 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_35796\\939854204.py:18: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  \"Sadness\": mne.concatenate_epochs([epochs['Sadness'] for epochs in all_epochs_emotions]),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "312 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_35796\\939854204.py:19: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  \"Neutral\": mne.concatenate_epochs([epochs['Neutral'] for epochs in all_epochs_emotions]),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "312 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_35796\\939854204.py:20: RuntimeWarning: Concatenation of Annotations within Epochs is not supported yet. All annotations will be dropped.\n",
      "  \"Surprise\": mne.concatenate_epochs([epochs['Surprise'] for epochs in all_epochs_emotions])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "312 matching events found\n",
      "Applying baseline correction (mode: mean)\n"
     ]
    }
   ],
   "source": [
    "all_epochs_faces = []\n",
    "all_epochs_emotions = []\n",
    "\n",
    "for raw_haemo in raw_haemo_good_recordings:\n",
    "    raw_haemo.reorder_channels(all_channels)\n",
    "    face_epochs = relabel_annotations(raw_haemo.copy(), mode='face_type')\n",
    "    emotion_epochs = relabel_annotations(raw_haemo.copy(), mode='emotion')\n",
    "    all_epochs_faces.append(face_epochs)\n",
    "    all_epochs_emotions.append(emotion_epochs)\n",
    "\n",
    "all_epochs = {\n",
    "        \"Real\": mne.concatenate_epochs([epochs['Real'] for epochs in all_epochs_faces]),\n",
    "        \"Virtual\": mne.concatenate_epochs([epochs['Virt'] for epochs in all_epochs_faces]),\n",
    "        \"Joy\": mne.concatenate_epochs([epochs['Joy'] for epochs in all_epochs_emotions]),\n",
    "        \"Fear\": mne.concatenate_epochs([epochs['Fear'] for epochs in all_epochs_emotions]),\n",
    "        \"Anger\": mne.concatenate_epochs([epochs['Anger'] for epochs in all_epochs_emotions]),\n",
    "        \"Disgust\": mne.concatenate_epochs([epochs['Disgust'] for epochs in all_epochs_emotions]),\n",
    "        \"Sadness\": mne.concatenate_epochs([epochs['Sadness'] for epochs in all_epochs_emotions]),\n",
    "        \"Neutral\": mne.concatenate_epochs([epochs['Neutral'] for epochs in all_epochs_emotions]),\n",
    "        \"Surprise\": mne.concatenate_epochs([epochs['Surprise'] for epochs in all_epochs_emotions])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERP Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add hbt\n",
    "\n",
    "if get_erp_mean_regions_plots or get_erp_per_region_plots:\n",
    "    include_hbr = False\n",
    "\n",
    "    # set the y-axis limit\n",
    "    lims = dict(hbo=[-6, 6], hbr=[-6, 6])\n",
    "\n",
    "    # Get all emotions in the dataset\n",
    "    emotions = [\"Joy\", \"Fear\", \"Anger\", \"Disgust\", \"Sadness\", \"Neutral\", \"Surprise\"]\n",
    "\n",
    "    # Create pairwise combinations of emotions\n",
    "    emotion_pairs = list(itertools.combinations(emotions, 2))\n",
    "\n",
    "    if include_hbr:\n",
    "        color_dict_faces = dict(RealHbo=\"r\", RealHbr=\"r\", VirtualHbo=\"b\", VirtualHbr=\"b\")\n",
    "        styles_dict_faces = dict(RealHbo=dict(linestyle=\"solid\"), RealHbr=dict(linestyle=\"dotted\"), VirtualHbo=dict(linestyle=\"solid\"), VirtualHbr=dict(linestyle=\"dotted\"))\n",
    "\n",
    "        evoked_dict_faces = dict(\n",
    "        RealHbo=list(all_epochs[\"Real\"].pick(picks=\"hbo\").iter_evoked()),\n",
    "        RealHbr=list(all_epochs[\"Real\"].pick(picks=\"hbr\").iter_evoked()),\n",
    "        VirtualHbo=list(all_epochs[\"Virtual\"].pick(picks=\"hbo\").iter_evoked()),\n",
    "        VirtualHbr=list(all_epochs[\"Virtual\"].pick(picks=\"hbr\").iter_evoked()),\n",
    "        )\n",
    "\n",
    "        evoked_dict_emotions = dict(\n",
    "        JoyHbo=list(all_epochs[\"Joy\"].pick(picks=\"hbo\").iter_evoked()),\n",
    "        JoyHbr=all_epochs[\"Joy\"].pick(picks=\"hbr\"),\n",
    "        FearHbo=list(all_epochs[\"Fear\"].pick(picks=\"hbo\").iter_evoked()),\n",
    "        FearHbr=all_epochs[\"Fear\"].pick(picks=\"hbr\"),\n",
    "        AngerHbo=list(all_epochs[\"Anger\"].pick(picks=\"hbo\").iter_evoked()),\n",
    "        AngerHbr=all_epochs[\"Anger\"].pick(picks=\"hbr\"),\n",
    "        DisgustHbo=list(all_epochs[\"Disgust\"].pick(picks=\"hbo\").iter_evoked()),\n",
    "        DisgustHbr=all_epochs[\"Disgust\"].pick(picks=\"hbr\"),\n",
    "        SadnessHbo=list(all_epochs[\"Sadness\"].pick(picks=\"hbo\").iter_evoked()),\n",
    "        SadnessHbr=all_epochs[\"Sadness\"].pick(picks=\"hbr\"),\n",
    "        NeutralHbo=list(all_epochs[\"Neutral\"].pick(picks=\"hbo\").iter_evoked()),\n",
    "        NeutralHbr=all_epochs[\"Neutral\"].pick(picks=\"hbr\"),\n",
    "        SurpriseHbo=list(all_epochs[\"Surprise\"].pick(picks=\"hbo\").iter_evoked()),\n",
    "        SurpriseHbr=all_epochs[\"Surprise\"].pick(picks=\"hbr\"),\n",
    "        )\n",
    "\n",
    "        color_dict_emotions = dict(\n",
    "        JoyHbo=\"yellow\",        # A bright and classic representation of happiness\n",
    "        JoyHbr=\"yellow\",        # Same as JoyHbo\n",
    "        FearHbo=\"purple\",       # Associated with tension or mystery\n",
    "        FearHbr=\"purple\",       # Same as FearHbo\n",
    "        AngerHbo=\"red\",         # Classic association with anger\n",
    "        AngerHbr=\"red\",         # Same as AngerHbo\n",
    "        DisgustHbo=\"green\",     # Green, but brighter to differentiate from olive\n",
    "        DisgustHbr=\"green\",     # Same as DisgustHbo\n",
    "        SadnessHbo=\"blue\",      # Commonly associated with sadness\n",
    "        SadnessHbr=\"blue\",      # Same as SadnessHbo\n",
    "        NeutralHbo=\"gray\",      # Neutral tones\n",
    "        NeutralHbr=\"gray\",      # Same as NeutralHbo\n",
    "        SurpriseHbo=\"orange\",   # Bright, attention-grabbing orange\n",
    "        SurpriseHbr=\"orange\"    # Same as SurpriseHbo\n",
    "        )\n",
    "\n",
    "        styles_dict_emotions = dict(\n",
    "        JoyHbo=dict(linestyle=\"solid\"),\n",
    "        JoyHbr=dict(linestyle=\"dotted\"),\n",
    "        FearHbo=dict(linestyle=\"solid\"),\n",
    "        FearHbr=dict(linestyle=\"dotted\"),\n",
    "        AngerHbo=dict(linestyle=\"solid\"),\n",
    "        AngerHbr=dict(linestyle=\"dotted\"),\n",
    "        DisgustHbo=dict(linestyle=\"solid\"),\n",
    "        DisgustHbr=dict(linestyle=\"dotted\"),\n",
    "        SadnessHbo=dict(linestyle=\"solid\"),\n",
    "        SadnessHbr=dict(linestyle=\"dotted\"),\n",
    "        NeutralHbo=dict(linestyle=\"solid\"),\n",
    "        NeutralHbr=dict(linestyle=\"dotted\"),\n",
    "        SurpriseHbo=dict(linestyle=\"solid\"),\n",
    "        SurpriseHbr=dict(linestyle=\"dotted\"),\n",
    "        )\n",
    "    else:\n",
    "        color_dict_faces = dict(RealHbo=\"r\", VirtualHbo=\"b\")\n",
    "        styles_dict_faces = dict(RealHbo=dict(linestyle=\"solid\"), VirtualHbo=dict(linestyle=\"solid\"))\n",
    "\n",
    "        evoked_dict_faces = dict(\n",
    "        RealHbo=list(all_epochs[\"Real\"].pick(picks=\"hbo\").iter_evoked()),\n",
    "        VirtualHbo=list(all_epochs[\"Virtual\"].pick(picks=\"hbo\").iter_evoked()),\n",
    "        )\n",
    "\n",
    "        evoked_dict_emotions = dict(\n",
    "        JoyHbo=list(all_epochs[\"Joy\"].pick(picks=\"hbo\").iter_evoked()),\n",
    "        FearHbo=list(all_epochs[\"Fear\"].pick(picks=\"hbo\").iter_evoked()),\n",
    "        AngerHbo=list(all_epochs[\"Anger\"].pick(picks=\"hbo\").iter_evoked()),\n",
    "        DisgustHbo=list(all_epochs[\"Disgust\"].pick(picks=\"hbo\").iter_evoked()),\n",
    "        SadnessHbo=list(all_epochs[\"Sadness\"].pick(picks=\"hbo\").iter_evoked()),\n",
    "        NeutralHbo=list(all_epochs[\"Neutral\"].pick(picks=\"hbo\").iter_evoked()),\n",
    "        SurpriseHbo=list(all_epochs[\"Surprise\"].pick(picks=\"hbo\").iter_evoked()),\n",
    "        )\n",
    "\n",
    "        color_dict_emotions = dict(\n",
    "        JoyHbo=\"yellow\",        # A bright and classic representation of happiness\n",
    "        FearHbo=\"purple\",       # Associated with tension or mystery\n",
    "        AngerHbo=\"red\",         # Classic association with anger\n",
    "        DisgustHbo=\"green\",     # Green, but brighter to differentiate from olive\n",
    "        SadnessHbo=\"blue\",      # Commonly associated with sadness\n",
    "        NeutralHbo=\"gray\",      # Neutral tones\n",
    "        SurpriseHbo=\"orange\"    # Bright, attention-grabbing orange\n",
    "        )\n",
    "\n",
    "        styles_dict_emotions = dict(\n",
    "        JoyHbo=dict(linestyle=\"solid\"),\n",
    "        FearHbo=dict(linestyle=\"solid\"),\n",
    "        AngerHbo=dict(linestyle=\"solid\"),\n",
    "        DisgustHbo=dict(linestyle=\"solid\"),\n",
    "        SadnessHbo=dict(linestyle=\"solid\"),\n",
    "        NeutralHbo=dict(linestyle=\"solid\"),\n",
    "        SurpriseHbo=dict(linestyle=\"solid\"),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERP Plots Mean Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_erp_mean_regions_plots:\n",
    "    # Plot the real vs virtual evoked responses\n",
    "    mne.viz.plot_compare_evokeds(\n",
    "        evoked_dict_faces,\n",
    "        combine=\"mean\",\n",
    "        ci=0.95,\n",
    "        colors=color_dict_faces,\n",
    "        styles=styles_dict_faces,\n",
    "        show=False,\n",
    "        ylim=lims,\n",
    "        title=\"Real vs Virtual\",\n",
    "        legend=\"lower left\",\n",
    "        truncate_yaxis=True,\n",
    "    )\n",
    "\n",
    "    plt.savefig(f'plots/erp/mean_all_regions/Real vs Virtual_All Regions.png', dpi=dpi)\n",
    "    plt.close()\n",
    "\n",
    "    # Iterate through each pair of emotions and plot them\n",
    "    for emo1, emo2 in emotion_pairs:\n",
    "        # Create a sub-dictionary for the current pair\n",
    "        evoked_pair_dict = {\n",
    "            f\"{emo1}Hbo\": evoked_dict_emotions[f\"{emo1}Hbo\"],\n",
    "            #f\"{emo1}Hbr\": evoked_dict_emotions[f\"{emo1}Hbr\"],\n",
    "            f\"{emo2}Hbo\": evoked_dict_emotions[f\"{emo2}Hbo\"],\n",
    "            #f\"{emo2}Hbr\": evoked_dict_emotions[f\"{emo2}Hbr\"],\n",
    "        }\n",
    "\n",
    "        # Create a color and style dictionary for the current pair\n",
    "        color_pair_dict = {key: color_dict_emotions[key] for key in evoked_pair_dict}\n",
    "        styles_pair_dict = {key: styles_dict_emotions[key] for key in evoked_pair_dict}\n",
    "\n",
    "        # Plot the pair\n",
    "        mne.viz.plot_compare_evokeds(\n",
    "            evoked_pair_dict,\n",
    "            combine=\"mean\",\n",
    "            ci=0.95,\n",
    "            show=False,\n",
    "            colors=color_pair_dict,\n",
    "            styles=styles_pair_dict,\n",
    "            ylim=lims,\n",
    "            title=f\"{emo1} vs {emo2}\",\n",
    "            legend=\"lower left\",\n",
    "            truncate_yaxis=True,\n",
    "        )\n",
    "\n",
    "        if emo1 == \"Neutral\" or emo2 == \"Neutral\":\n",
    "            plt.savefig(f'plots/erp/mean_all_regions_neutral/{emo1} vs {emo2}_All Regions.png', dpi=dpi)\n",
    "        else:\n",
    "            plt.savefig(f'plots/erp/mean_all_regions/{emo1} vs {emo2}_All Regions.png', dpi=dpi)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ERP Plots Per Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_erp_per_region_plots:\n",
    "    # Iterate through each region in ch_mapping\n",
    "    for region, channels in ch_mapping_hbo.items():\n",
    "        # Get the indices for the current region from group_boundaries\n",
    "        keys_list = list(ch_mapping_hbo.keys())\n",
    "        start_idx = group_boundaries[keys_list.index(region)]\n",
    "        end_idx = group_boundaries[keys_list.index(region) + 1] if keys_list.index(region) + 1 < len(group_boundaries) else None\n",
    "        \n",
    "        # Filter the epochs for the current region\n",
    "        region_evoked_faces = {\n",
    "            key: [evoked.copy().pick(evoked.ch_names[start_idx:end_idx]) for evoked in evokeds]\n",
    "            for key, evokeds in evoked_dict_faces.items()\n",
    "        }\n",
    "        region_evoked_emotions = {\n",
    "            key: [evoked.copy().pick(evoked.ch_names[start_idx:end_idx]) for evoked in evokeds]\n",
    "            for key, evokeds in evoked_dict_emotions.items()\n",
    "        }\n",
    "\n",
    "        # Plot Real vs Virtual for the current region\n",
    "        mne.viz.plot_compare_evokeds(\n",
    "            region_evoked_faces,\n",
    "            combine=\"mean\",\n",
    "            ci=0.95,\n",
    "            colors=color_dict_faces,\n",
    "            styles=styles_dict_faces,\n",
    "            show=False,\n",
    "            ylim=lims,\n",
    "            title=f\"Real vs Virtual - {region}\",\n",
    "            legend=\"lower left\",\n",
    "            truncate_yaxis=True,\n",
    "        )\n",
    "        plt.savefig(f'plots/erp/per_region/Real_vs_Virtual_{region}.png', dpi=dpi)\n",
    "        plt.close()\n",
    "\n",
    "        # Plot each pair of emotions for the current region\n",
    "        for emo1, emo2 in emotion_pairs:\n",
    "            # Create a sub-dictionary for the current pair\n",
    "            evoked_pair_dict = {\n",
    "                f\"{emo1}Hbo\": region_evoked_emotions[f\"{emo1}Hbo\"],\n",
    "                #f\"{emo1}Hbr\": region_evoked_emotions[f\"{emo1}Hbr\"],\n",
    "                f\"{emo2}Hbo\": region_evoked_emotions[f\"{emo2}Hbo\"],\n",
    "                #f\"{emo2}Hbr\": region_evoked_emotions[f\"{emo2}Hbr\"],\n",
    "            }\n",
    "\n",
    "            # Create a color and style dictionary for the current pair\n",
    "            color_pair_dict = {key: color_dict_emotions[key] for key in evoked_pair_dict}\n",
    "            styles_pair_dict = {key: styles_dict_emotions[key] for key in evoked_pair_dict}\n",
    "\n",
    "            # Plot the pair\n",
    "            mne.viz.plot_compare_evokeds(\n",
    "                evoked_pair_dict,\n",
    "                combine=\"mean\",\n",
    "                ci=0.95,\n",
    "                show=False,\n",
    "                colors=color_pair_dict,\n",
    "                styles=styles_pair_dict,\n",
    "                ylim=lims,\n",
    "                title=f\"{emo1} vs {emo2} - {region}\",\n",
    "                legend=\"lower left\",\n",
    "                truncate_yaxis=True,\n",
    "            )\n",
    "\n",
    "            if emo1 == \"Neutral\" or emo2 == \"Neutral\":\n",
    "                plt.savefig(f'plots/erp/per_region_neutral/{emo1}_vs_{emo2}_{region}.png', dpi=dpi)\n",
    "            else:\n",
    "                plt.savefig(f'plots/erp/per_region/{emo1}_vs_{emo2}_{region}.png', dpi=dpi)\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topographic Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add hbt\n",
    "if get_topo_condition_plots:\n",
    "    if include_hbr:\n",
    "        evoked_dict_topomap = {\n",
    "            \"Real (Hbo)\": all_epochs[\"Real\"].average(picks=\"hbo\"),\n",
    "            \"RealHbr\": all_epochs[\"Real\"].average(picks=\"hbr\"),\n",
    "            \"Virtual (Hbo)\": all_epochs[\"Virtual\"].average(picks=\"hbo\"),\n",
    "            \"VirtualHbr\": all_epochs[\"Virtual\"].average(picks=\"hbr\"),\n",
    "            \"Joy (Hbo)\": all_epochs[\"Joy\"].average(picks=\"hbo\"),\n",
    "            \"JoyHbr\": all_epochs[\"Joy\"].average(picks=\"hbr\"),\n",
    "            \"Fear (Hbo)\": all_epochs[\"Fear\"].average(picks=\"hbo\"),\n",
    "            \"FearHbr\": all_epochs[\"Fear\"].average(picks=\"hbr\"),\n",
    "            \"Anger (Hbo)\": all_epochs[\"Anger\"].average(picks=\"hbo\"),\n",
    "            \"AngerHbr\": all_epochs[\"Anger\"].average(picks=\"hbr\"),\n",
    "            \"Disgust (Hbo)\": all_epochs[\"Disgust\"].average(picks=\"hbo\"),\n",
    "            \"DisgustHbr\": all_epochs[\"Disgust\"].average(picks=\"hbr\"),\n",
    "            \"Sadness (Hbo)\": all_epochs[\"Sadness\"].average(picks=\"hbo\"),\n",
    "            \"SadnessHbr\": all_epochs[\"Sadness\"].average(picks=\"hbr\"),\n",
    "            \"Neutral (Hbo)\": all_epochs[\"Neutral\"].average(picks=\"hbo\"),\n",
    "            \"NeutralHbr\": all_epochs[\"Neutral\"].average(picks=\"hbr\"),\n",
    "            \"Surprise (Hbo)\": all_epochs[\"Surprise\"].average(picks=\"hbo\"),\n",
    "            \"SurpriseHbr\": all_epochs[\"Surprise\"].average(picks=\"hbr\")\n",
    "        }\n",
    "    else:\n",
    "        evoked_dict_topomap = {\n",
    "            \"Real (Hbo)\": all_epochs[\"Real\"].average(picks=\"hbo\"),\n",
    "            \"Virtual (Hbo)\": all_epochs[\"Virtual\"].average(picks=\"hbo\"),\n",
    "            \"Joy (Hbo)\": all_epochs[\"Joy\"].average(picks=\"hbo\"),\n",
    "            \"Fear (Hbo)\": all_epochs[\"Fear\"].average(picks=\"hbo\"),\n",
    "            \"Anger (Hbo)\": all_epochs[\"Anger\"].average(picks=\"hbo\"),\n",
    "            \"Disgust (Hbo)\": all_epochs[\"Disgust\"].average(picks=\"hbo\"),\n",
    "            \"Sadness (Hbo)\": all_epochs[\"Sadness\"].average(picks=\"hbo\"),\n",
    "            \"Neutral (Hbo)\": all_epochs[\"Neutral\"].average(picks=\"hbo\"),\n",
    "            \"Surprise (Hbo)\": all_epochs[\"Surprise\"].average(picks=\"hbo\")\n",
    "        }\n",
    "\n",
    "    for condition in evoked_dict_topomap:\n",
    "        evoked_dict_topomap[condition].plot_topomap(\n",
    "            times=[8],\n",
    "            average=16,\n",
    "            extrapolate=\"head\",\n",
    "            colorbar=True,\n",
    "            size=2,\n",
    "            vlim=(-15, 15),\n",
    "            show=False\n",
    "        )\n",
    "        plt.suptitle(f'{condition}')\n",
    "        plt.savefig(f'plots/topomaps/average for all 16/{condition}.png', dpi=dpi)\n",
    "        plt.close()\n",
    "\n",
    "    for condition in evoked_dict_topomap:\n",
    "        evoked_dict_topomap[condition].plot_topomap(\n",
    "            times=[4, 8, 12, 16],\n",
    "            average=4,\n",
    "            extrapolate=\"head\",\n",
    "            colorbar=True,\n",
    "            size=2,\n",
    "            vlim=(-15, 15),\n",
    "            show=False\n",
    "        )\n",
    "        plt.suptitle(f'{condition}')\n",
    "        plt.savefig(f'plots/topomaps/4-8-12-16/{condition}.png', dpi=dpi)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topographic Difference Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_topo_diff_plots:\n",
    "    evoked_dict_differences_topomap = {\n",
    "        \"Real - Virtual (Hbo)\": mne.combine_evoked([evoked_dict_topomap[\"Real (Hbo)\"], evoked_dict_topomap[\"Virtual (Hbo)\"]], weights=[1, -1]),\n",
    "        \"Virtual - Real (Hbo)\": mne.combine_evoked([evoked_dict_topomap[\"Virtual (Hbo)\"], evoked_dict_topomap[\"Real (Hbo)\"]], weights=[1, -1]),\n",
    "    }\n",
    "\n",
    "    # Get all emotions in the dataset\n",
    "    emotions = [\"Joy\", \"Fear\", \"Anger\", \"Disgust\", \"Sadness\", \"Neutral\", \"Surprise\"]\n",
    "\n",
    "    for emotion in itertools.combinations(emotions, 2):\n",
    "        evoked_dict_differences_topomap[f\"{emotion[0]} - {emotion[1]} (Hbo)\"] = mne.combine_evoked(\n",
    "            [evoked_dict_topomap[f\"{emotion[0]} (Hbo)\"], evoked_dict_topomap[f\"{emotion[1]} (Hbo)\"]],\n",
    "            weights=[1, -1]\n",
    "        )\n",
    "        evoked_dict_differences_topomap[f\"{emotion[1]} - {emotion[0]} (Hbo)\"] = mne.combine_evoked(\n",
    "            [evoked_dict_topomap[f\"{emotion[1]} (Hbo)\"], evoked_dict_topomap[f\"{emotion[0]} (Hbo)\"]],\n",
    "            weights=[1, -1]\n",
    "        )\n",
    "\n",
    "    for condition in evoked_dict_differences_topomap:\n",
    "        evoked_dict_differences_topomap[condition].plot_topomap(\n",
    "            times=[8],\n",
    "            average=16,\n",
    "            extrapolate=\"head\",\n",
    "            colorbar=True,\n",
    "            size=2,\n",
    "            vlim=(-15, 15),\n",
    "            show=False\n",
    "        )\n",
    "        plt.title(f'{condition}')\n",
    "        if 'Neutral' in condition:\n",
    "            plt.savefig(f'plots/topomaps/average differences for all 16_neutral/{condition}.png', dpi=dpi)\n",
    "        else:\n",
    "            plt.savefig(f'plots/topomaps/average differences for all 16/{condition}.png', dpi=dpi)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual Connectivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = ['all', 'face_type', 'emotion']\n",
    "conditions_list = {\n",
    "    'all': ['Blck'],\n",
    "    'face_type': ['Real', 'Virt', 'Base'],\n",
    "    'emotion': ['Joy', 'Fear', 'Anger', 'Disgust', 'Sadness', 'Neutral', 'Surprise']\n",
    "}\n",
    "\n",
    "# pick the channels\n",
    "channel_types = ['hbo', 'hbr', 'hbt']\n",
    "\n",
    "# pick the connectivity method\n",
    "method = \"coh\"\n",
    "\n",
    "# pick the mode\n",
    "con_mode = \"cwt_morlet\"\n",
    "\n",
    "# pick the frequency range\n",
    "cwt_freqs = np.linspace(0.01, 0.5, 10)\n",
    "\n",
    "# pick the number of cycles\n",
    "cwt_n_cycles = 1\n",
    "\n",
    "# average the connectivity matrices across frequencies\n",
    "faverage = True\n",
    "\n",
    "if run_ind_connectivity:\n",
    "    # clear the processed_data\\connectivity\\individual_cons folder\n",
    "    for file in os.listdir(f'processed_data\\\\connectivity\\\\individual_cons'):\n",
    "        os.remove(f'processed_data\\\\connectivity\\\\individual_cons\\\\{file}')\n",
    "    \n",
    "    for mode in modes:\n",
    "        # for each raw_haemo in raw_haemo_good_recordings, compute the connectivity\n",
    "        for i, raw_haemo in enumerate(raw_haemo_good_recordings, 1):\n",
    "\n",
    "            # relabel the annotations\n",
    "            epochs = relabel_annotations(raw_haemo.copy(), mode=mode)\n",
    "\n",
    "            for condition in conditions_list[mode]:\n",
    "\n",
    "                # crop the epochs to the condition\n",
    "                epochs_cond = epochs[condition]\n",
    "\n",
    "                # create an empty list to store each channel type's connectivity\n",
    "                cons = []\n",
    "\n",
    "                # for each channel type in channel_types\n",
    "                for channel_type in channel_types:\n",
    "                    # pick the channels\n",
    "                    epochs_cond_channel = pick_channels(epochs_cond, channel_type)\n",
    "                \n",
    "                    # use spectral_connectivity_epochs to compute the connectivity\n",
    "                    con = spectral_connectivity_epochs(\n",
    "                        epochs_cond_channel,\n",
    "                        method=method,\n",
    "                        mode=con_mode,\n",
    "                        cwt_freqs=cwt_freqs,\n",
    "                        cwt_n_cycles=cwt_n_cycles,\n",
    "                        faverage=faverage,\n",
    "                        n_jobs=n_jobs,\n",
    "                        verbose=True\n",
    "                    )\n",
    "\n",
    "                    # append the connectivity to the cons list\n",
    "                    cons.append(con.get_data())\n",
    "                \n",
    "                # save the connectivity to disk\n",
    "                np.save(f\"processed_data\\\\connectivity\\\\individual_cons\\\\{mode}_{condition}_con_{i}.npy\", np.array(cons))\n",
    "\n",
    "    # make a dictionsary to store the connectivity parameters\n",
    "    ind_connectivity_params = {\n",
    "        \"channel_types\": channel_types,\n",
    "        \"method\": method,\n",
    "        \"con_mode\": con_mode,\n",
    "        \"cwt_freqs\": cwt_freqs.tolist(),\n",
    "        \"cwt_n_cycles\": cwt_n_cycles,\n",
    "        \"faverage\": faverage,\n",
    "        \"ch_names\": pick_channels(raw_haemo, channel_types).ch_names\n",
    "    }\n",
    "\n",
    "    # save the connectivity parameters to disk in preprocessed_data\\connectivity\n",
    "    with open(\"processed_data\\\\connectivity\\\\ind_connectivity_params.json\", \"w\") as f:\n",
    "        json.dump(ind_connectivity_params, f)\n",
    "\n",
    "# get number of .npy files in the processed_data\\connectivity\\individual_cons folder that has the substring 'Blck'\n",
    "num_cons = len([f for f in os.listdir('processed_data\\\\connectivity\\\\individual_cons') if 'Blck' in f])\n",
    "\n",
    "# load the numpy files from disk so we have ind_con[condition][mode][i] where i is the participant number and we get a (3, 10609, 1, 99) array\n",
    "ind_con = {\n",
    "    mode: {\n",
    "        condition: [\n",
    "            np.load(f'processed_data\\\\connectivity\\\\individual_cons\\\\{mode}_{condition}_con_{i}.npy')\n",
    "            for i in range(1, num_cons + 1)\n",
    "        ]\n",
    "        for condition in conditions_list[mode]\n",
    "    }\n",
    "    for mode in modes\n",
    "}\n",
    "\n",
    "# load the connectivity parameters from disk\n",
    "with open(\"processed_data\\\\connectivity\\\\ind_connectivity_params.json\", \"r\") as f:\n",
    "    ind_connectivity_params = json.load(f)\n",
    "\n",
    "dict_channel_types = {\n",
    "    channel_type: idx for idx, channel_type in enumerate(ind_connectivity_params[\"channel_types\"])\n",
    "}\n",
    "\n",
    "channel_type = 'hbo'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual Connectivity Heatmap Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_ind_con_plots:\n",
    "    for mode in modes:\n",
    "        for condition in conditions_list[mode]:\n",
    "            # check if a folder exists for the current mode and condition\n",
    "            if not os.path.exists(f'plots\\\\connectivity\\\\heatmaps\\\\individual\\\\{mode}_{condition}_cons'):\n",
    "                os.makedirs(f'plots\\\\connectivity\\\\heatmaps\\\\individual\\\\{mode}_{condition}_cons')\n",
    "            \n",
    "            # clear the folder\n",
    "            for file in os.listdir(f'plots\\\\connectivity\\\\heatmaps\\\\individual\\\\{mode}_{condition}_cons'):\n",
    "                os.remove(f'plots\\\\connectivity\\\\heatmaps\\\\individual\\\\{mode}_{condition}_cons\\\\{file}')\n",
    "\n",
    "            for i in range(1, num_cons + 1):\n",
    "                # get the averaged data\n",
    "                averaged_data = np.mean(ind_con[mode][condition][i - 1][dict_channel_types[channel_type]], axis=(1, 2)) # shape is now (10609,)\n",
    "\n",
    "                # Get the grid size\n",
    "                grid_size = int(np.sqrt(averaged_data.size))\n",
    "\n",
    "                # Reshape the data into a 2D grid\n",
    "                heatmap_data = averaged_data.reshape((grid_size, grid_size))\n",
    "\n",
    "                # Make the matrix symmetric\n",
    "                symmetric_data = heatmap_data + heatmap_data.T\n",
    "\n",
    "                # Set the diagonal to the highest value\n",
    "                np.fill_diagonal(symmetric_data, np.max(symmetric_data))\n",
    "\n",
    "                # Plot the heatmap\n",
    "                fig, ax = plt.subplots(figsize=(25, 25))\n",
    "                im = ax.imshow(symmetric_data, cmap='viridis')\n",
    "                ax.set_title(f'Connectivity Heatmap for Participant {i}, Mode: {mode}, Condition: {condition}, Channel Type: {channel_type}')\n",
    "                ax.set_xlabel('Channel')\n",
    "                ax.set_ylabel('Channel')\n",
    "                ax.set_xticks(np.arange(grid_size))\n",
    "                ax.set_yticks(np.arange(grid_size))\n",
    "                ch_names = [ch_name for ch_name in ind_connectivity_params['ch_names'] if channel_type in ch_name]\n",
    "                ax.set_xticklabels(ch_names)\n",
    "                ax.set_yticklabels(ch_names)\n",
    "                plt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "                cbar = fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "                cbar.set_label('Connectivity Value')\n",
    "                plt.savefig(f'plots/connectivity/heatmaps/individual/{mode}_{condition}_cons/con_{i}.png', dpi=dpi / 4)\n",
    "                plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Individual Connectivity over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_avg_ind_con_plot_avg:\n",
    "    averaged_data_across_time = []\n",
    "    for i in range(1, num_cons + 1):\n",
    "        # get the averaged data\n",
    "        averaged_data = np.mean(ind_con['all']['Blck'][i - 1][dict_channel_types[channel_type]], axis=(0, 1)) # shape is now (99,)\n",
    "        averaged_data_across_time.append(averaged_data)\n",
    "\n",
    "    # get the mean connectivity across participants\n",
    "    mean_connectivity = np.mean(averaged_data_across_time, axis=0)\n",
    "\n",
    "    # plot the connectivity\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(mean_connectivity)\n",
    "    plt.title(f'Average Channel Connectivity over time for all individuals, {channel_type}, {ind_connectivity_params[\"method\"]}, {ind_connectivity_params[\"con_mode\"]}')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Mean Connectivity Value')\n",
    "    # change the 99 time points to 16 seconds\n",
    "    plt.xticks(np.arange(0, 99, 11), np.arange(0, 17, 2))\n",
    "    plt.savefig(f'plots/connectivity/over_time/Average Channel Connectivity over time for all individuals.png', dpi=dpi / 4)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Condition Connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_types = ['hbo', 'hbr', 'hbt']\n",
    "\n",
    "# pick the connectivity method\n",
    "method = \"coh\"\n",
    "\n",
    "# pick the mode\n",
    "mode = \"cwt_morlet\"\n",
    "\n",
    "# pick the frequency range\n",
    "cwt_freqs = np.linspace(0.01, 0.5, 10)\n",
    "\n",
    "# pick the number of cycles\n",
    "cwt_n_cycles = 1\n",
    "\n",
    "# average the connectivity matrices across frequencies\n",
    "faverage = True\n",
    "\n",
    "if run_condition_connectivity:\n",
    "    # for each group of epochs, calculate the connectivity matrix\n",
    "    for condition, epoch in all_epochs.items():\n",
    "        cons = []\n",
    "        for channel_type in channel_types:\n",
    "            # pick the channels\n",
    "            epoch_channel = pick_channels(epoch, channel_type)\n",
    "        \n",
    "            # use spectral_connectivity_time to compute the connectivity\n",
    "            con = spectral_connectivity_epochs(\n",
    "                data = epoch_channel,\n",
    "                method=method,\n",
    "                mode=mode,\n",
    "                cwt_freqs=cwt_freqs,\n",
    "                cwt_n_cycles=cwt_n_cycles,\n",
    "                faverage=faverage,\n",
    "                n_jobs=n_jobs,\n",
    "                verbose=True\n",
    "            )\n",
    "            cons.append(con.get_data())\n",
    "        np.save(f\"processed_data\\\\connectivity\\\\{condition}_connectivity.npy\", np.array(cons))\n",
    "\n",
    "    # make a dictionsary to store the connectivity parameters\n",
    "    connectivity_params = {\n",
    "        \"channel_types\": channel_types,\n",
    "        \"method\": method,\n",
    "        \"mode\": mode,\n",
    "        \"cwt_freqs\": cwt_freqs.tolist(),\n",
    "        \"cwt_n_cycles\": cwt_n_cycles,\n",
    "        \"faverage\": faverage,\n",
    "        \"ch_names\": pick_channels(epoch, channel_types).ch_names\n",
    "    }\n",
    "\n",
    "    # save the connectivity parameters to disk in preprocessed_data\\connectivity\n",
    "    with open(\"processed_data\\\\connectivity\\\\connectivity_params.json\", \"w\") as f:\n",
    "        json.dump(connectivity_params, f)\n",
    "\n",
    "# load the numpy array from disk\n",
    "all_con = {\n",
    "    \"Real\": np.load(\"processed_data\\\\connectivity\\\\Real_connectivity.npy\"),\n",
    "    \"Virtual\": np.load(\"processed_data\\\\connectivity\\\\Virtual_connectivity.npy\"),\n",
    "    \"Joy\": np.load(\"processed_data\\\\connectivity\\\\Joy_connectivity.npy\"),\n",
    "    \"Fear\": np.load(\"processed_data\\\\connectivity\\\\Fear_connectivity.npy\"),\n",
    "    \"Anger\": np.load(\"processed_data\\\\connectivity\\\\Anger_connectivity.npy\"),\n",
    "    \"Disgust\": np.load(\"processed_data\\\\connectivity\\\\Disgust_connectivity.npy\"),\n",
    "    \"Sadness\": np.load(\"processed_data\\\\connectivity\\\\Sadness_connectivity.npy\"),\n",
    "    \"Neutral\": np.load(\"processed_data\\\\connectivity\\\\Neutral_connectivity.npy\"),\n",
    "    \"Surprise\": np.load(\"processed_data\\\\connectivity\\\\Surprise_connectivity.npy\")\n",
    "}\n",
    "\n",
    "# load the connectivity parameters from disk\n",
    "if os.path.exists(\"processed_data\\\\connectivity\\\\connectivity_params.json\"):\n",
    "    with open(\"processed_data\\\\connectivity\\\\connectivity_params.json\", \"r\") as f:\n",
    "        connectivity_params = json.load(f)\n",
    "\n",
    "channel_types = connectivity_params[\"channel_types\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Condition Connectivity Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_avg_condition_con_plot:\n",
    "    all_con_avg = {key: np.mean(con, axis=(0, 1, 2)) for key, con in all_con.items()}\n",
    "\n",
    "    # plot the connectivity over the 99 time points\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for condition, con in all_con_avg.items():\n",
    "        plt.plot(con, label=condition)\n",
    "    plt.title(f'Average Channel Connectivity over time for all conditions, {connectivity_params[\"channel_types\"]}, {connectivity_params[\"method\"]}, {connectivity_params[\"mode\"]}')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.xticks(np.arange(0, 99, 11), np.arange(0, 17, 2))\n",
    "    plt.ylabel('Connectivity')\n",
    "    plt.legend()\n",
    "    plt.savefig('plots/connectivity/over_time/Average Channel Connectivity over time for all conditions.png', dpi=dpi / 4)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram connectivity Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_hist_con_plots:\n",
    "    for condition, con in all_con.items():\n",
    "        # Average across channel_type first, then across time points\n",
    "        averaged_data = np.mean(np.mean(con, axis=(0)), axis=(1, 2))\n",
    "\n",
    "        # Remove any values that are 0\n",
    "        averaged_data = averaged_data[averaged_data != 0]\n",
    "\n",
    "        # plot a histogram of the connectivity values\n",
    "        plt.hist(averaged_data, bins=40, edgecolor='black')\n",
    "        plt.xlabel('Connectivity Value')\n",
    "        plt.ylabel('Number of Values')\n",
    "        plt.suptitle(f'Connectivity Histogram ({condition}), {connectivity_params[\"channel_types\"]}, {connectivity_params[\"method\"]}, {connectivity_params[\"mode\"]}')\n",
    "        plt.title(\"Removed connectivity values of 0\")\n",
    "        plt.savefig(f'plots/connectivity/histograms/conditions/{condition}_connectivity_histogram.png', dpi=dpi)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram difference in connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_hist_diff_emotion_plots:\n",
    "    # Get all face types in the dataset\n",
    "    face_types = [\"Real\", \"Virtual\"]\n",
    "\n",
    "    # Get all emotions in the dataset\n",
    "    emotions = [\"Joy\", \"Fear\", \"Anger\", \"Disgust\", \"Sadness\", \"Neutral\", \"Surprise\"]\n",
    "\n",
    "    # Placeholder for the result dictionary\n",
    "    diff_results = {}\n",
    "\n",
    "    # for both types of conditions\n",
    "    for cond_type in [face_types, emotions]:\n",
    "        # Loop through all combinations of two conditions\n",
    "        for cond1, cond2 in itertools.combinations(cond_type, 2):\n",
    "            # Calculate the difference in both directions\n",
    "            diff_1_2 = np.mean(np.mean(all_con[cond1], axis=(0)), axis=(1, 2)) - np.mean(np.mean(all_con[cond2], axis=(0)), axis=(1, 2))\n",
    "            diff_2_1 = np.mean(np.mean(all_con[cond2], axis=(0)), axis=(1, 2)) - np.mean(np.mean(all_con[cond1], axis=(0)), axis=(1, 2))\n",
    "            \n",
    "            # Store the results in the dictionary\n",
    "            diff_results[f\"{cond1}-{cond2}\"] = diff_1_2\n",
    "            diff_results[f\"{cond2}-{cond1}\"] = diff_2_1\n",
    "\n",
    "    for diff in diff_results:\n",
    "        # Remove any values that are 0\n",
    "        diff_results[diff] = diff_results[diff][diff_results[diff] != 0]\n",
    "\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        # plot a histogram of the connectivity values\n",
    "        plt.hist(diff_results[diff], bins=40, edgecolor='black')\n",
    "        plt.xlabel('Connectivity Value')\n",
    "        plt.ylabel('Number of Values')\n",
    "        plt.suptitle(f'Connectivity Histogram ({diff}), {connectivity_params[\"channel_types\"]}, {connectivity_params[\"method\"]}, {connectivity_params[\"mode\"]}')\n",
    "        plt.title(\"Removed connectivity values of 0\")\n",
    "        if 'Neutral' in diff:\n",
    "            plt.savefig(f'plots/connectivity/histograms/differences_neutral/{diff}_connectivity_histogram.png', dpi=dpi / 4)\n",
    "        else:\n",
    "            plt.savefig(f'plots/connectivity/histograms/differences/{diff}_connectivity_histogram.png', dpi=dpi / 4)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap connectivity Plots/Distance Measure Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_heatmap_con_dist_plots:\n",
    "    heatmaps = []\n",
    "\n",
    "    for con in all_con:\n",
    "        # Average across participants, frequencies, and time points\n",
    "        averaged_data = np.mean(np.mean(all_con[con], axis=(0)), axis=(1, 2))\n",
    "\n",
    "        # Get the grid size\n",
    "        grid_size = int(np.sqrt(averaged_data.size))\n",
    "\n",
    "        # Reshape the data into a 2D grid\n",
    "        heatmap_data = averaged_data.reshape((grid_size, grid_size))\n",
    "\n",
    "        # Make the matrix symmetric\n",
    "        symmetric_data = heatmap_data + heatmap_data.T\n",
    "\n",
    "        # Set the diagonal to the highest value\n",
    "        np.fill_diagonal(symmetric_data, np.max(symmetric_data))\n",
    "\n",
    "        heatmaps.append((con, symmetric_data))\n",
    "\n",
    "        # Plot the heatmap\n",
    "        fig, ax = plt.subplots(figsize=(20, 20))\n",
    "        ax.set_title(con)\n",
    "        im = ax.imshow(symmetric_data, cmap='viridis')\n",
    "        ax.set_xlabel('Channel')\n",
    "        ax.set_ylabel('Channel')\n",
    "        # set the x and y ticks to connectivity_params['ch_names']\n",
    "        ax.set_xticks(np.arange(grid_size))\n",
    "        ax.set_yticks(np.arange(grid_size))\n",
    "        ch_names = [ch_name for ch_name in connectivity_params['ch_names'] if channel_type in ch_name]\n",
    "        ax.set_xticklabels(ch_names)\n",
    "        ax.set_yticklabels(ch_names)\n",
    "        plt.xticks(rotation=90)\n",
    "\n",
    "        # Add a single colorbar for the entire figure\n",
    "        cbar = fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "        cbar.set_label('Connectivity Value')\n",
    "        plt.savefig(f'plots/connectivity/heatmaps/conditions/{con}.png', dpi=dpi / 3)\n",
    "        plt.close()\n",
    "\n",
    "    # create a 2D numpy array to store the differences between the heatmaps\n",
    "    diff_heatmaps = np.zeros((len(heatmaps), len(heatmaps)))\n",
    "\n",
    "    # calculate the absolute differences between the heatmaps\n",
    "    for i, (con1, heatmap1) in enumerate(heatmaps):\n",
    "        for j, (con2, heatmap2) in enumerate(heatmaps):\n",
    "            diff_heatmaps[i, j] = np.sum(np.abs(heatmap1 - heatmap2))\n",
    "\n",
    "    # plot the heatmap of the differences, with the numbers on the heatmap\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    plt.imshow(diff_heatmaps, cmap='viridis')\n",
    "    plt.title('Condition Differences')\n",
    "    plt.ylabel('Condition')\n",
    "    plt.xticks(range(len(heatmaps)), [con for con, _ in heatmaps], rotation=45)\n",
    "    plt.yticks(range(len(heatmaps)), [con for con, _ in heatmaps])\n",
    "    plt.colorbar(label='Difference', fraction=0.046, pad=0.04)\n",
    "    # add the numbers to the heatmap\n",
    "    for i in range(len(heatmaps)):\n",
    "        for j in range(len(heatmaps)):\n",
    "            plt.text(j, i, f'{diff_heatmaps[i, j]:.0f}', ha='center', va='center', color='white')\n",
    "    plt.savefig(f'plots/connectivity/heatmaps/condition_sum_of_differences.png', dpi=dpi)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap difference in connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_heatmap_diff_emotion_plots:\n",
    "    # Get all face types in the dataset\n",
    "    face_types = [\"Real\", \"Virtual\"]\n",
    "\n",
    "    # Get all emotions in the dataset\n",
    "    emotions = [\"Joy\", \"Fear\", \"Anger\", \"Disgust\", \"Sadness\", \"Neutral\", \"Surprise\"]\n",
    "\n",
    "    # Placeholder for the result dictionary\n",
    "    diff_results = {}\n",
    "\n",
    "    # for both types of conditions\n",
    "    for cond_type in [face_types, emotions]:\n",
    "        # Loop through all combinations of two conditions\n",
    "        for cond1, cond2 in itertools.combinations(cond_type, 2):\n",
    "            # Calculate the difference in both directions\n",
    "            diff_1_2 = np.mean(np.mean(all_con[cond1], axis=(0)), axis=(1, 2)) - np.mean(np.mean(all_con[cond2], axis=(0)), axis=(1, 2))\n",
    "            diff_2_1 = np.mean(np.mean(all_con[cond2], axis=(0)), axis=(1, 2)) - np.mean(np.mean(all_con[cond1], axis=(0)), axis=(1, 2))\n",
    "            \n",
    "            # Store the results in the dictionary\n",
    "            diff_results[f\"{cond1}-{cond2}\"] = diff_1_2\n",
    "            diff_results[f\"{cond2}-{cond1}\"] = diff_2_1\n",
    "\n",
    "    for diff in diff_results:\n",
    "        # Get the grid size\n",
    "        grid_size = int(np.sqrt(diff_results[diff].size))\n",
    "\n",
    "        # Reshape the data into a 2D grid\n",
    "        heatmap_data = diff_results[diff].reshape((grid_size, grid_size))\n",
    "\n",
    "        # Make the matrix symmetric\n",
    "        symmetric_data = heatmap_data + heatmap_data.T\n",
    "\n",
    "        # Set the diagonal to the lowest value\n",
    "        np.fill_diagonal(symmetric_data, np.min(symmetric_data))\n",
    "\n",
    "        # plot the heatmap\n",
    "        plt.figure(figsize=(20, 20))\n",
    "        plt.imshow(symmetric_data, cmap='viridis')\n",
    "        plt.xlabel('Channel')\n",
    "        plt.ylabel('Channel')\n",
    "        plt.title(f'Difference in Connectivity Matrices: {diff}, {connectivity_params[\"channel_types\"]}, {connectivity_params[\"method\"]}, {connectivity_params[\"mode\"]}')\n",
    "        ch_names = [ch_name for ch_name in connectivity_params['ch_names'] if channel_type in ch_name]\n",
    "        plt.xticks(np.arange(grid_size), ch_names)\n",
    "        plt.yticks(np.arange(grid_size), ch_names)\n",
    "        plt.xticks(rotation=90)\n",
    "\n",
    "        # Add a single colorbar for the entire figure\n",
    "        cbar = plt.colorbar(fraction=0.046, pad=0.04)\n",
    "        cbar.set_label('Difference in Connectivity Strength')\n",
    "\n",
    "        if 'Neutral' in diff:\n",
    "            plt.savefig(f'plots/connectivity/heatmaps/differences_neutral/{diff}.png', dpi=dpi / 3)\n",
    "        else:\n",
    "            plt.savefig(f'plots/connectivity/heatmaps/differences/{diff}.png', dpi=dpi / 3)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chord plots for Connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_chord_con_plots:\n",
    "    # make a color scheme\n",
    "    colorscheme = dict(\n",
    "        facecolor='white',\n",
    "        textcolor='black',\n",
    "        colormap='hot',\n",
    "        facecolor2='black',\n",
    "        textcolor2='white',\n",
    "    )\n",
    "\n",
    "    # get the node angles\n",
    "    node_angles = circular_layout(\n",
    "        ch_names_original, all_channels_names, start_pos=90, group_boundaries=group_boundaries\n",
    "    )\n",
    "\n",
    "    for con in all_con:\n",
    "        averaged_data = np.mean(np.mean(all_con[con], axis=(0)), axis=(1, 2))\n",
    "        grid_size = int(np.sqrt(averaged_data.size))\n",
    "        heatmap_data = averaged_data.reshape((grid_size, grid_size))\n",
    "        plot_connectivity_circle(\n",
    "            heatmap_data,\n",
    "            node_names=ch_names_original,\n",
    "            node_angles=node_angles,\n",
    "            n_lines=10000,\n",
    "            title=con,\n",
    "            colorbar_size=1,\n",
    "            fontsize_colorbar=8,\n",
    "            facecolor=colorscheme['facecolor'],\n",
    "            textcolor=colorscheme['textcolor'],\n",
    "            colormap=colorscheme['colormap'],\n",
    "            padding=3,\n",
    "            vmin=0.2,\n",
    "            vmax=0.6,\n",
    "            fontsize_title=24,\n",
    "            colorbar=True,\n",
    "            show=False\n",
    "        )\n",
    "\n",
    "        plt.savefig(f'plots/connectivity/chord_plots/conditions/{con}.png', dpi=dpi)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chord plot difference in connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_chord_diff_emotion_plots:\n",
    "    # Get all face types in the dataset\n",
    "    face_types = [\"Real\", \"Virtual\"]\n",
    "\n",
    "    # Get all emotions in the dataset\n",
    "    emotions = [\"Joy\", \"Fear\", \"Anger\", \"Disgust\", \"Sadness\", \"Neutral\", \"Surprise\"]\n",
    "\n",
    "    # Placeholder for the result dictionary\n",
    "    diff_results = {}\n",
    "\n",
    "    # for both types of conditions\n",
    "    for cond_type in [face_types, emotions]:\n",
    "        # Loop through all combinations of two conditions\n",
    "        for cond1, cond2 in itertools.combinations(cond_type, 2):\n",
    "            # Calculate the difference in both directions\n",
    "            diff_1_2 = np.mean(np.mean(all_con[cond1], axis=(0)), axis=(1, 2)) - np.mean(np.mean(all_con[cond2], axis=(0)), axis=(1, 2))\n",
    "            diff_2_1 = np.mean(np.mean(all_con[cond2], axis=(0)), axis=(1, 2)) - np.mean(np.mean(all_con[cond1], axis=(0)), axis=(1, 2))\n",
    "            \n",
    "            # Store the results in the dictionary\n",
    "            diff_results[f\"{cond1}-{cond2}\"] = diff_1_2\n",
    "            diff_results[f\"{cond2}-{cond1}\"] = diff_2_1\n",
    "\n",
    "    for diff in diff_results:\n",
    "        grid_size = int(np.sqrt(diff_results[diff].size))\n",
    "        heatmap_data = diff_results[diff].reshape((grid_size, grid_size))\n",
    "        plot_connectivity_circle(\n",
    "            heatmap_data,\n",
    "            node_names=ch_names_original,\n",
    "            node_angles=node_angles,\n",
    "            n_lines=10000,\n",
    "            title=diff,\n",
    "            colorbar_size=1,\n",
    "            fontsize_colorbar=8,\n",
    "            facecolor=colorscheme['facecolor'],\n",
    "            textcolor=colorscheme['textcolor'],\n",
    "            colormap=colorscheme['colormap'],\n",
    "            padding=3,\n",
    "            vmin=0.075,\n",
    "            vmax=0.15,\n",
    "            fontsize_title=24,\n",
    "            colorbar=True,\n",
    "            show=False\n",
    "        )\n",
    "\n",
    "        if 'Neutral' in diff:\n",
    "            plt.savefig(f'plots/connectivity/chord_plots/differences_neutral/{diff}.png', dpi=dpi)\n",
    "        else:\n",
    "            plt.savefig(f'plots/connectivity/chord_plots/differences/{diff}.png', dpi=dpi)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Time Series Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
      "C:\\Users\\super\\AppData\\Local\\Temp\\ipykernel_39428\\3652892813.py:35: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'Base' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n"
     ]
    }
   ],
   "source": [
    "modes = ['face_type', 'emotion']\n",
    "\n",
    "if get_time_series:\n",
    "    for mode in modes:\n",
    "        time_series = pd.DataFrame()\n",
    "\n",
    "        for i, raw_haemo in enumerate(raw_haemo_good_recordings, 1):\n",
    "            raw_haemo_annots = raw_haemo.copy()\n",
    "            relabel_annotations(raw_haemo_annots, mode=mode)\n",
    "\n",
    "            df = raw_haemo_annots.to_data_frame()\n",
    "\n",
    "            annots = raw_haemo_annots.annotations.to_data_frame(time_format='ms')\n",
    "\n",
    "            # drop the duration column\n",
    "            annots.drop(columns=['duration'], inplace=True)\n",
    "\n",
    "            # set the first onset to 0 and each onset to the previous onset + duration in seconds\n",
    "            annots['onset'] = (annots['onset'] - annots['onset'][0]) / 1000\n",
    "\n",
    "            # insert a new column after the time column called 'event'\n",
    "            df.insert(1, 'event', np.nan)\n",
    "\n",
    "            # create an empty last row and shuffle all the descriptions down one row\n",
    "            annots.loc[len(annots)] = None\n",
    "            annots['description'] = annots['description'].shift(1)\n",
    "\n",
    "            # remove the first row\n",
    "            annots = annots.iloc[1:]\n",
    "\n",
    "            for row in annots.iterrows():\n",
    "                time = row[1]['onset']\n",
    "\n",
    "                # fill all the events that occur since the last non NaN event and the current event\n",
    "                df.loc[(df['time'] < time) & (df['event'].isna()), 'event'] = row[1]['description']\n",
    "\n",
    "            # get the last event\n",
    "            last_event = annots['description'].iloc[-1]\n",
    "\n",
    "            # fill all the events that occur after the last event with the last event\n",
    "            df.loc[df['event'].isna(), 'event'] = last_event\n",
    "\n",
    "            # create a new column called 'participant' and fill it with the participant number\n",
    "            df['participant'] = i\n",
    "\n",
    "            time_series = pd.concat([time_series, df])\n",
    "\n",
    "        # save to csv\n",
    "        time_series.to_csv(f'processed_data/time_series/datasets/time_series_{mode}.csv', index=False)\n",
    "\n",
    "# load the time series from disk\n",
    "all_time_series = {\n",
    "    mode: pd.read_csv(f'processed_data/time_series/datasets/time_series_{mode}.csv')\n",
    "    for mode in modes\n",
    "}\n",
    "\n",
    "# drop the Base event from both time series\n",
    "drop_base_event = True\n",
    "if drop_base_event:\n",
    "    for mode in modes:\n",
    "        all_time_series[mode] = all_time_series[mode][all_time_series[mode]['event'] != 'Base']\n",
    "\n",
    "label_mapping = {\n",
    "    mode: dict(sorted(dict(zip(LabelEncoder().fit_transform(all_time_series[mode][\"event\"]), all_time_series[mode][\"event\"])).items()))\n",
    "    for mode in modes\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_models:\n",
    "    models_to_run = [\n",
    "        # RandomForestClassifier(n_estimators=250, class_weight='balanced', random_state=42, n_jobs=n_jobs),\n",
    "        # XGBClassifier(n_jobs=n_jobs, random_state=42),\n",
    "        # LGBMClassifier(n_jobs=n_jobs, random_state=42),\n",
    "        # HistGradientBoostingClassifier(random_state=42),\n",
    "        # LogisticRegression(solver=\"lbfgs\", max_iter=1000, random_state=42, n_jobs=n_jobs),\n",
    "        # KNeighborsClassifier(n_neighbors=2, algorithm='auto', leaf_size=10, weights='distance', n_jobs=n_jobs),\n",
    "        LinearSVC(dual=\"auto\", random_state=42),\n",
    "        # SGDClassifier(loss=\"hinge\", random_state=42, n_jobs=n_jobs),\n",
    "        # MLPClassifier(hidden_layer_sizes=(256, 128, 64), activation='relu', solver='adam', alpha=1e-4,\n",
    "        #               batch_size=256, learning_rate='adaptive', max_iter=500, early_stopping=True, random_state=42)\n",
    "    ]\n",
    "\n",
    "    # Define parameter grids for models to be tuned if needed\n",
    "    param_grids = {\n",
    "        # 'RandomForestClassifier': { 'n_estimators': [100, 250, 500] },\n",
    "        # Add other models' grids as needed\n",
    "    }\n",
    "\n",
    "    # Create result folders for each model if they don't exist\n",
    "    for model in models_to_run:\n",
    "        model_folder = f'processed_data/time_series/results/{model.__class__.__name__}'\n",
    "        if not os.path.exists(model_folder):\n",
    "            os.makedirs(model_folder)\n",
    "\n",
    "    # Use LeaveOneGroupOut for LOSO cross-validation\n",
    "    logo = LeaveOneGroupOut()\n",
    "\n",
    "    for mode, time_series in all_time_series.items():\n",
    "        # Convert 'event' to numerical labels\n",
    "        label_encoder = LabelEncoder()\n",
    "        time_series[\"event\"] = label_encoder.fit_transform(time_series[\"event\"])\n",
    "\n",
    "        # Convert 'time' to numeric type\n",
    "        time_series[\"time\"] = pd.to_numeric(time_series[\"time\"], errors=\"coerce\")\n",
    "\n",
    "        # Normalize all sensor data (assuming first two columns are 'time' and 'event')\n",
    "        scaler = StandardScaler()\n",
    "        sensor_columns = time_series.columns[2:]\n",
    "        time_series[sensor_columns] = scaler.fit_transform(time_series[sensor_columns])\n",
    "\n",
    "        # Optionally check for missing values\n",
    "        missing_values = time_series.isnull().sum().sum()\n",
    "        if missing_values > 0:\n",
    "            print(f\"Warning: There are {missing_values} missing values in mode {mode}.\")\n",
    "\n",
    "        # Split data into features and target variable\n",
    "        X = time_series.drop(columns=[\"event\", \"time\", \"participant\"])\n",
    "        y = time_series[\"event\"]\n",
    "        groups = time_series[\"participant\"]\n",
    "\n",
    "        for model in models_to_run:\n",
    "            model_name = model.__class__.__name__\n",
    "\n",
    "            # Use GridSearchCV with GroupKFold if a parameter grid is provided\n",
    "            if model_name in param_grids:\n",
    "                grid_search = GridSearchCV(estimator=model,\n",
    "                                           param_grid=param_grids[model_name],\n",
    "                                           cv=logo,\n",
    "                                           n_jobs=4,\n",
    "                                           scoring='accuracy',\n",
    "                                           verbose=3)\n",
    "                grid_search.fit(X, y, groups=groups)\n",
    "                model = grid_search.best_estimator_\n",
    "                print(f\"Best params for {model_name}: {model.get_params()}\")\n",
    "\n",
    "            # Generate cross-validated predictions ensuring no participant overlap\n",
    "            y_pred = cross_val_predict(model, X, y, cv=logo, groups=groups, n_jobs=1)\n",
    "\n",
    "            # Create a classification report\n",
    "            report_dict = classification_report(y, y_pred, output_dict=True)\n",
    "            report = pd.DataFrame(report_dict).transpose()\n",
    "\n",
    "            # Optionally rename class indices using your mapping if available\n",
    "            for index in report.index:\n",
    "                if index.isdigit():\n",
    "                    report.rename(index={index: label_mapping[mode][int(index)]}, inplace=True)\n",
    "\n",
    "            # If the model provides feature importances, save them\n",
    "            if hasattr(model, 'feature_importances_'):\n",
    "                feature_importances = pd.DataFrame(model.feature_importances_,\n",
    "                                                   index=X.columns,\n",
    "                                                   columns=['importance'])\n",
    "                feature_importances.sort_values(by='importance', ascending=False, inplace=True)\n",
    "                feature_importances.to_csv(\n",
    "                    f'processed_data/time_series/results/{model_name}/feature_importances_{mode}.csv'\n",
    "                )\n",
    "\n",
    "            # Save the classification report to disk\n",
    "            report.to_csv(f'processed_data/time_series/results/{model_name}/report_{mode}.csv')\n",
    "\n",
    "# Load results from disk into a dictionary\n",
    "models = [folder for folder in os.listdir('processed_data/time_series/results')]\n",
    "\n",
    "results = {\n",
    "    model: {\n",
    "        mode: {\n",
    "            'feature_importances': pd.read_csv(f'processed_data/time_series/results/{model}/feature_importances_{mode}.csv', index_col=0)\n",
    "                                    if os.path.exists(f'processed_data/time_series/results/{model}/feature_importances_{mode}.csv') else None,\n",
    "            'report': pd.read_csv(f'processed_data/time_series/results/{model}/report_{mode}.csv', index_col=0)\n",
    "                      if os.path.exists(f'processed_data/time_series/results/{model}/report_{mode}.csv') else None\n",
    "        }\n",
    "        for mode in modes\n",
    "    }\n",
    "    for model in models\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Classifier Accuracy Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mode in modes:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    x = np.arange(len(label_mapping[mode]))  # the label locations\n",
    "    width = 0.1  # the width of the bars\n",
    "\n",
    "    model_index = 0\n",
    "    for i, model in enumerate(models):\n",
    "        if results[model][mode]['report'] is not None:\n",
    "            report = results[model][mode]['report']\n",
    "            report = report.loc[report.index.intersection(label_mapping[mode].values())]\n",
    "            plt.bar(x + width * model_index, report['precision'], width, label=model)\n",
    "            model_index += 1\n",
    "\n",
    "    plt.title(f'Accuracy per Condition for {mode}')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Condition')\n",
    "    plt.xticks(x + width * (model_index - 1) / 2, label_mapping[mode].values())\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/time_series/accuracy/accuracy_per_condition_{mode}.png', dpi=dpi / 4)\n",
    "    plt.close()\n",
    "\n",
    "for mode in modes:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    model_index = 0\n",
    "    models_used = []\n",
    "    for i, model in enumerate(models):\n",
    "        if results[model][mode]['report'] is not None:\n",
    "            report = results[model][mode]['report']\n",
    "            accuracy = report['precision'].accuracy\n",
    "            plt.bar(model_index, accuracy, label=model)\n",
    "            models_used.append(model)\n",
    "            model_index += 1\n",
    "\n",
    "    plt.title(f'Total Accuracy for {mode}')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Model')\n",
    "    plt.xticks(range(len(models_used)), models_used, rotation=90)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/time_series/accuracy/total_accuracy_{mode}.png', dpi=dpi / 4)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Feature Importances per Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the feature importances for each model\n",
    "for model in models:\n",
    "    for mode in modes:\n",
    "        if results[model][mode]['feature_importances'] is not None:\n",
    "            feature_importances = results[model][mode]['feature_importances']\n",
    "\n",
    "            plt.figure(figsize=(6, 30))\n",
    "            plt.barh(feature_importances.index, feature_importances['importance'])\n",
    "            plt.title(f'Feature Importances for {model} ({mode})')\n",
    "            plt.xlabel('Importance')\n",
    "            plt.ylabel('Feature')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'plots/time_series/feature_importances/{model}_{mode}.png', dpi=dpi / 4)\n",
    "            plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
