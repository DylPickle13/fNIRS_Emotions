{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports/Pick Analyses to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import mne\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import matplotlib.patches as mpatches\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import re\n",
    "import seaborn as sns\n",
    "from statsmodels.formula.api import mixedlm\n",
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "\n",
    "from preprocess_nirs import *\n",
    "\n",
    "from mne_nirs.channels import picks_pair_to_idx, get_long_channels\n",
    "from mne_nirs.preprocessing import peak_power, scalp_coupling_index_windowed\n",
    "from mne.preprocessing.nirs import source_detector_distances, scalp_coupling_index\n",
    "from mne_connectivity import spectral_connectivity_epochs, spectral_connectivity_time\n",
    "from mne_connectivity.viz import plot_connectivity_circle\n",
    "from mne.viz import circular_layout\n",
    "from mne_nirs.experimental_design import make_first_level_design_matrix\n",
    "from mne_nirs.statistics import run_glm, statsmodels_to_results\n",
    "from mne_nirs.visualisation import plot_glm_group_topo\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis, LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import LeaveOneGroupOut, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set the image dpi\n",
    "dpi = 600\n",
    "\n",
    "# Set the number of cores to use\n",
    "n_jobs = 1\n",
    "if os.cpu_count() is not None:\n",
    "    n_jobs = int(np.ceil(os.cpu_count() * 0.75))\n",
    "\n",
    "# Set the random seed\n",
    "os.environ['PYTHONHASHSEED'] = '42'\n",
    "\n",
    "# Preprocess the data\n",
    "process_data = False\n",
    "sci_threshold = 0.5\n",
    "cv_threshold = 15\n",
    "good_threshold = 0.7\n",
    "\n",
    "# Get the participant info plots\n",
    "get_participant_age_head_size_plot = False\n",
    "get_meas_dates_plot = False\n",
    "\n",
    "# Get the non windowed SCI and CV measure\n",
    "get_full_sci = False\n",
    "get_full_cv = False\n",
    "\n",
    "# Get the peak power/scalp coupling index dataframes\n",
    "get_peak_power_sci_df = False\n",
    "peak_power_threshold = 0.1\n",
    "get_good_windows_plot = False\n",
    "get_slope_pp_sci_plot = False\n",
    "get_head_size_vs_sci_plot = False\n",
    "get_across_participant_sci_plots = False\n",
    "get_participant_sci_plots = False\n",
    "\n",
    "# Get the epochs\n",
    "get_epochs = False\n",
    "get_face_type_emotion_epochs = False\n",
    "\n",
    "# Get the GLM data/plots\n",
    "get_glm_analysis = False\n",
    "get_ind_glm_plots = False\n",
    "get_group_glm_plots = False\n",
    "get_group_contrast_plots = False\n",
    "\n",
    "# Get the average timeseries activity\n",
    "get_roi_timeseries_activity = False\n",
    "\n",
    "# Get the ERP plots\n",
    "get_erp_plots = False\n",
    "\n",
    "# Get the connectivity data\n",
    "run_ind_connectivity = False\n",
    "get_condition_con_plots = False\n",
    "get_condition_con_gif_plots = False\n",
    "get_variance_con_plots = False\n",
    "run_group_level_t_tests = False\n",
    "get_group_level_t_tests_chord_plots = False\n",
    "get_group_level_t_tests_roi_chord_plots = False\n",
    "\n",
    "# Run decoding analysis\n",
    "run_traditional_raw_within_decoding = False\n",
    "run_traditional_con_within_decoding = False\n",
    "run_traditional_raw_across_decoding = False\n",
    "run_dl_raw_across_decoding = False\n",
    "run_traditional_con_across_decoding = False\n",
    "run_dl_con_across_decoding = False\n",
    "\n",
    "# Get the decoding plots\n",
    "get_decoding_table_scores_plots = False\n",
    "get_decoding_individual_scores_plots = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder path\n",
    "data_path = os.path.join(os.getcwd(), 'data')\n",
    "\n",
    "# Get a list of paths of all the subfolders of the folders labeled 'P_1', 'P_2', etc.\n",
    "participants = [os.path.join(data_path, f) for f in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, f))]\n",
    "\n",
    "participants_with_same_order = []\n",
    "\n",
    "# remove participants P_1 to P_11 but keep P_10, and P_12 onwards\n",
    "# P_1 to P_11 have the same order of faces, and P_86 and P_87 have the same order of faces\n",
    "for i in range(1, 12):\n",
    "    if i != 10:\n",
    "        participants_with_same_order.append(os.path.join(data_path, f'P_{i}'))\n",
    "        #participants.remove(os.path.join(data_path, f'P_{i}'))\n",
    "\n",
    "participants_with_same_order.append(os.path.join(data_path, f'P_87'))\n",
    "#participants.remove(os.path.join(data_path, f'P_87'))\n",
    "\n",
    "# remove participants P_13 due to not recording\n",
    "participants.remove(os.path.join(data_path, f'P_13'))\n",
    "\n",
    "# remove participants P_50 due to ending early\n",
    "participants.remove(os.path.join(data_path, f'P_50'))\n",
    "\n",
    "# participant P_54 used their phone\n",
    "participants.remove(os.path.join(data_path, f'P_54'))\n",
    "\n",
    "# Search recursively for the folder with the .snirf extension\n",
    "fnirs_folders = []\n",
    "for participant in participants:\n",
    "    for root, dirs, files in os.walk(participant):\n",
    "        for file in files:\n",
    "            if file.endswith('.snirf'):\n",
    "                fnirs_folders.append(root)\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    raws = []\n",
    "    raw_ods = []\n",
    "    raw_haemos = []\n",
    "\n",
    "    if process_data:\n",
    "        # Load the snirf files\n",
    "        for folder in fnirs_folders:\n",
    "            # find all the .snirf files in the folder but get the full path\n",
    "            snirf_files = [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith('.snirf')]\n",
    "            if len(snirf_files) == 0:\n",
    "                print(f\"No .snirf files found in {folder}\")\n",
    "                continue\n",
    "            elif len(snirf_files) > 1:\n",
    "                raise Exception(f\"Multiple .snirf files found in {folder}\")\n",
    "            else:\n",
    "                raw = mne.io.read_raw_snirf(snirf_files[0], optode_frame='mri', preload=True, verbose=False)\n",
    "            \n",
    "            # find all the 'description.json' files in the folder but get the full path\n",
    "            description_files = [f for f in os.listdir(folder) if 'description.json' in f]\n",
    "            if len(description_files) == 0:\n",
    "                print(f\"No description.json files found in {folder}\")\n",
    "                continue\n",
    "            elif len(description_files) > 1:\n",
    "                raise Exception(f\"Multiple description.json files found in {folder}\")\n",
    "            else:\n",
    "                description = json.load(open(os.path.join(folder, description_files[0])))\n",
    "\n",
    "            # add the description to the raw object\n",
    "            raw.info['description'] = str(description)\n",
    "            raws.append(raw)\n",
    "\n",
    "        # sort the raws by the measurement date\n",
    "        raws = sorted(raws, key=lambda x: x.info['meas_date'])\n",
    "\n",
    "        i = 1\n",
    "        for raw in raws:\n",
    "            raw_od, raw_haemo = preprocess_data(raw)\n",
    "            raw_ods.append(raw_od)\n",
    "            raw_haemos.append(raw_haemo)\n",
    "            i += 1\n",
    "\n",
    "        # clear any files in each folder\n",
    "        for folder in ['processed_data/raws', 'processed_data/raw_ods', 'processed_data/raw haemos']:\n",
    "            for f in os.listdir(folder):\n",
    "                os.remove(os.path.join(folder, f))\n",
    "\n",
    "        for i, (raw, raw_od, raw_haemo) in enumerate(zip(raws, raw_ods, raw_haemos), 1):\n",
    "            # save raw as a fif file\n",
    "            raw.save(f'processed_data/raws/raw{i}.fif', overwrite=True, verbose=False)\n",
    "\n",
    "            # save raw_od as a fif file\n",
    "            raw_od.save(f'processed_data/raw_ods/raw_od{i}.fif', overwrite=True, verbose=False)\n",
    "\n",
    "            # save raw_haemo as a fif file\n",
    "            raw_haemo.save(f'processed_data/raw haemos/raw_haemo{i}.fif', overwrite=True, verbose=False)\n",
    "        \n",
    "        raws = []\n",
    "        raw_ods = []\n",
    "        raw_haemos = []\n",
    "\n",
    "    # count how many files are in the processed_data/raw_haemos folder\n",
    "    processed_data_count = len([f for f in os.listdir('processed_data/raw haemos') if f.endswith('.fif')])\n",
    "\n",
    "    # Load the processed data\n",
    "    for i in range(1, processed_data_count + 1):\n",
    "        raw = mne.io.read_raw_fif(f'processed_data/raws/raw{i}.fif', preload=True, verbose=False)\n",
    "        raws.append(raw)\n",
    "\n",
    "        raw_od = mne.io.read_raw_fif(f'processed_data/raw_ods/raw_od{i}.fif', preload=True, verbose=False)\n",
    "        raw_ods.append(raw_od)\n",
    "\n",
    "        raw_haemo = mne.io.read_raw_fif(f'processed_data/raw haemos/raw_haemo{i}.fif', preload=True, verbose=False)\n",
    "        raw_haemos.append(raw_haemo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Participant Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Participant Age/Head Size Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_participant_age_head_size_plot:\n",
    "    # Find the file with description.json as a substring in each subfolder\n",
    "    description_files = [os.path.join(subfolder, f) for subfolder in fnirs_folders for f in os.listdir(subfolder) if 'description.json' in f]\n",
    "\n",
    "    # Load the description files\n",
    "    descriptions = [json.load(open(description_file)) for description_file in description_files]\n",
    "\n",
    "    # Get the average age of the participants and convert it to a float\n",
    "    ages = [float(description['age']) for description in descriptions]\n",
    "    average_age = sum(ages) / len(ages)\n",
    "    min_age = min(ages)\n",
    "    max_age = max(ages)\n",
    "    std_age = np.std(ages)\n",
    "\n",
    "    # Convert the remarks to a float and get the average\n",
    "    remarks = [description['remarks'] for description in descriptions]\n",
    "    # if remark is '', replace it with None\n",
    "    remarks = [float(remark) if remark != '' else None for remark in remarks]\n",
    "    average_head_circumference = sum(remark for remark in remarks if remark is not None) / len([remark for remark in remarks if remark is not None])\n",
    "    std_head_circumference = np.std([remark for remark in remarks if remark is not None])\n",
    "    # Create subplots\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    # Age histogram\n",
    "    axs[0].hist(ages, bins=40, edgecolor='black')\n",
    "    axs[0].set_xlabel('Age')\n",
    "    axs[0].set_ylabel('Number of participants')\n",
    "    axs[0].set_title(f'Age distribution of participants\\n{len(ages)} participants, Mean: {round(average_age, 2)}, Std: {round(std_age, 2)}')\n",
    "\n",
    "    # Head circumference histogram\n",
    "    axs[1].hist([remark * 2.54 for remark in remarks if remark is not None], bins=40, edgecolor='black')\n",
    "    axs[1].set_xlabel('Head circumference (cm)')\n",
    "    axs[1].set_ylabel('Number of participants')\n",
    "    axs[1].set_title(f'Head circumference distribution of participants\\nMean: {round(average_head_circumference * 2.54, 2)} cm, Std: {round(std_head_circumference * 2.54, 2)} cm')\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/participants/participant_info.png', dpi=dpi)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurement Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_meas_dates_plot:\n",
    "    # Extract the measurement dates\n",
    "    measurement_dates = [raw_haemo.info['meas_date'] for raw_haemo in raw_haemos]\n",
    "\n",
    "    # Convert to pandas datetime\n",
    "    measurement_dates = pd.to_datetime(measurement_dates)\n",
    "\n",
    "    # Create a plot of the measurement dates\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.plot(measurement_dates, range(1, len(measurement_dates) + 1), 'o-')\n",
    "    plt.xlabel('Measurement date')\n",
    "    plt.ylabel('Participant number')\n",
    "    plt.title('Measurement dates of participants, N = ' + str(len(measurement_dates)))\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/participants/measurement_dates.png', dpi=dpi / 4)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Short Distance Channels Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 participants with 4 short channels (< 1 cm) and 206 long channels (>= 1 cm)\n",
      "40 participants with 16 short channels (< 1 cm) and 206 long channels (>= 1 cm)\n"
     ]
    }
   ],
   "source": [
    "distance_counts = [\n",
    "    (\n",
    "        sum(distances < 0.01),\n",
    "        sum((distances >= 0.01))\n",
    "    )\n",
    "    for raw in raws\n",
    "    for distances in [np.array(source_detector_distances(raw.info))]\n",
    "]\n",
    "\n",
    "# Count unique tuples\n",
    "unique_distance_counts = Counter(distance_counts)\n",
    "\n",
    "# Display the results\n",
    "for (short_count, long_count), participant_count in unique_distance_counts.items():\n",
    "    print(f\"{participant_count} participants with {short_count} short channels (< 1 cm) and {long_count} long channels (>= 1 cm)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping brain regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the channel names for hbo\n",
    "ch_names_hbo = [ch_name for ch_name in raw_haemos[0].ch_names if 'hbo' in ch_name]\n",
    "\n",
    "ch_mapping_hbo = {\n",
    "    \"Left Frontal\": [],\n",
    "    \"Right Frontal\": [],\n",
    "    \"Left Prefrontal\": [],\n",
    "    \"Right Prefrontal\": [],\n",
    "    \"Left Parietal\": [],\n",
    "    \"Right Parietal\": [],\n",
    "    \"Left Occipital\": [],\n",
    "    \"Right Occipital\": []\n",
    "}\n",
    "\n",
    "group_boundaries = [0]\n",
    "\n",
    "ch_mapping_hbo[\"Left Frontal\"].append('S1_D1 hbo')\n",
    "\n",
    "ch_mapping_hbo[\"Left Frontal\"].append('S1_D2 hbo')\n",
    "\n",
    "ch_mapping_hbo[\"Left Frontal\"].append('S1_D17 hbo')\n",
    "\n",
    "# find the channels that have 'S2_', 'S3_', 'S4_', 'S5_' in them\n",
    "for ch_name in [ch_name for ch_name in ch_names_hbo if 'S2_' in ch_name or 'S3_' in ch_name or 'S4_' in ch_name or 'S5_' in ch_name]:\n",
    "    ch_mapping_hbo[\"Left Frontal\"].append(ch_name)\n",
    "\n",
    "ch_mapping_hbo[\"Left Frontal\"].append('S6_D2 hbo')\n",
    "\n",
    "ch_mapping_hbo[\"Left Frontal\"].append('S6_D3 hbo')\n",
    "\n",
    "ch_mapping_hbo[\"Left Frontal\"].append('S6_D18 hbo')\n",
    "\n",
    "group_boundaries.append(len(ch_mapping_hbo[\"Left Frontal\"]))\n",
    "\n",
    "# find the channels that have 'S9_', 'S10_', 'S11_', 'S12_' in them\n",
    "for ch_name in [ch_name for ch_name in ch_names_hbo if 'S9_' in ch_name or 'S10_' in ch_name or 'S11_' in ch_name or 'S12_' in ch_name]:\n",
    "    ch_mapping_hbo[\"Right Frontal\"].append(ch_name)\n",
    "\n",
    "group_boundaries.append(len(ch_mapping_hbo[\"Right Frontal\"]) + group_boundaries[-1])\n",
    "\n",
    "ch_mapping_hbo[\"Left Prefrontal\"].append('S6_D31 hbo')\n",
    "\n",
    "# find the channels that have 'S7_', 'S8_', 'S25_', 'S26_' in them\n",
    "for ch_name in [ch_name for ch_name in ch_names_hbo if 'S7_' in ch_name or 'S8_' in ch_name or 'S25_' in ch_name or 'S26_' in ch_name]:\n",
    "    ch_mapping_hbo[\"Left Prefrontal\"].append(ch_name)\n",
    "\n",
    "group_boundaries.append(len(ch_mapping_hbo[\"Left Prefrontal\"]) + group_boundaries[-1])\n",
    "\n",
    "# find the channels that have 'S13_', 'S14_', 'S15_', 'S16_' in them\n",
    "for ch_name in [ch_name for ch_name in ch_names_hbo if 'S13_' in ch_name or 'S14_' in ch_name or 'S15_' in ch_name or 'S16_' in ch_name]:\n",
    "    ch_mapping_hbo[\"Right Prefrontal\"].append(ch_name)\n",
    "\n",
    "group_boundaries.append(len(ch_mapping_hbo[\"Right Prefrontal\"]) + group_boundaries[-1])\n",
    "\n",
    "# find the channels that have 'S27_', 'S28_', 'S29_', 'S30_' in them\n",
    "for ch_name in [ch_name for ch_name in ch_names_hbo if 'S27_' in ch_name or 'S28_' in ch_name or 'S29_' in ch_name or 'S30_' in ch_name]:\n",
    "    ch_mapping_hbo[\"Left Parietal\"].append(ch_name)\n",
    "\n",
    "group_boundaries.append(len(ch_mapping_hbo[\"Left Parietal\"]) + group_boundaries[-1])\n",
    "\n",
    "ch_mapping_hbo[\"Right Occipital\"].append('S21_D13 hbo')\n",
    "\n",
    "ch_mapping_hbo[\"Right Occipital\"].append('S21_D16 hbo')\n",
    "\n",
    "ch_mapping_hbo[\"Right Occipital\"].append('S23_D15 hbo')\n",
    "\n",
    "ch_mapping_hbo[\"Right Occipital\"].append('S23_D16 hbo')\n",
    "\n",
    "# find the channels that have 'S17_', 'S18_', 'S19_', 'S20_' in them\n",
    "for ch_name in [ch_name for ch_name in ch_names_hbo if 'S17_' in ch_name or 'S18_' in ch_name or 'S19_' in ch_name or 'S20_' in ch_name]:\n",
    "    ch_mapping_hbo[\"Right Parietal\"].append(ch_name)\n",
    "\n",
    "group_boundaries.append(len(ch_mapping_hbo[\"Right Parietal\"]) + group_boundaries[-1])\n",
    "\n",
    "# find the channels that have 'S32_', 'S31_' in them\n",
    "for ch_name in [ch_name for ch_name in ch_names_hbo if 'S32_' in ch_name or 'S31_' in ch_name]:\n",
    "    ch_mapping_hbo[\"Left Occipital\"].append(ch_name)\n",
    "\n",
    "group_boundaries.append(len(ch_mapping_hbo[\"Left Occipital\"]) + group_boundaries[-1])\n",
    "\n",
    "# find the channels that have 'S22_', 'S24_' in them\n",
    "for ch_name in [ch_name for ch_name in ch_names_hbo if 'S22_' in ch_name or 'S24_' in ch_name]:\n",
    "    ch_mapping_hbo[\"Right Occipital\"].append(ch_name)\n",
    "\n",
    "ch_mapping_hbo[\"Right Occipital\"].append('S21_D28 hbo')\n",
    "\n",
    "ch_mapping_hbo[\"Right Occipital\"].append('S23_D30 hbo')\n",
    "\n",
    "ch_mapping_hbr = {region: [channel.replace('hbo', 'hbr') for channel in ch_mapping_hbo[region]] for region in ch_mapping_hbo}\n",
    "\n",
    "ch_mapping_hbt = {region: [channel.replace('hbo', 'hbt') for channel in ch_mapping_hbo[region]] for region in ch_mapping_hbo}\n",
    "\n",
    "ch_mapping_all = {region: ch_mapping_hbo[region] + ch_mapping_hbr[region] + ch_mapping_hbt[region] for region in ch_mapping_hbo}\n",
    "\n",
    "# concatenate the values of the dictionary into a list\n",
    "all_channels_hbo = [channel for region in ch_mapping_hbo.values() for channel in region]\n",
    "\n",
    "# duplicate all_channels but replace 'hbo' with 'hbr'\n",
    "all_channels_hbr = [channel.replace('hbo', 'hbr') for channel in all_channels_hbo]\n",
    "\n",
    "all_channels_hbt = [channel.replace('hbo', 'hbt') for channel in all_channels_hbo]\n",
    "\n",
    "# concatenate all_channels_hbo and all_channels_hbr\n",
    "all_channels = all_channels_hbo + all_channels_hbr + all_channels_hbt\n",
    "\n",
    "# make a dictionary called ch_mapping_names that has the channel names without the 'hbo' or 'hbr' at the end\n",
    "ch_mapping_names = {region: [channel[:-4] for channel in ch_mapping_hbo[region]] for region in ch_mapping_hbo}\n",
    "\n",
    "# make a list of all the channel names without the 'hbo' or 'hbr' at the end\n",
    "all_channels_names = [channel[:-4] for channel in all_channels_hbo]\n",
    "\n",
    "ch_names_original = [ch_name[:-4] for ch_name in ch_names_hbo]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalp Coupling Index (SCI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_full_sci:\n",
    "    # for each recording, count the number of channels with a sci greater than good_threshold\n",
    "    good_channels = [sum(scalp_coupling_index(raw_od, verbose=False) >= sci_threshold) for raw_od in raw_ods]\n",
    "    bad_channels = [sum(scalp_coupling_index(raw_od, verbose=False) < sci_threshold) for raw_od in raw_ods]\n",
    "    good_recordings = sum([good_channel >= good_threshold * (good_channel + bad_channel) for good_channel, bad_channel in zip(good_channels, bad_channels)])\n",
    "\n",
    "    # Plot the good vs bad channels for each recording in a dual bar chart\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.bar(range(len(good_channels)), good_channels, label='Good Channels')\n",
    "    ax.bar(range(len(bad_channels)), bad_channels, bottom=good_channels, label='Bad Channels')\n",
    "    ax.set_xlabel('Recording')\n",
    "    ax.set_ylabel('Number of Channels')\n",
    "    ax.axhline(raw_od.info['nchan'] * good_threshold, color='green', linestyle='--')\n",
    "    title = f'Good vs Bad Channels (T = {sci_threshold})\\nGood Recordings: {good_recordings}, N = {len(raw_ods)}, Retention Rate: {good_recordings / len(raw_ods) * 100:.2f}%'\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    plt.savefig(f'plots/signal quality/Signal Quality (SCI).png', dpi=dpi)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coefficient of Variance (CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_full_cv:\n",
    "    # for each recording, count the number of channels with a cv less than cov_threshold\n",
    "    good_channels = [sum(100 * np.std(ch) / np.mean(ch) < cv_threshold for ch in get_long_channels(raw).get_data()) for raw in raws]\n",
    "    bad_channels = [sum(100 * np.std(ch) / np.mean(ch) >= cv_threshold for ch in get_long_channels(raw).get_data()) for raw in raws]\n",
    "    good_recordings = sum([good_channel >= good_threshold * (good_channel + bad_channel) for good_channel, bad_channel in zip(good_channels, bad_channels)])\n",
    "\n",
    "    # Plot the good vs bad channels for each recording in a dual bar chart\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.bar(range(len(good_channels)), good_channels, label='Good Channels')\n",
    "    ax.bar(range(len(bad_channels)), bad_channels, bottom=good_channels, label='Bad Channels')\n",
    "    ax.set_xlabel('Recording')\n",
    "    ax.set_ylabel('Number of Channels')\n",
    "    ax.axhline(raw.info['nchan'] * good_threshold, color='green', linestyle='--')\n",
    "    title = f'Good vs Bad Channels (T = {cv_threshold})\\nGood Recordings: {good_recordings}, N = {len(raws)}, Retention Rate: {good_recordings / len(raws) * 100:.2f}%'\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "    plt.savefig(f'plots/signal quality/Signal Quality (CV).png', dpi=dpi)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Peak Power/SCI Sliding Window CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_peak_power_sci_df:\n",
    "    peak_power_df = pd.DataFrame()\n",
    "    sci_df = pd.DataFrame()\n",
    "    for i, raw_od in enumerate(raw_ods, 1):\n",
    "        raw_od_annotated_pp, scores_pp, times_pp = peak_power(raw_od, time_window=5, verbose=False)\n",
    "        raw_od_annotated_sci, scores_sci, times_sci = scalp_coupling_index_windowed(raw_od, time_window=5, verbose=False)\n",
    "\n",
    "        # Convert scores and times to a DataFrame\n",
    "        df_pp = pd.DataFrame(scores_pp.T, columns=[ch_name for ch_name in raw_od.ch_names])\n",
    "        df_sci = pd.DataFrame(scores_sci.T, columns=[ch_name for ch_name in raw_od.ch_names])\n",
    "\n",
    "        # Add time window information\n",
    "        df_pp[\"Start_Time\"] = [t[0] for t in times_pp]\n",
    "        df_pp[\"End_Time\"] = [t[1] for t in times_pp]\n",
    "        df_sci[\"Start_Time\"] = [t[0] for t in times_sci]\n",
    "        df_sci[\"End_Time\"] = [t[1] for t in times_sci]\n",
    "\n",
    "        # Add an index column for window number\n",
    "        df_pp.insert(0, 'Window', range(1, len(df_pp) + 1))\n",
    "        df_sci.insert(0, 'Window', range(1, len(df_sci) + 1))\n",
    "\n",
    "        # Reorder columns so time comes first\n",
    "        df_pp = df_pp[[\"Start_Time\", \"End_Time\"] + list(df_pp.columns[:-2])]\n",
    "        df_sci = df_sci[[\"Start_Time\", \"End_Time\"] + list(df_sci.columns[:-2])]\n",
    "\n",
    "        # remove the columns with '850' in the name\n",
    "        df_pp = df_pp.loc[:, ~df_pp.columns.str.contains('850')]\n",
    "        df_sci = df_sci.loc[:, ~df_sci.columns.str.contains('850')]\n",
    "\n",
    "        # rename the columns to remove the ' 760' at the end if it exists\n",
    "        df_pp.columns = [col[:-4] if col.endswith(' 760') else col for col in df_pp.columns]\n",
    "        df_sci.columns = [col[:-4] if col.endswith(' 760') else col for col in df_sci.columns]\n",
    "\n",
    "        # Add a column for participant number, make it the first column\n",
    "        df_pp.insert(0, 'Participant', i)\n",
    "        df_sci.insert(0, 'Participant', i)\n",
    "\n",
    "        # Append the DataFrame to the list\n",
    "        peak_power_df = pd.concat([peak_power_df, df_pp])\n",
    "        sci_df = pd.concat([sci_df, df_sci])\n",
    "        print(f\"Processed participant {i}\")\n",
    "\n",
    "    # reset the index\n",
    "    peak_power_df.reset_index(drop=True, inplace=True)\n",
    "    sci_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    peak_power_df.to_csv('processed_data/windows/peak_power.csv', index=False)\n",
    "    sci_df.to_csv('processed_data/windows/sci.csv', index=False)\n",
    "\n",
    "# Load the DataFrame\n",
    "peak_power_df = pd.read_csv('processed_data/windows/peak_power.csv')\n",
    "sci_df = pd.read_csv('processed_data/windows/sci.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peak Power/SCI Sliding Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_good_windows_plot:\n",
    "    # Compute the proportion of windows with peak power > peak_power_threshold for each channel\n",
    "    percentage_good_windows_peak_power_df = (\n",
    "        peak_power_df.groupby(\"Participant\")[peak_power_df.columns[4:]]\n",
    "        .apply(lambda df: (df > peak_power_threshold).sum() / len(df))\n",
    "    )\n",
    "\n",
    "    # add a good recordings column\n",
    "    good_recordings = (percentage_good_windows_peak_power_df > good_threshold).sum(axis=1) / len(percentage_good_windows_peak_power_df.columns)\n",
    "    percentage_good_windows_peak_power_df.insert(0, f'Good Recordings (peak_power > {peak_power_threshold} for > {good_threshold * 100}% of channels)', good_recordings)\n",
    "\n",
    "    # Compute the proportion of windows with SCI > good_threshold for each channel\n",
    "    percentage_good_windows_sci_df = (\n",
    "        sci_df.groupby(\"Participant\")[sci_df.columns[4:]]\n",
    "        .apply(lambda df: (df >= sci_threshold).sum() / len(df))\n",
    "    )\n",
    "\n",
    "    # add a good recordings column\n",
    "    good_recordings = (percentage_good_windows_sci_df > good_threshold).sum(axis=1) / len(percentage_good_windows_sci_df.columns)\n",
    "    percentage_good_windows_sci_df.insert(0, f'Good Recordings (SCI > {sci_threshold} for > {good_threshold * 100}% of channels)', good_recordings)\n",
    "\n",
    "    # merge the two dataframes on the first 2 columns\n",
    "    percentage_good_windows_df = pd.merge(percentage_good_windows_peak_power_df[percentage_good_windows_peak_power_df.columns[:1]], percentage_good_windows_sci_df[percentage_good_windows_sci_df.columns[:1]], on='Participant')\n",
    "\n",
    "    # create a new column that is true if both columns are greater than good_threshold\n",
    "    percentage_good_windows_df['Good Recording'] = (percentage_good_windows_df.iloc[:, 0] > good_threshold) & (percentage_good_windows_df.iloc[:, 1] > good_threshold)\n",
    "\n",
    "    # save the dataframe to a csv file\n",
    "    percentage_good_windows_df.to_csv('processed_data/windows/percentage_good_windows.csv', index=False)\n",
    "\n",
    "    # Plot a bar chart where the SCI and peak power windows are compared, two bars next to each other for each participant\n",
    "    fig, ax = plt.subplots(figsize=(24, 8))\n",
    "    bar_width = 0.45\n",
    "    x = np.arange(len(percentage_good_windows_df))\n",
    "    ax.bar(x, percentage_good_windows_df.iloc[:, 0], bar_width, label='Peak Power')\n",
    "    ax.bar(x + bar_width, percentage_good_windows_df.iloc[:, 1], bar_width, label='SCI')\n",
    "    ax.axhline(good_threshold, color='green', linestyle='--')\n",
    "    ax.set_xlabel('Participant')\n",
    "    ax.set_ylabel('Percentage of Good Windows')\n",
    "    title = f'Percentage of Good Windows: peak_power > {peak_power_threshold}, SCI > {sci_threshold}\\nGood Recordings: {percentage_good_windows_df[\"Good Recording\"].sum()}, N = {len(percentage_good_windows_df)}, Retention Rate: {percentage_good_windows_df[\"Good Recording\"].sum() / len(percentage_good_windows_df) * 100:.2f}%'\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(x + bar_width / 2)\n",
    "    ax.set_xticklabels(percentage_good_windows_df.index)\n",
    "    ax.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/signal quality/Percentage of Good Windows.png', dpi=dpi / 2)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peak Power/SCI Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_slope_pp_sci_plot:\n",
    "    pp_channel_slopes = []\n",
    "    sci_channel_slopes = []\n",
    "    for pp_channel, sci_channel in zip(percentage_good_windows_peak_power_df.columns[4:], percentage_good_windows_sci_df.columns[4:]):\n",
    "        pp_slope = []\n",
    "        sci_slope = []\n",
    "        for participant in peak_power_df['Participant'].unique():\n",
    "            # get the data for the channel\n",
    "            pp_array = peak_power_df[peak_power_df['Participant'] == participant][pp_channel]\n",
    "            sci_array = sci_df[sci_df['Participant'] == participant][sci_channel]\n",
    "\n",
    "            # get a line of best fit for the data\n",
    "            x = np.arange(len(pp_array))\n",
    "            pp_m, pp_b = np.polyfit(x, pp_array, 1)\n",
    "            pp_slope.append(pp_m)\n",
    "\n",
    "            sci_m, sci_b = np.polyfit(x, sci_array, 1)\n",
    "            sci_slope.append(sci_m)\n",
    "        pp_channel_slopes.append((pp_channel, np.mean(pp_slope)))\n",
    "        sci_channel_slopes.append((sci_channel, np.mean(sci_slope)))\n",
    "\n",
    "    # plot the slopes\n",
    "    fig, ax = plt.subplots(figsize=(20, 6))\n",
    "    # sort the channels by slope\n",
    "    pp_channel_slopes.sort(key=lambda x: x[1])\n",
    "    sci_channel_slopes.sort(key=lambda x: x[1])\n",
    "    bar_width = 0.45\n",
    "    x = np.arange(len(pp_channel_slopes))\n",
    "    ax.bar(x, [slope for channel, slope in pp_channel_slopes], bar_width, label='Peak Power')\n",
    "    ax.bar(x + bar_width, [slope for channel, slope in sci_channel_slopes], bar_width, label='SCI')\n",
    "    ax.set_xlabel('Channel')\n",
    "    ax.set_ylabel('Slope')\n",
    "    ax.set_title('Slope of Peak Power and SCI over Time')\n",
    "    ax.set_xticks(x + bar_width / 2)\n",
    "    ax.set_xticklabels([channel for channel, slope in pp_channel_slopes])\n",
    "    ax.legend()\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/signal quality/Slope of Peak Power and SCI over Time.png', dpi=dpi)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Head Size vs. SCI Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_head_size_vs_sci_plot:\n",
    "    cap_size = 58\n",
    "    percentage_good_windows_sci_df_with_head_size = percentage_good_windows_sci_df.copy()\n",
    "\n",
    "    # add the head size to the percentage_good_windows_sci_df as the first column\n",
    "    if 'Head Size (cm)' not in percentage_good_windows_sci_df_with_head_size.columns:\n",
    "        percentage_good_windows_sci_df_with_head_size.insert(0, 'Head Size (cm)', cap_size)\n",
    "\n",
    "    no_head_size = []\n",
    "\n",
    "    for i, raw_haemo in enumerate(raw_haemos, 1):\n",
    "        # get the head size\n",
    "        head_size = get_info(raw_haemo)['remarks']\n",
    "        if head_size:\n",
    "            head_size = float(head_size) * 2.54\n",
    "        else:\n",
    "            # add the participant number to the no_head_size list\n",
    "            no_head_size.append(i)\n",
    "            continue\n",
    "        \n",
    "        # append the head_size to percentage_good_windows_sci_df\n",
    "        percentage_good_windows_sci_df_with_head_size.loc[i, 'Head Size (cm)'] = head_size\n",
    "\n",
    "    # remove the participants with no head size\n",
    "    percentage_good_windows_sci_df_with_head_size = percentage_good_windows_sci_df_with_head_size.drop(no_head_size)\n",
    "\n",
    "    # get the correlation between head size and the channels\n",
    "    correlations = []\n",
    "    for channel in percentage_good_windows_sci_df_with_head_size.columns[2:]:\n",
    "        correlation = percentage_good_windows_sci_df_with_head_size['Head Size (cm)'].corr(percentage_good_windows_sci_df_with_head_size[channel])\n",
    "        correlations.append((channel, correlation))\n",
    "\n",
    "    # get the correlation between head size and the second column\n",
    "    correlation = percentage_good_windows_sci_df_with_head_size['Head Size (cm)'].corr(percentage_good_windows_sci_df_with_head_size.iloc[:, 1])\n",
    "\n",
    "    # plot the correlations\n",
    "    fig, ax = plt.subplots(figsize=(20, 6))\n",
    "    # sort the channels by correlation\n",
    "    correlations.sort(key=lambda x: x[1])\n",
    "    bar_width = 0.45\n",
    "    x = np.arange(len(correlations))\n",
    "    ax.bar(x, [correlation for channel, correlation in correlations], bar_width)\n",
    "    ax.set_xlabel('Channel')\n",
    "    ax.set_ylabel('Correlation')\n",
    "    ax.set_title('Correlation between Head Size and SCI, Correlation with Good Recordings: ' + str(correlation) + ', N = ' + str(len(percentage_good_windows_sci_df_with_head_size)))\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([channel for channel, correlation in correlations])\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/signal quality/Correlation between Head Size and SCI.png', dpi=dpi / 3)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average SCI per Channel across Participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_across_participant_sci_plots:\n",
    "    # get a list of participants in percentage_good_windows_sci_df where Good Recording is True\n",
    "    good_participants = percentage_good_windows_df[percentage_good_windows_df['Good Recording'] == True].index\n",
    "\n",
    "    # make a dataframe of the average sci for each channel\n",
    "    avg_sci_df = sci_df.groupby('Participant').mean().drop(columns=['Window', 'Start_Time', 'End_Time'])\n",
    "\n",
    "    # drop the participants that are not in good_participants\n",
    "    avg_sci_df_good = avg_sci_df.loc[good_participants]\n",
    "\n",
    "    # drop the participants that are in good_participants\n",
    "    avg_sci_df_bad = avg_sci_df.drop(index=good_participants)\n",
    "\n",
    "    # make a list of the dataframes\n",
    "    avg_sci_dfs = [avg_sci_df, avg_sci_df_good, avg_sci_df_bad]\n",
    "    df_names = ['All Participants', 'Good Participants', 'Bad Participants']\n",
    "    color_list = ['red', 'blue', 'green', 'purple', 'orange', 'brown', 'pink', 'gray']\n",
    "\n",
    "    for df, df_name in zip(avg_sci_dfs, df_names):\n",
    "\n",
    "        # make a violin plot of the average sci for each channel\n",
    "        fig, ax = plt.subplots(figsize=(35, 6))\n",
    "        parts = ax.violinplot(df, showmeans=False, widths=1, showextrema=False)\n",
    "\n",
    "        # match the violin plot colors to the columns in avg_sci_df to the channels in ch_mapping_names\n",
    "        color_i = 0\n",
    "        colors = []\n",
    "        region_labels = []\n",
    "\n",
    "        # for each region in ch_mapping_names, apply the color to the channels in that region\n",
    "        for region, channels in ch_mapping_names.items():\n",
    "            for channel in channels:\n",
    "                if channel in df.columns:\n",
    "                    colors.append(color_list[color_i])\n",
    "                    region_labels.append(region)\n",
    "            color_i += 1\n",
    "\n",
    "        # set the colors of the violins\n",
    "        for i, pc in enumerate(parts['bodies']):\n",
    "            pc.set_facecolor(colors[i])\n",
    "            pc.set_edgecolor('black')\n",
    "            pc.set_alpha(1)\n",
    "\n",
    "        # create a legend\n",
    "        handles = [plt.Rectangle((0, 0), 1, 1, color=color) for color in list(dict.fromkeys(colors))]\n",
    "        ax.legend(handles, list(dict.fromkeys(region_labels)), loc='lower left')\n",
    "\n",
    "        # add a white scatter plot of the mean sci for each channel\n",
    "        ax.scatter(np.arange(1, len(df.columns) + 1), df.mean(), color='white', zorder=3)\n",
    "\n",
    "        ax.set_xlabel('Channel')\n",
    "        ax.set_ylabel('Average SCI')\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.axhline(good_threshold, color='green', linestyle='--')\n",
    "        ax.set_title(f'Average SCI per Channel: ({df_name}), N = {len(df)}')\n",
    "        ax.set_xticks(np.arange(1, len(df.columns) + 1))\n",
    "        ax.set_xticklabels(df.columns)\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'plots/signal quality/Average SCI (Windowed) per Channel/{df_name}.png', dpi=dpi / 3)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCI of Windows per Channel for each Participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_participant_sci_plots:\n",
    "    for i in sci_df['Participant'].unique():\n",
    "        df = sci_df[sci_df['Participant'] == i]\n",
    "\n",
    "        # get the good recording status\n",
    "        good_recording = False\n",
    "        if percentage_good_windows_df['Good Recording'][i]:\n",
    "            good_recording = True\n",
    "\n",
    "        # make a violin plot of the sci for each channel\n",
    "        fig, ax = plt.subplots(figsize=(35, 6))\n",
    "        parts = ax.violinplot(df[df.columns[4:]], showmeans=False, widths=1, showextrema=False)\n",
    "\n",
    "        # match the violin plot colors to the columns in sci_df to the channels in ch_mapping_names\n",
    "        color_i = 0\n",
    "        colors = []\n",
    "        region_labels = []\n",
    "\n",
    "        # for each region in ch_mapping_names, apply the color to the channels in that region\n",
    "        for region, channels in ch_mapping_names.items():\n",
    "            for channel in channels:\n",
    "                if channel in df.columns:\n",
    "                    colors.append(color_list[color_i])\n",
    "                    region_labels.append(region)\n",
    "            color_i += 1\n",
    "\n",
    "        # set the colors of the violins\n",
    "        for j, pc in enumerate(parts['bodies']):\n",
    "            pc.set_facecolor(colors[j])\n",
    "            pc.set_edgecolor('black')\n",
    "            pc.set_alpha(1)\n",
    "\n",
    "        # create a legend\n",
    "        handles = [plt.Rectangle((0, 0), 1, 1, color=color) for color in list(dict.fromkeys(colors))]\n",
    "        ax.legend(handles, list(dict.fromkeys(region_labels)), loc='lower left')\n",
    "\n",
    "        # add a white scatter plot of the mean sci for each channel\n",
    "        ax.scatter(np.arange(1, len(df.columns) - 3), df[df.columns[4:]].mean(), color='white', zorder=3)\n",
    "\n",
    "        ax.set_xlabel('Channel')\n",
    "        ax.set_ylabel('SCI')\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.axhline(good_threshold, color='green', linestyle='--')\n",
    "        ax.set_title(f'SCI per Channel: Participant {i}, Windows = {len(df)}, Good Recording = {good_recording}')\n",
    "        ax.set_xticks(np.arange(1, len(df.columns) - 3))\n",
    "        ax.set_xticklabels(df.columns[4:])\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'plots/signal quality/Average SCI (Windowed) per Channel/individual/Participant {i}.png', dpi=dpi / 3)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Good Recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the percentage_good_windows_df and assert that it is not empty\n",
    "percentage_good_windows_df = pd.read_csv('processed_data/windows/percentage_good_windows.csv')\n",
    "assert not percentage_good_windows_df.empty\n",
    "\n",
    "raw_haemo_good_recordings = []\n",
    "for i, raw_haemo in enumerate(raw_haemos, 1):\n",
    "    if len(percentage_good_windows_df) >= i:\n",
    "        if percentage_good_windows_df.iloc[i - 1]['Good Recording']:\n",
    "            raw_haemo_good = raw_haemo.copy().reorder_channels(all_channels)\n",
    "            raw_haemo_good_recordings.append(raw_haemo_good)\n",
    "\n",
    "len(raw_haemo_good_recordings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Epoch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = ['face_type', 'emotion']\n",
    "conditions_list = {\n",
    "    'face_type': ['Real', 'Virt'],\n",
    "    'emotion': ['Joy', 'Fear', 'Anger', 'Disgust', 'Sadness', 'Neutral', 'Surprise']\n",
    "}\n",
    "channel_types = ['hbo', 'hbr', 'hbt']\n",
    "\n",
    "if get_epochs:\n",
    "    for mode in modes:\n",
    "        for condition in conditions_list[mode]:\n",
    "            participants = []\n",
    "            for raw_haemo in raw_haemo_good_recordings:\n",
    "                epochs = relabel_annotations(raw_haemo.copy(), mode=mode)[condition]\n",
    "                channels = []\n",
    "                for channel_type in channel_types:\n",
    "                    epoch_channel = pick_channels(epochs, channel_type)\n",
    "                    channels.append(epoch_channel.get_data())\n",
    "\n",
    "                participants.append(np.array(channels))\n",
    "            participants = np.array(participants)\n",
    "            participants = np.moveaxis(participants, 1, 2)\n",
    "            np.save(f'processed_data/epochs/{mode}_{condition}.npy', participants)\n",
    "\n",
    "# We now have epochs[mode][condition].shape = (n_participants, n_epochs, n_channel_types, n_channels, n_times)\n",
    "epochs = {\n",
    "    mode: {\n",
    "        condition: np.load(f'processed_data/epochs/{mode}_{condition}.npy') \n",
    "        if os.path.exists(f'processed_data/epochs/{mode}_{condition}.npy') \n",
    "        else None\n",
    "        for condition in conditions_list[mode]\n",
    "    }\n",
    "    for mode in modes\n",
    "}\n",
    "\n",
    "dict_channel_types = {\n",
    "    channel_type: idx for idx, channel_type in enumerate(channel_types)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Type/Emotion Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = ['face_type_emotion']\n",
    "conditions_list = {\n",
    "    'face_type': ['Real', 'Virt'],\n",
    "    'emotion': ['Joy', 'Fear', 'Anger', 'Disgust', 'Sadness', 'Neutral', 'Surprise'],\n",
    "}\n",
    "conditions_list['face_type_emotion'] = [''.join(pair) for pair in itertools.product(conditions_list['face_type'], conditions_list['emotion'])]\n",
    "channel_types = ['hbo', 'hbr', 'hbt']\n",
    "\n",
    "if get_face_type_emotion_epochs:\n",
    "    for mode in modes:\n",
    "        for condition in conditions_list[mode]:\n",
    "            all_condition_list = []\n",
    "            for i, raw_haemo in enumerate(raw_haemo_good_recordings, 1):\n",
    "                epochs = relabel_annotations(raw_haemo.copy(), mode=mode)\n",
    "\n",
    "                if condition not in list(set(epochs.annotations.description)):\n",
    "                    print(f'Participant {i} missing {condition}')\n",
    "                    continue\n",
    "\n",
    "                epochs = epochs[condition]\n",
    "\n",
    "                channels = []\n",
    "                for channel_type in channel_types:\n",
    "                    epoch_channel = pick_channels(epochs, channel_type)\n",
    "                    channels.append(epoch_channel.get_data())\n",
    "\n",
    "                all_condition_list.append(np.array(channels))\n",
    "                    \n",
    "            all_conds = None\n",
    "            for cond in all_condition_list:\n",
    "                if all_conds is None:\n",
    "                    all_conds = cond\n",
    "                else:\n",
    "                    all_conds = np.concatenate((all_conds, cond), axis=1)\n",
    "            all_conds = np.moveaxis(all_conds, 0, 1)\n",
    "            np.save(f'processed_data/epochs/{mode}_{condition}.npy', all_conds)\n",
    "\n",
    "# We now have face_emotion_epochs[mode][condition].shape = (n_epochs, n_channel_types, n_channels, n_times)\n",
    "face_emotion_epochs = {\n",
    "    mode: {\n",
    "        condition: np.load(f'processed_data/epochs/{mode}_{condition}.npy') \n",
    "        if os.path.exists(f'processed_data/epochs/{mode}_{condition}.npy') \n",
    "        else None\n",
    "        for condition in conditions_list[mode]\n",
    "    }\n",
    "    for mode in modes\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLM analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = ['face_type', 'emotion', 'neutral_vs_emotion']\n",
    "conditions_list = {\n",
    "    'face_type': ['Real', 'Virt', 'Base'],\n",
    "    'emotion': ['Joy', 'Fear', 'Anger', 'Disgust', 'Sadness', 'Neutral', 'Surprise'],\n",
    "    'neutral_vs_emotion': ['Emotion', 'Neutral', 'Base']\n",
    "}\n",
    "\n",
    "if get_glm_analysis:\n",
    "    channel_types = ['hbo', 'hbr', 'hbt']\n",
    "    for mode in modes:\n",
    "        cha_df = pd.DataFrame()\n",
    "        roi_df = pd.DataFrame()\n",
    "        con_df = pd.DataFrame()\n",
    "        for i, raw_haemo in enumerate(raw_haemo_good_recordings, 1):\n",
    "            raw_haemo_annots = pick_channels(raw_haemo, channel_types)\n",
    "            relabel_annotations(raw_haemo_annots, mode=mode)\n",
    "\n",
    "            # Create a design matrix\n",
    "            design_matrix = make_first_level_design_matrix(\n",
    "                raw_haemo_annots,\n",
    "                drift_model=\"cosine\",\n",
    "                high_pass=0.03125,  # The cutoff period (1/high_pass) should be set as the longest period between two trials of the same condition multiplied by 2\n",
    "                hrf_model=\"spm\",\n",
    "                stim_dur=16.0,\n",
    "            )\n",
    "            \n",
    "            # Run GLM\n",
    "            glm_est = run_glm(raw_haemo_annots, design_matrix, n_jobs=n_jobs)\n",
    "\n",
    "            cha = glm_est.to_dataframe()\n",
    "\n",
    "            # in ch_mapping_all, for each list of channels in the dict, each string is formatted as 'S{number}_D{number} {hbo/hbr}', extract the number from the string and replace the string with [number, number]\n",
    "            groups = {region: [[int(re.findall(r'\\d+', channel)[0]), int(re.findall(r'\\d+', channel)[1])] for channel in ch_mapping_all[region]] for region in ch_mapping_all}\n",
    "            # apply picks_pair_to_idx to each region in groups\n",
    "            for region in groups:\n",
    "                groups[region] = picks_pair_to_idx(raw_haemo_annots, groups[region], on_missing='ignore')\n",
    "\n",
    "            # Compute region of interest results from channel data\n",
    "            roi = glm_est.to_dataframe_region_of_interest(\n",
    "                groups, design_matrix.columns, demographic_info=True\n",
    "            )\n",
    "\n",
    "            # Define contrasts\n",
    "            contrast_matrix = np.eye(design_matrix.shape[1])\n",
    "            basic_conts = dict(\n",
    "                [(column, contrast_matrix[j]) for j, column in enumerate(design_matrix.columns)]\n",
    "            )\n",
    "            contrasts = []\n",
    "            unique_annots = np.unique(raw_haemo_annots.annotations.description).tolist()\n",
    "            pairs = list(itertools.combinations(unique_annots, 2))\n",
    "            # include the opposite of each pair\n",
    "            pairs = pairs + [(pair[1], pair[0]) for pair in pairs]\n",
    "\n",
    "            # Compute defined contrast pairs\n",
    "            for pair in pairs:\n",
    "                con = glm_est.compute_contrast(basic_conts[pair[0]] - basic_conts[pair[1]]).to_dataframe()\n",
    "                con[\"contrast_pair\"] = f\"{pair[0]} - {pair[1]}\"\n",
    "                contrasts.append(con)\n",
    "\n",
    "            # Add the participant ID to the dataframes\n",
    "            roi[\"Participant\"] = cha[\"Participant\"] = i\n",
    "            for con in contrasts:\n",
    "                con[\"Participant\"] = i\n",
    "\n",
    "            # Convert to uM for nicer plotting below.\n",
    "            cha[\"theta\"] = [t * 1.0e6 for t in cha[\"theta\"]]\n",
    "            roi[\"theta\"] = [t * 1.0e6 for t in roi[\"theta\"]]\n",
    "            for con in contrasts:\n",
    "                con[\"effect\"] = [t * 1.0e6 for t in con[\"effect\"]]\n",
    "\n",
    "            # Append the dataframes to the main dataframes\n",
    "            cha_df = pd.concat([cha_df, cha])\n",
    "            roi_df = pd.concat([roi_df, roi])\n",
    "            for con in contrasts:\n",
    "                con_df = pd.concat([con_df, con])\n",
    "\n",
    "        cha_df.to_csv('processed_data/glm/cha/cha_df_' + mode + '.csv', index=False)\n",
    "        roi_df.to_csv('processed_data/glm/roi/roi_df_' + mode + '.csv', index=False)\n",
    "        con_df.to_csv('processed_data/glm/cons/con_df_' + mode + '.csv', index=False)\n",
    "\n",
    "# load the dataframes\n",
    "glm = {\n",
    "    mode: {\n",
    "        'cha': pd.read_csv(f'processed_data/glm/cha/cha_df_{mode}.csv'),\n",
    "        'roi': pd.read_csv(f'processed_data/glm/roi/roi_df_{mode}.csv'),\n",
    "        'con': pd.read_csv(f'processed_data/glm/cons/con_df_{mode}.csv')\n",
    "    }\n",
    "    for mode in modes\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual GLM Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_ind_glm_plots:\n",
    "    channel_types = ['hbo', 'hbr', 'hbt']\n",
    "    for mode in modes:\n",
    "        grp_results = glm[mode]['roi'].query(f\"Condition in {conditions_list[mode]}\")\n",
    "        grp_results = grp_results.query(f\"Chroma in {channel_types}\")\n",
    "\n",
    "        theta_min = grp_results['theta'].min()\n",
    "        theta_max = grp_results['theta'].max()\n",
    "\n",
    "        # clear any files in the plots/glm/individual folder\n",
    "        for f in os.listdir('plots/glm/individual_' + mode):\n",
    "            os.remove(os.path.join('plots/glm/individual_' + mode, f))\n",
    "\n",
    "        for i in grp_results['Participant'].unique():\n",
    "            fig, axes = plt.subplots(1, len(channel_types), figsize=(18, 6), sharey=True)\n",
    "            fig.suptitle(f'GLM Results for Participant {i}')\n",
    "\n",
    "            for ax, channel_type in zip(axes, channel_types):\n",
    "                sns.swarmplot(data=grp_results.query(f\"Participant == {i} and Chroma == '{channel_type}'\"), \n",
    "                              x='Condition', y='theta', hue='ROI', ax=ax, dodge=False)\n",
    "                ax.set_title(f'{channel_type}')\n",
    "                ax.set_ylabel('Theta (uM)')\n",
    "                ax.set_ylim(theta_min, theta_max)\n",
    "                ax.set_xlabel('Condition')\n",
    "                ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "            plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "            plt.savefig(f'plots/glm/individual_{mode}/Participant {i}.png', dpi=dpi / 4)\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group GLM Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_group_glm_plots:\n",
    "    channel_types = ['hbo', 'hbr', 'hbt']\n",
    "    for mode in modes:\n",
    "        grp_results = glm[mode]['roi'].query(f\"Condition in {conditions_list[mode]}\")\n",
    "        grp_results = grp_results.query(f\"Chroma in {channel_types}\")\n",
    "\n",
    "        # Run a GLM model\n",
    "        roi_model = mixedlm(\"theta ~ -1 + ROI:Condition:Chroma\", grp_results, groups=grp_results[\"Participant\"]).fit(method=\"nm\")\n",
    "\n",
    "        # Get the results of the model and put it in a csv file\n",
    "        roi_model_results = statsmodels_to_results(roi_model)\n",
    "\n",
    "        # plot the results of the model\n",
    "        fig, axes = plt.subplots(1, len(channel_types), figsize=(18, 6), sharey=True)\n",
    "        fig.suptitle(f'GLM Results for Group')\n",
    "\n",
    "        for ax, channel_type in zip(axes, channel_types):\n",
    "            sns.swarmplot(data=roi_model_results.query(f\"Chroma == '{channel_type}'\"), x='Condition', y='Coef.', hue='ROI', ax=ax, dodge=False)\n",
    "            ax.set_title(f'{channel_type}')\n",
    "            ax.set_ylabel('Theta (uM)')\n",
    "            ax.set_xlabel('Condition')\n",
    "            ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        \n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "        plt.savefig(f'plots/glm/group_results/results_{mode}.png', dpi=dpi / 4)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Contrasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add hbt as plot_glm_group_topo does not support hbt\n",
    "\n",
    "if get_group_contrast_plots:\n",
    "\n",
    "    # pick channel type\n",
    "    channel_type = 'hbo'\n",
    "\n",
    "    for mode in modes:\n",
    "        pairs = list(itertools.combinations(conditions_list[mode], 2))\n",
    "        for pair in pairs:\n",
    "            con_summary = glm[mode]['con'].query(f\"contrast_pair == '{pair[0]} - {pair[1]}'\")\n",
    "\n",
    "            if len(con_summary) == 0:\n",
    "                print(f\"No data for contrast {pair[0]} - {pair[1]}\")\n",
    "                continue\n",
    "\n",
    "            raw_haemo_channel = pick_channels(raw_haemo, channel_type)\n",
    "\n",
    "            con_summary_channel = con_summary.query(f\"Chroma in {[channel_type]}\")\n",
    "\n",
    "            # Run group level model and convert to dataframe\n",
    "            con_model = mixedlm(\"effect ~ -1 + ch_name:Chroma\", con_summary_channel, groups=con_summary_channel[\"Participant\"]).fit(method=\"nm\")\n",
    "\n",
    "            # Get the results of the model\n",
    "            con_model_df = statsmodels_to_results(con_model, order=raw_haemo_channel.ch_names)\n",
    "\n",
    "            # Set the limits for the colorbar\n",
    "            vlim = max(abs(con_model_df['Coef.'].min()), abs(con_model_df['Coef.'].max()))\n",
    "            vlim = (-vlim, vlim)\n",
    "\n",
    "            # Plot the topographic map\n",
    "            fig, ax = plt.subplots()\n",
    "            plot_glm_group_topo(\n",
    "                raw_haemo_channel, con_model_df.query(f\"Chroma == '{channel_type}'\"), colorbar=True, extrapolate=\"head\", threshold=True, vlim=vlim, axes=ax\n",
    "            )\n",
    "\n",
    "            # Add a title to the plot\n",
    "            ax.set_title(f\"Contrast: {pair[0]} > {pair[1]}\")\n",
    "\n",
    "            plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "            if \"Neutral\" in pair:\n",
    "                plt.savefig(f'plots/glm/contrasts/differences_neutral/Contrast_{pair[0]}-{pair[1]}.png', dpi=dpi / 4)\n",
    "            else:\n",
    "                plt.savefig(f'plots/glm/contrasts/differences/Contrast_{pair[0]}-{pair[1]}.png', dpi=dpi / 4)\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROI Timeseries Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_roi_timeseries_activity:\n",
    "    channel_types = ['hbo', 'hbr', 'hbt']\n",
    "    \n",
    "    # create an empty dataframe\n",
    "    roi_timeseries_activity = pd.DataFrame()\n",
    "\n",
    "    tmin = 4\n",
    "    tmax = 16\n",
    "\n",
    "    i = 1\n",
    "    for raw_haemo in raw_haemo_good_recordings:\n",
    "        face_epochs = relabel_annotations(raw_haemo.copy(), mode='face_type')\n",
    "        emotion_epochs = relabel_annotations(raw_haemo.copy(), mode='emotion')\n",
    "\n",
    "        # crop the epochs to tmin-tmax\n",
    "        face_epochs.crop(tmin=tmin, tmax=tmax)\n",
    "\n",
    "        # convert the epochs to a dataframe\n",
    "        face_epochs_df = face_epochs.to_data_frame()\n",
    "\n",
    "        # remove the baseline condition\n",
    "        face_epochs_df = face_epochs_df.where(face_epochs_df['condition'] != 'Base').dropna()\n",
    "\n",
    "        # average these columns: column_names[3:].tolist(), by the epoch column and condition column\n",
    "        face_epochs_df = face_epochs_df.groupby(['epoch', 'condition']).mean().reset_index()\n",
    "\n",
    "        # crop the epochs to tmix-tmax\n",
    "        emotion_epochs.crop(tmin=tmin, tmax=tmax)\n",
    "\n",
    "        # convert the epochs to a dataframe\n",
    "        emotion_epochs_df = emotion_epochs.to_data_frame()\n",
    "\n",
    "        # remove the baseline condition\n",
    "        emotion_epochs_df = emotion_epochs_df.where(emotion_epochs_df['condition'] != 'Base').dropna()\n",
    "\n",
    "        # average these columns: column_names[3:].tolist(), by the epoch column and condition column\n",
    "        emotion_epochs_df = emotion_epochs_df.groupby(['epoch', 'condition']).mean().reset_index()\n",
    "\n",
    "        # add the condition column from the emotion_epochs_df to the face_epochs_df and line it up with the epoch column\n",
    "        face_epochs_df['emotion'] = emotion_epochs_df['condition']\n",
    "\n",
    "        # put the emotion column in the third column\n",
    "        all_epochs_df = face_epochs_df[['epoch', 'condition', 'emotion'] + face_epochs_df.columns[2:-1].tolist()]\n",
    "\n",
    "        # rename the condition column to face type\n",
    "        all_epochs_df.rename(columns={'condition': 'face type'}, inplace=True)\n",
    "\n",
    "        # divide the epoch column by 2 and floor it and convert it to an integer\n",
    "        all_epochs_df['epoch'] = (all_epochs_df['epoch'] // 2).astype(int)\n",
    "\n",
    "        # remove the time column\n",
    "        all_epochs_df.drop(columns='time', inplace=True)\n",
    "\n",
    "        if 'hbo' in channel_types:\n",
    "            for region, channels in ch_mapping_hbo.items():\n",
    "                # Ensure the channels exist in the dataframe to avoid errors\n",
    "                valid_channels = [channel for channel in channels if channel in all_epochs_df.columns]\n",
    "                if valid_channels:\n",
    "                    # Create a new column for the region's average\n",
    "                    all_epochs_df[region + ' Average Hbo'] = all_epochs_df[valid_channels].mean(axis=1)\n",
    "\n",
    "        if 'hbr' in channel_types:\n",
    "            for region, channels in ch_mapping_hbr.items():\n",
    "                # Ensure the channels exist in the dataframe to avoid errors\n",
    "                valid_channels = [channel for channel in channels if channel in all_epochs_df.columns]\n",
    "                if valid_channels:\n",
    "                    # Create a new column for the region's average\n",
    "                    all_epochs_df[region + ' Average Hbr'] = all_epochs_df[valid_channels].mean(axis=1)\n",
    "\n",
    "        if 'hbt' in channel_types:\n",
    "            for region, channels in ch_mapping_hbt.items():\n",
    "                # Ensure the channels exist in the dataframe to avoid errors\n",
    "                valid_channels = [channel for channel in channels if channel in all_epochs_df.columns]\n",
    "                if valid_channels:\n",
    "                    # Create a new column for the region's average\n",
    "                    all_epochs_df[region + ' Average Hbt'] = all_epochs_df[valid_channels].mean(axis=1)\n",
    "\n",
    "        # drop all the channel columns\n",
    "        all_epochs_df.drop(columns=all_channels, inplace=True)\n",
    "\n",
    "        # add participant number column\n",
    "        all_epochs_df['Participant'] = i\n",
    "\n",
    "        # make the participant number the first column\n",
    "        all_epochs_df = all_epochs_df[['Participant'] + all_epochs_df.columns[:-1].tolist()]\n",
    "\n",
    "        # add measurement date column\n",
    "        #all_epochs_df['Measurement Date'] = raw_haemo.info['meas_date']\n",
    "\n",
    "        # add an empty column for repetition and put it after the emotion column\n",
    "        all_epochs_df.insert(4, 'Repetition', '')\n",
    "\n",
    "        conditions = defaultdict(int)\n",
    "        for index, row in all_epochs_df.iterrows():\n",
    "            # add the condition-emotion pair to the conditions dictionary and increment the count\n",
    "            conditions[f\"{row['face type']}-{row['emotion']}\"] += 1\n",
    "\n",
    "            # add the count to the repetition column\n",
    "            all_epochs_df.at[index, 'Repetition'] = conditions[f\"{row['face type']}-{row['emotion']}\"]\n",
    "\n",
    "        # put it after the repetition column\n",
    "        all_epochs_df.insert(5, 'Sex', '')\n",
    "\n",
    "        # add the sex column\n",
    "        all_epochs_df['Sex'] = get_info(raw_haemo)['gender']\n",
    "\n",
    "        # add the dataframe to the average_timeseries_activity datafram\n",
    "        roi_timeseries_activity = pd.concat([roi_timeseries_activity, all_epochs_df])\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    # reset the index\n",
    "    roi_timeseries_activity.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # name the index column to 'observation'\n",
    "    roi_timeseries_activity.index.name = 'Observation'\n",
    "\n",
    "    # replace any spaces in the column names with underscores\n",
    "    roi_timeseries_activity.columns = roi_timeseries_activity.columns.str.replace(' ', '_')\n",
    "\n",
    "    # capitalize the column names\n",
    "    roi_timeseries_activity.columns = roi_timeseries_activity.columns.str.capitalize()\n",
    "\n",
    "    mappings = {}\n",
    "    for col in roi_timeseries_activity.select_dtypes(include=['object']).columns:\n",
    "        # Create a mapping dictionary for the column\n",
    "        unique_values = roi_timeseries_activity[col].unique()\n",
    "        col_mapping = {val: idx for idx, val in enumerate(unique_values)}\n",
    "        mappings[col] = col_mapping\n",
    "        \n",
    "        # Replace the column values in the DataFrame with numeric values\n",
    "        roi_timeseries_activity[col] = roi_timeseries_activity[col].map(col_mapping)\n",
    "\n",
    "    # Save mappings to a JSON file\n",
    "    with open('processed_data/roi_timeseries_activity/mappings.json', 'w') as json_file:\n",
    "        json.dump(mappings, json_file, indent=4)\n",
    "\n",
    "    # Get the unique number of participants\n",
    "    num_participants = roi_timeseries_activity['Participant'].nunique()\n",
    "\n",
    "    # save the dataframe to a csv file\n",
    "    roi_timeseries_activity.to_csv(f'processed_data/roi_timeseries_activity/roi_timeseries_activity_sci{sci_threshold}_n{num_participants}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ERP Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = ['face_type', 'emotion']\n",
    "conditions_list = {\n",
    "    'face_type': ['Real', 'Virt'],\n",
    "    'emotion': ['Joy', 'Fear', 'Anger', 'Disgust', 'Sadness', 'Neutral', 'Surprise']\n",
    "}\n",
    "channel_types = ['hbo', 'hbr', 'hbt']\n",
    "\n",
    "if get_erp_plots:\n",
    "    # make a color list for the different channel types\n",
    "    colors = {\n",
    "        'hbo': 'red',\n",
    "        'hbr': 'blue',\n",
    "        'hbt': 'purple'\n",
    "    }\n",
    "\n",
    "    # pick a channel type to plot the difference between regions\n",
    "    difference_channel_type = 'hbo'\n",
    "\n",
    "    for mode in modes:\n",
    "        for condition in conditions_list[mode]:\n",
    "\n",
    "            fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "            for channel_type in channel_types:\n",
    "                # get epoch data, shape is now (n_participants, n_times)\n",
    "                epoch_data = np.mean(epochs[mode][condition][:, :, dict_channel_types[channel_type]], axis=(1, 2))\n",
    "                epoch_data_mean = np.mean(epoch_data, axis=0)\n",
    "                \n",
    "                # get the standard error of the mean\n",
    "                epoch_data_sem = np.std(epoch_data, axis=0) / np.sqrt(epoch_data.shape[0])\n",
    "\n",
    "                # plot the mean\n",
    "                ax.plot(epoch_data_mean, label=channel_type, color=colors[channel_type])\n",
    "\n",
    "                # plot the standard error of the mean as a shaded area\n",
    "                ax.fill_between(np.arange(epoch_data_mean.shape[0]), epoch_data_mean - epoch_data_sem, epoch_data_mean + epoch_data_sem, color=colors[channel_type], alpha=0.3)\n",
    "            \n",
    "            ax.set_xlabel('Time (s)')\n",
    "            ax.set_ylabel('uM')\n",
    "            ax.set_title(f'ERP Plot for {mode}: {condition}, N = {len(epochs[mode][condition])}, Shaded Area: SEM')\n",
    "            ax.legend()\n",
    "            ax.set_ylim(-3e-6, 3e-6) # set the ylims to -+3 * 10^-6\n",
    "            \n",
    "            # set the xticks to the time points, 0-16 seconds, instead of 99 time points\n",
    "            ax.set_xticks(np.arange(0, 99, 6))\n",
    "            ax.set_xticklabels(np.arange(0, 17, 1))\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f'plots/erp/erp_conditions/{condition}.png', dpi=dpi / 2)\n",
    "            plt.close()\n",
    "\n",
    "        for cond1, cond2 in itertools.combinations(conditions_list[mode], 2):\n",
    "            # create a 8x1 subplot that shares the x-axis and i can iterate over\n",
    "            fig, axes = plt.subplots(8, 1, figsize=(12, 24), sharex=True)        \n",
    "\n",
    "            # in ch_mapping_all, for each list of channels in the dict, filter for the channels with difference_channel_type in the string\n",
    "            groups = {region: [channel for channel in channels if difference_channel_type in channel] for region, channels in ch_mapping_all.items()}\n",
    "            group_i = 0\n",
    "            for region, ch_name in groups.items():\n",
    "                # Get the indices for the current region from group_boundaries\n",
    "                keys_list = list(groups.keys())\n",
    "                start_idx = group_boundaries[keys_list.index(region)]\n",
    "                end_idx = group_boundaries[keys_list.index(region) + 1] if keys_list.index(region) + 1 < len(group_boundaries) else None\n",
    "\n",
    "                # get the data for the current region\n",
    "                epoch_data_region1 = np.mean(epochs[mode][cond1][:, :, dict_channel_types[difference_channel_type], start_idx:end_idx], axis=(0, 1))\n",
    "                epoch_data_region2 = np.mean(epochs[mode][cond2][:, :, dict_channel_types[difference_channel_type], start_idx:end_idx], axis=(0, 1))\n",
    "\n",
    "                # get the mean of the data\n",
    "                epoch_data_region1_mean = np.mean(epoch_data_region1, axis=0)\n",
    "                epoch_data_region2_mean = np.mean(epoch_data_region2, axis=0)\n",
    "\n",
    "                # get the standard error of the mean\n",
    "                epoch_data_region1_sem = np.std(epoch_data_region1, axis=0) / np.sqrt(epoch_data_region1.shape[0])\n",
    "                epoch_data_region2_sem = np.std(epoch_data_region2, axis=0) / np.sqrt(epoch_data_region2.shape[0])\n",
    "\n",
    "                # plot the mean\n",
    "                axes[group_i].plot(epoch_data_region1_mean, label=cond1, color='red')\n",
    "                axes[group_i].plot(epoch_data_region2_mean, label=cond2, color='blue')\n",
    "\n",
    "                # plot the standard error of the mean as a shaded area\n",
    "                axes[group_i].fill_between(np.arange(epoch_data_region1_mean.shape[0]), epoch_data_region1_mean - epoch_data_region1_sem, epoch_data_region1_mean + epoch_data_region1_sem, color='red', alpha=0.3)\n",
    "                axes[group_i].fill_between(np.arange(epoch_data_region2_mean.shape[0]), epoch_data_region2_mean - epoch_data_region2_sem, epoch_data_region2_mean + epoch_data_region2_sem, color='blue', alpha=0.3)\n",
    "\n",
    "                # set the title of the subplot to the region\n",
    "                if group_i == 0:\n",
    "                    axes[group_i].set_title(f'ERP Plot for {mode}: {cond1} - {cond2}, N = {len(epochs[mode][cond1])}, Shaded Area: SEM\\n{region}')\n",
    "                else:\n",
    "                    axes[group_i].set_title(region)\n",
    "                \n",
    "                # set the ylims to -+7 * 10^-6\n",
    "                axes[group_i].set_ylim(-7e-6, 7e-6)\n",
    "\n",
    "                # add a legend to the subplot\n",
    "                axes[group_i].legend()\n",
    "\n",
    "                group_i += 1\n",
    "\n",
    "            axes[-1].set_xlabel('Time (s)')\n",
    "            axes[-1].set_xticks(np.arange(0, 99, 6))\n",
    "            axes[-1].set_xticklabels(np.arange(0, 17, 1))\n",
    "            axes[-1].set_ylabel('uM')\n",
    "            plt.tight_layout()\n",
    "            if \"Neutral\" == cond1 or \"Neutral\" == cond2:\n",
    "                plt.savefig(f'plots/erp/erp_differences_neutral/{cond1}_{cond2}.png', dpi=dpi / 2)\n",
    "            else:\n",
    "                plt.savefig(f'plots/erp/erp_differences/{cond1}_{cond2}.png', dpi=dpi / 2)\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connectivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = ['all', 'face_type', 'emotion']\n",
    "conditions_list = {\n",
    "    'all': ['Blck', 'Base'],\n",
    "    'face_type': ['Real', 'Virt', 'Base'],\n",
    "    'emotion': ['Joy', 'Fear', 'Anger', 'Disgust', 'Sadness', 'Neutral', 'Surprise']\n",
    "}\n",
    "\n",
    "connectivity_method = [spectral_connectivity_time, spectral_connectivity_epochs]\n",
    "\n",
    "# pick the channels\n",
    "channel_types = ['hbo', 'hbr', 'hbt']\n",
    "\n",
    "# pick the connectivity method\n",
    "method = \"coh\"\n",
    "\n",
    "# pick the mode\n",
    "con_mode = \"cwt_morlet\"\n",
    "\n",
    "# pick the frequency range\n",
    "cwt_freqs = np.linspace(0.1, 0.5, 5)\n",
    "\n",
    "# pick the number of cycles\n",
    "cwt_n_cycles = 1\n",
    "\n",
    "# average the connectivity matrices across frequencies\n",
    "faverage = True\n",
    "\n",
    "if run_ind_connectivity:\n",
    "    for func in connectivity_method:\n",
    "        for mode in modes:\n",
    "            for condition in conditions_list[mode]:\n",
    "                # create an empty list to store the connectivity for each participant\n",
    "                participant_cons = []\n",
    "\n",
    "                # for each raw_haemo in raw_haemo_good_recordings, compute the connectivity\n",
    "                for i, raw_haemo in enumerate(raw_haemo_good_recordings, 1):\n",
    "\n",
    "                    # relabel the annotations and pick the condition\n",
    "                    epochs = relabel_annotations(raw_haemo.copy(), mode=mode)[condition]\n",
    "\n",
    "                    # create an empty list to store each channel type's connectivity\n",
    "                    cons = []\n",
    "\n",
    "                    # for each channel type in channel_types\n",
    "                    for channel_type in channel_types:\n",
    "                        # pick the channels\n",
    "                        epochs_channel = pick_channels(epochs, channel_type)\n",
    "                    \n",
    "                        # use func to compute the connectivity\n",
    "                        if func.__name__ == 'spectral_connectivity_epochs':\n",
    "                            con = func(\n",
    "                                epochs_channel,\n",
    "                                method=method,\n",
    "                                mode=con_mode,\n",
    "                                cwt_freqs=cwt_freqs,\n",
    "                                cwt_n_cycles=cwt_n_cycles,\n",
    "                                faverage=faverage,\n",
    "                                n_jobs=n_jobs,\n",
    "                                verbose=False\n",
    "                            )\n",
    "\n",
    "                            data = con.get_data()\n",
    "                            data = np.moveaxis(np.moveaxis(data, 1, 2), 0, 1)\n",
    "                        elif func.__name__ == 'spectral_connectivity_time':\n",
    "                            con = func(\n",
    "                                epochs_channel,\n",
    "                                method=method,\n",
    "                                mode=con_mode,\n",
    "                                freqs=cwt_freqs,\n",
    "                                n_cycles=cwt_n_cycles,\n",
    "                                faverage=faverage,\n",
    "                                n_jobs=n_jobs,\n",
    "                                verbose=False\n",
    "                            )\n",
    "\n",
    "                            data = con.get_data()\n",
    "                        else:\n",
    "                            raise ValueError('func must be spectral_connectivity_epochs or spectral_connectivity_time')\n",
    "\n",
    "                        # append the connectivity to the cons list\n",
    "                        cons.append(data)\n",
    "\n",
    "                    participant_cons.append(np.array(cons))\n",
    "                    \n",
    "                # save the connectivity to disk\n",
    "                np.save(f'processed_data\\\\{func.__name__}\\\\individual_cons\\\\{mode}_{condition}_con.npy', np.array(participant_cons))\n",
    "\n",
    "        # make a dictionary to store the connectivity parameters\n",
    "        ind_connectivity_params = {\n",
    "            \"func\": func.__name__,\n",
    "            \"channel_types\": channel_types,\n",
    "            \"method\": method,\n",
    "            \"con_mode\": con_mode,\n",
    "            \"cwt_freqs\": cwt_freqs.tolist(),\n",
    "            \"cwt_n_cycles\": cwt_n_cycles,\n",
    "            \"faverage\": faverage,\n",
    "            \"ch_names\": pick_channels(raw_haemo, channel_types).ch_names\n",
    "        }\n",
    "\n",
    "        # save the connectivity parameters to disk in preprocessed_data\\connectivity\n",
    "        with open(f\"processed_data\\\\{func.__name__}\\\\ind_connectivity_params.json\", \"w\") as f:\n",
    "            json.dump(ind_connectivity_params, f)\n",
    "\n",
    "# load the numpy files from disk so we have ind_con[func_name][mode][condition].shape = (n_participants, n_channel_types, n_epochs OR n_times, n_channels, n_freqs\n",
    "ind_con = {\n",
    "    func.__name__: {\n",
    "        mode: {\n",
    "            condition: np.load(f'processed_data\\\\{func.__name__}\\\\individual_cons\\\\{mode}_{condition}_con.npy')\n",
    "            for condition in conditions_list[mode]\n",
    "        }\n",
    "        for mode in modes\n",
    "    }\n",
    "    for func in connectivity_method\n",
    "}\n",
    "\n",
    "ind_connectivity_params = {\n",
    "    func.__name__: json.load(open(f\"processed_data\\\\{func.__name__}\\\\ind_connectivity_params.json\", \"r\"))\n",
    "    for func in connectivity_method\n",
    "}\n",
    "\n",
    "dict_channel_types = {\n",
    "    channel_type: idx for idx, channel_type in enumerate(channel_types)\n",
    "}\n",
    "\n",
    "channel_type = 'hbo'\n",
    "participants = ind_con[connectivity_method[0].__name__][modes[0]][conditions_list[modes[0]][0]].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Condition Connectivity Heatmap/Chord Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = ['face_type', 'emotion']\n",
    "conditions_list = {\n",
    "    'face_type': ['Real', 'Virt', 'Base'],\n",
    "    'emotion': ['Joy', 'Fear', 'Anger', 'Disgust', 'Sadness', 'Neutral', 'Surprise']\n",
    "}\n",
    "\n",
    "if get_condition_con_plots:\n",
    "    # make a color scheme\n",
    "    colorscheme = dict(\n",
    "        facecolor='white',\n",
    "        textcolor='black',\n",
    "        colormap='hot',\n",
    "        facecolor2='black',\n",
    "        textcolor2='white',\n",
    "    )\n",
    "\n",
    "    # get the node angles\n",
    "    node_angles = circular_layout(\n",
    "        ch_names_original, all_channels_names, start_pos=90, group_boundaries=group_boundaries\n",
    "    )\n",
    "\n",
    "    for func in connectivity_method:\n",
    "        for mode in modes:\n",
    "            for condition in conditions_list[mode]:\n",
    "                averaged_data = np.mean(ind_con[func.__name__][mode][condition][:, dict_channel_types[channel_type]], axis=(0, 1, 3))\n",
    "                \n",
    "                # Get the grid size\n",
    "                grid_size = int(np.sqrt(averaged_data.size))\n",
    "\n",
    "                # Reshape the data into a 2D grid\n",
    "                heatmap_data = averaged_data.reshape((grid_size, grid_size))\n",
    "\n",
    "                # Make the matrix symmetric\n",
    "                symmetric_data = heatmap_data + heatmap_data.T\n",
    "\n",
    "                # Set the diagonal to the highest value\n",
    "                np.fill_diagonal(symmetric_data, np.max(symmetric_data))\n",
    "\n",
    "                # Plot the heatmap\n",
    "                fig, ax = plt.subplots(figsize=(25, 25))\n",
    "                im = ax.imshow(symmetric_data, cmap='viridis')\n",
    "                ax.set_title(f'Connectivity Heatmap for {func.__name__}, Mode: {mode}, Condition: {condition}, Channel Type: {channel_type}')\n",
    "                ax.set_xlabel('Channel')\n",
    "                ax.set_ylabel('Channel')\n",
    "                ax.set_xticks(np.arange(grid_size))\n",
    "                ax.set_yticks(np.arange(grid_size))\n",
    "                ch_names = [ch_name for ch_name in ind_connectivity_params[func.__name__]['ch_names'] if channel_type in ch_name]\n",
    "                ax.set_xticklabels(ch_names)\n",
    "                ax.set_yticklabels(ch_names)\n",
    "                plt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "                cbar = fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "                cbar.set_label('Connectivity Value')\n",
    "                plt.savefig(f'plots/{func.__name__}/heatmaps/conditions/{mode}_{condition}_con.png', dpi=dpi / 4)\n",
    "                plt.close()\n",
    "\n",
    "                # find the min of the averaged_data, when dropping all the 0s\n",
    "                min_val = np.min(averaged_data[np.nonzero(averaged_data)])\n",
    "\n",
    "                # find the max of the averaged_data, when dropping all the 0s\n",
    "                max_val = np.max(averaged_data[np.nonzero(averaged_data)])\n",
    "\n",
    "                # Plot the connectivity circle\n",
    "                plot_connectivity_circle(\n",
    "                    heatmap_data,\n",
    "                    node_names=ch_names_original,\n",
    "                    node_angles=node_angles,\n",
    "                    n_lines=10000,\n",
    "                    title=f'{func.__name__}, {condition}, {channel_type}',\n",
    "                    colorbar_size=1,\n",
    "                    fontsize_colorbar=12,\n",
    "                    facecolor=colorscheme['facecolor'],\n",
    "                    textcolor=colorscheme['textcolor'],\n",
    "                    colormap=colorscheme['colormap'],\n",
    "                    padding=3,\n",
    "                    vmin=min_val,\n",
    "                    vmax=max_val,\n",
    "                    fontsize_title=12,\n",
    "                    colorbar=True,\n",
    "                    show=False\n",
    "                )\n",
    "                plt.savefig(f'plots/{func.__name__}/chord_plots/conditions/{mode}_{condition}_con.png', dpi=dpi / 4)\n",
    "                plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Condition Connectivity Heatmaps Across Time GIFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_condition_con_gif_plots:\n",
    "    for mode in modes:\n",
    "        for condition in conditions_list[mode]:\n",
    "            # the shape of averaged_data is (n_times, n_channels)\n",
    "            averaged_data = ind_con['spectral_connectivity_epochs'][mode][condition][:, dict_channel_types[channel_type], :, :, 0].mean(axis=0)\n",
    "\n",
    "            # reshape the data to (n_times, sqrt(n_channels), sqrt(n_channels))\n",
    "            averaged_data = averaged_data.reshape(averaged_data.shape[0], int(np.sqrt(averaged_data.shape[1])), int(np.sqrt(averaged_data.shape[1])))\n",
    "\n",
    "            # make the matrix symmetric for the (sqrt(n_channels), sqrt(n_channels)) part\n",
    "            for matrix in averaged_data:\n",
    "                matrix += matrix.T\n",
    "                np.fill_diagonal(matrix, np.max(matrix))\n",
    "\n",
    "            # create a funcanimation object to animate the connectivity across time\n",
    "            fig, ax = plt.subplots(figsize=(12, 10))\n",
    "            im = ax.imshow(averaged_data[0], cmap='viridis', animated=True)\n",
    "            ax.set_title(f'{mode} {condition} {channel_type} - Frame 1/{averaged_data.shape[0]}')\n",
    "            ax.set_xlabel('Channel')\n",
    "            ax.set_ylabel('Channel')\n",
    "\n",
    "            plt.colorbar(im)\n",
    "            plt.tight_layout()\n",
    "\n",
    "            def update(frame):\n",
    "                im.set_array(averaged_data[frame])\n",
    "                ax.set_title(f'{mode} {condition} {channel_type} - Time Point: {frame + 1}/{averaged_data.shape[0]}')\n",
    "                return im,\n",
    "\n",
    "            ani = FuncAnimation(fig, update, frames=averaged_data.shape[0], blit=True)\n",
    "            ani.save(f'plots/spectral_connectivity_epochs/heatmaps/gifs/{mode}_{condition}.gif', writer='pillow', fps=8, dpi=dpi / 4)\n",
    "            plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Condition Connectivity Variance Across Participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_variance_con_plots:\n",
    "    for mode in modes:\n",
    "        vmax = None\n",
    "        for condition in conditions_list[mode]:\n",
    "            # the shape of averaged_data is (n_participants, n_channels)\n",
    "            averaged_data = ind_con['spectral_connectivity_time'][mode][condition][:, dict_channel_types[channel_type], :, :, 0].mean(axis=1)\n",
    "\n",
    "            # reshape the data to (n_participants, n_channels, n_channels)\n",
    "            averaged_data = averaged_data.reshape(averaged_data.shape[0], int(np.sqrt(averaged_data.shape[1])), int(np.sqrt(averaged_data.shape[1])))\n",
    "\n",
    "            # make the matrix symmetric for the (sqrt(n_channels), sqrt(n_channels)) part\n",
    "            for matrix in averaged_data:\n",
    "                matrix += matrix.T\n",
    "                np.fill_diagonal(matrix, np.max(matrix))\n",
    "\n",
    "            # create a matrix of the variance of each channel pair over all participants\n",
    "            variance_matrix = np.var(averaged_data, axis=0)\n",
    "\n",
    "            # Plot the heatmap\n",
    "            fig, ax = plt.subplots(figsize=(10, 10))\n",
    "            im = ax.imshow(variance_matrix, cmap='viridis')\n",
    "            ax.set_title(f'Variance Heatmap for Mode: {mode}, Condition: {condition}, Channel Type: {channel_type}')\n",
    "            ax.set_xlabel('Channel')\n",
    "            ax.set_ylabel('Channel')\n",
    "            \n",
    "            cbar = plt.colorbar(im, ax=ax)\n",
    "            cbar.set_label('Variance Value')\n",
    "            if vmax is None:\n",
    "                vmax = variance_matrix.max()\n",
    "            \n",
    "            im.set_clim(0, vmax)\n",
    "            plt.tight_layout()\n",
    "\n",
    "            plt.savefig(f'plots/spectral_connectivity_time/heatmaps/variance/{mode}_{condition}_con.png', dpi=dpi / 4)\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Level Connectivity t-tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connectivity_method = [spectral_connectivity_time, spectral_connectivity_epochs]\n",
    "\n",
    "modes = ['face_type', 'emotion']\n",
    "conditions_list = {\n",
    "    'face_type': ['Real', 'Virt', 'Base'],\n",
    "    'emotion': ['Joy', 'Fear', 'Anger', 'Disgust', 'Sadness', 'Neutral', 'Surprise']\n",
    "}\n",
    "\n",
    "if run_group_level_t_tests:\n",
    "    for func in connectivity_method:\n",
    "        for mode in modes:\n",
    "            for condition1, condition2 in itertools.combinations(conditions_list[mode], 2):\n",
    "                # Get the data for the two conditions\n",
    "                data1 = np.mean(ind_con[func.__name__][mode][condition1][:, dict_channel_types[channel_type]], axis=(1, 3))\n",
    "                data2 = np.mean(ind_con[func.__name__][mode][condition2][:, dict_channel_types[channel_type]], axis=(1, 3))\n",
    "\n",
    "                # reshape the data to (num_participants, int(np.sqrt(num_channel_connections)), int(np.sqrt(num_channel_connections)))\n",
    "                data1 = data1.reshape(data1.shape[0], int(np.sqrt(data1.shape[1])), int(np.sqrt(data1.shape[1])))\n",
    "                data2 = data2.reshape(data2.shape[0], int(np.sqrt(data2.shape[1])), int(np.sqrt(data2.shape[1])))\n",
    "\n",
    "                # ensure the data shapes are the same\n",
    "                assert data1.shape == data2.shape\n",
    "\n",
    "                # ensure the data is not empty or contains NaNs\n",
    "                assert not np.isnan(data1).any()\n",
    "                assert not np.isnan(data2).any()\n",
    "\n",
    "                # apply fisher z transform to the data\n",
    "                data1 = np.arctanh(data1)\n",
    "                data2 = np.arctanh(data2)\n",
    "\n",
    "                # Initialize matrices to store t and p-values\n",
    "                t_vals = np.zeros(data1.shape[1:])\n",
    "                p_values = np.zeros(data1.shape[1:])\n",
    "\n",
    "                # Iterate over each pair of channels\n",
    "                for i in range(data1.shape[1]):\n",
    "                    for j in range(i): # because the matrix is lower triangular\n",
    "                        # Run the t-test\n",
    "                        t_val, p_val = stats.ttest_rel(data1[:, i, j], data2[:, i, j])\n",
    "\n",
    "                        # Store the results\n",
    "                        t_vals[i, j] = t_val\n",
    "                        p_values[i, j] = p_val\n",
    "\n",
    "                # Flatten the lower triangular valid p-values (non-nan) for FDR correction\n",
    "                valid_idx = ~np.isnan(p_values)\n",
    "                p_values_flat = p_values[valid_idx]\n",
    "\n",
    "                # Correct p-values for multiple comparisons\n",
    "                _, p_values_flat_fdr = fdrcorrection(p_values_flat)\n",
    "\n",
    "                # Reshape the corrected p-values back to matrix form\n",
    "                p_values_fdr = p_values_flat_fdr.reshape(p_values.shape)\n",
    "\n",
    "                # Save the p-values to disk\n",
    "                np.save(f'processed_data\\\\{func.__name__}\\\\paired_t_tests\\\\{mode}_{condition1}_{condition2}_p_values.npy', p_values_fdr)\n",
    "\n",
    "# Load the p-values from disk\n",
    "group_level_t_tests_p_values = {\n",
    "    func.__name__: {\n",
    "        mode: {\n",
    "            f\"{condition1}_{condition2}\": np.load(f'processed_data\\\\{func.__name__}\\\\paired_t_tests\\\\{mode}_{condition1}_{condition2}_p_values.npy')\n",
    "            for condition1, condition2 in itertools.combinations(conditions_list[mode], 2)\n",
    "        }\n",
    "        for mode in modes\n",
    "    }\n",
    "    for func in connectivity_method\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Level Connectivity t-test Chord Plots\n",
    "Remember to edit the plot_connectvity_circle function to include the diagonals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connectivity_method = [spectral_connectivity_time, spectral_connectivity_epochs]\n",
    "modes = ['face_type', 'emotion']\n",
    "conditions_list = {\n",
    "    'face_type': ['Real', 'Virt', 'Base'],\n",
    "    'emotion': ['Joy', 'Fear', 'Anger', 'Disgust', 'Sadness', 'Neutral', 'Surprise']\n",
    "}\n",
    "\n",
    "if get_group_level_t_tests_chord_plots:\n",
    "\n",
    "    # make a color scheme\n",
    "    colorscheme = dict(\n",
    "        facecolor='white',\n",
    "        textcolor='black',\n",
    "        colormap='hot',\n",
    "        facecolor2='black',\n",
    "        textcolor2='white',\n",
    "    )\n",
    "\n",
    "    # get the node angles\n",
    "    node_angles = circular_layout(\n",
    "        ch_names_original, all_channels_names, start_pos=90, group_boundaries=group_boundaries\n",
    "    )\n",
    "    \n",
    "    for func in connectivity_method:\n",
    "        for mode in modes:\n",
    "            for condition1, condition2 in itertools.combinations(conditions_list[mode], 2):\n",
    "                p_values = group_level_t_tests_p_values[func.__name__][mode][f\"{condition1}_{condition2}\"]\n",
    "\n",
    "                # apply –log₁₀(p) to the p-values\n",
    "                p_values = -np.log10(p_values)\n",
    "\n",
    "                # set any p-values that are < -log₁₀(0.05) to 0\n",
    "                p_values = np.where(p_values < -np.log10(0.05), 0, p_values)\n",
    "\n",
    "                # Set the diagonal to 0\n",
    "                np.fill_diagonal(p_values, 0)\n",
    "\n",
    "                plot_connectivity_circle(\n",
    "                    p_values,\n",
    "                    node_names=ch_names_original,\n",
    "                    node_angles=node_angles,\n",
    "                    n_lines=10000,\n",
    "                    title=f'{func.__name__}, {condition1} - {condition2}, {channel_type}, -log₁₀(p)',\n",
    "                    colorbar_size=1,\n",
    "                    fontsize_colorbar=12,\n",
    "                    facecolor=colorscheme['facecolor'],\n",
    "                    textcolor=colorscheme['textcolor'],\n",
    "                    colormap=colorscheme['colormap'],\n",
    "                    padding=3,\n",
    "                    vmin=0,\n",
    "                    vmax=-np.log10(0.0005),\n",
    "                    fontsize_title=16,\n",
    "                    colorbar=True,\n",
    "                    show=False\n",
    "                )\n",
    "                plt.savefig(f'plots/{func.__name__}/chord_plots/group_level_t_tests/{mode}_{condition1}_{condition2}_p_values.png', dpi=dpi / 4)\n",
    "                plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Level Connectivity t-test ROI Chord Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = ['face_type', 'emotion']\n",
    "conditions_list = {\n",
    "    'face_type': ['Real', 'Virt', 'Base'],\n",
    "    'emotion': ['Joy', 'Fear', 'Anger', 'Disgust', 'Sadness', 'Neutral', 'Surprise']\n",
    "}\n",
    "\n",
    "if get_group_level_t_tests_roi_chord_plots:\n",
    "    # Get the names of the regions\n",
    "    regions = list(ch_mapping_names.keys())\n",
    "\n",
    "    colorscheme = dict(\n",
    "        facecolor='white',\n",
    "        textcolor='black',\n",
    "        colormap='Reds',\n",
    "        facecolor2='black',\n",
    "        textcolor2='white',\n",
    "    )\n",
    "\n",
    "    # get the node angles\n",
    "    node_angles = circular_layout(\n",
    "        regions, regions, start_pos=90\n",
    "    )\n",
    "\n",
    "    # First, flatten the dictionary into a list of tuples (channel_name, region)\n",
    "    flat_list = [(channel, region) for region, channels in ch_mapping_names.items() for channel in channels]\n",
    "\n",
    "    # Now, replace the channel name with its enumeration (starting from 0)\n",
    "    reshaped_list = [(i, region) for i, (_, region) in enumerate(flat_list)]\n",
    "\n",
    "    for func in connectivity_method:\n",
    "        for mode in modes:\n",
    "            for condition1, condition2 in itertools.combinations(conditions_list[mode], 2):\n",
    "                p_values_by_roi = {}\n",
    "\n",
    "                for i, row in enumerate(group_level_t_tests_p_values[func.__name__][mode][f'{condition1}_{condition2}']):\n",
    "                    region_i = reshaped_list[i][1]\n",
    "                    for j, element in enumerate(row):\n",
    "                        region_j = reshaped_list[j][1]\n",
    "\n",
    "                        # skip the zeros\n",
    "                        if element == 0:\n",
    "                            continue\n",
    "\n",
    "                        # Store the significant p-values for each ROI pair\n",
    "                        if f'{region_i}_{region_j}' not in p_values_by_roi:\n",
    "                            p_values_by_roi[f'{region_i}_{region_j}'] = [element]\n",
    "                        else:\n",
    "                            p_values_by_roi[f'{region_i}_{region_j}'].append(element)\n",
    "\n",
    "                for roi_pair, p_values in p_values_by_roi.items():\n",
    "                    # filter out the p-values that are > 0.05\n",
    "                    p_values_by_roi[roi_pair] = [p_value for p_value in p_values if p_value < 0.05]\n",
    "\n",
    "                    # Get the count of p-values for each ROI pair\n",
    "                    p_values_by_roi[roi_pair] = len(p_values_by_roi[roi_pair])\n",
    "\n",
    "                # Create a matrix filled with NaNs\n",
    "                matrix = np.full((len(regions), len(regions)), np.nan)\n",
    "\n",
    "                # Fill the matrix using the dictionary values\n",
    "                for key, value in p_values_by_roi.items():\n",
    "                    r1, r2 = key.split('_')\n",
    "                    i = regions.index(r1)\n",
    "                    j = regions.index(r2)\n",
    "                    # Fill the lower triangular part of the matrix\n",
    "                    if i > j:\n",
    "                        matrix[i, j] = value\n",
    "                    else:\n",
    "                        matrix[j, i] = value\n",
    "\n",
    "                # replace any 0s with NaNs\n",
    "                matrix[matrix == 0] = np.nan\n",
    "\n",
    "                # Set the number of discrete colors to the highest number of significant p-values\n",
    "                discrete_cmap = cm.get_cmap('Reds', np.nanmax(matrix) + 1)\n",
    "\n",
    "                # Plot the connectivity circle\n",
    "                plot_connectivity_circle(\n",
    "                    matrix,\n",
    "                    node_names=regions,\n",
    "                    node_angles=node_angles,\n",
    "                    n_lines=10000,\n",
    "                    title=f'Number of significant p_values for {func.__name__}\\n{mode}, {condition1} - {condition2}',\n",
    "                    colorbar_size=1,\n",
    "                    fontsize_colorbar=12,\n",
    "                    node_width=40,\n",
    "                    linewidth=4.5,\n",
    "                    facecolor=colorscheme['facecolor'],\n",
    "                    textcolor=colorscheme['textcolor'],\n",
    "                    colormap=discrete_cmap,\n",
    "                    padding=3,\n",
    "                    fontsize_title=12,\n",
    "                    fontsize_names=10,\n",
    "                    colorbar=True,\n",
    "                    show=False\n",
    "                )\n",
    "                plt.savefig(f'plots/{func.__name__}/chord_plots/group_level_t_tests_roi/{mode}_{condition1}_{condition2}.png', dpi=dpi / 4)\n",
    "                plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "modes = ['all', 'face_type', 'emotion']\n",
    "\n",
    "conditions_list = {\n",
    "    'all': ['Blck', 'Base'],\n",
    "    'face_type': ['Real', 'Virt'],\n",
    "    'emotion': ['Joy', 'Fear', 'Anger', 'Disgust', 'Sadness', 'Neutral', 'Surprise']\n",
    "}\n",
    "\n",
    "channel_types = ['hbo', 'hbr', 'hbt']\n",
    "\n",
    "dict_channel_types = {\n",
    "    channel_type: idx for idx, channel_type in enumerate(channel_types)\n",
    "}\n",
    "\n",
    "channel_type = 'hbo'\n",
    "\n",
    "func = spectral_connectivity_time\n",
    "\n",
    "# load the numpy files from disk so we have ind_con[func_name][mode][condition].shape = (n_participants, n_channel_types, n_epochs, n_channels)\n",
    "ind_con = {\n",
    "    mode: {\n",
    "        condition: np.load(f'processed_data\\\\{func.__name__}\\\\individual_cons\\\\{mode}_{condition}_con.npy')\n",
    "        for condition in conditions_list[mode]\n",
    "    }\n",
    "    for mode in modes\n",
    "}\n",
    "\n",
    "def plot_history(history, mode, mean_score):\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Plot Loss on the primary y-axis\n",
    "    ax1.plot(history.history['loss'], label='Training Loss', color='blue', linestyle='dashed')\n",
    "    ax1.plot(history.history['val_loss'], label='Validation Loss', color='blue', linestyle='solid')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss', color='blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='blue')\n",
    "\n",
    "    # Create a second y-axis for Accuracy\n",
    "    ax2 = ax1.twinx()\n",
    "    if 'accuracy' in history.history:\n",
    "        ax2.plot(history.history['accuracy'], label='Training Accuracy', color='red', linestyle='dashed')\n",
    "        ax2.plot(history.history['val_accuracy'], label='Validation Accuracy', color='red', linestyle='solid')\n",
    "        ax2.set_ylabel('Accuracy', color='red')\n",
    "        ax2.tick_params(axis='y', labelcolor='red')\n",
    "\n",
    "    # Combine Legends\n",
    "    lines, labels = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax1.legend(lines + lines2, labels + labels2, loc='upper right')\n",
    "\n",
    "    # Title and Layout\n",
    "    plt.title(f'Training Loss & Accuracy for Mode: {mode}\\nEpochs run: {len(history.history[\"loss\"])}, Mean Score: {mean_score:.4f}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Within Participants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traditional Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_traditional_raw_within_decoding:\n",
    "    models_to_run = [\n",
    "        RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=n_jobs),\n",
    "        LogisticRegression(C=0.5, random_state=42, n_jobs=n_jobs),\n",
    "        LGBMClassifier(random_state=42, n_jobs=n_jobs),\n",
    "        KNeighborsClassifier(n_neighbors=15, n_jobs=n_jobs),\n",
    "        MLPClassifier(random_state=42, verbose=False),\n",
    "        QuadraticDiscriminantAnalysis(),\n",
    "        LinearDiscriminantAnalysis(),\n",
    "        GaussianNB()\n",
    "    ]\n",
    "\n",
    "    for model in models_to_run:\n",
    "        model_score = {mode: None for mode in modes}\n",
    "        for mode in modes:\n",
    "            st_scores = []\n",
    "            for raw_haemo in raw_haemo_good_recordings:\n",
    "                # Relabel the annotations\n",
    "                epochs = relabel_annotations(raw_haemo.copy(), mode=mode)[conditions_list[mode]]\n",
    "\n",
    "                # pick the channels\n",
    "                epochs = pick_channels(epochs, channel_type)\n",
    "\n",
    "                # Get the data and labels\n",
    "                X = epochs.get_data(copy=False)\n",
    "                y = epochs.events[:, 2]\n",
    "\n",
    "                # Reshape the data to (n_epochs, n_channels * n_times)\n",
    "                X = X.reshape(X.shape[0], -1)\n",
    "\n",
    "                # Standardize the data\n",
    "                X = StandardScaler().fit_transform(X)\n",
    "                \n",
    "                # Compute the cross-validated scores for the model\n",
    "                # Use StratifiedKFold with 5 splits\n",
    "                scores = 100 * cross_val_score(\n",
    "                    model, \n",
    "                    X,\n",
    "                    y, \n",
    "                    cv=5,\n",
    "                    n_jobs=n_jobs,\n",
    "                    scoring='accuracy',\n",
    "                    verbose=0\n",
    "                )\n",
    "\n",
    "                # Append the scores to the list\n",
    "                st_scores.append(np.mean(scores))\n",
    "\n",
    "            # Return a dictionary with each score, and the mean score\n",
    "            model_score[mode] = {\n",
    "                'participant_scores': st_scores,\n",
    "                'mean_score': np.mean(st_scores)\n",
    "            }\n",
    "\n",
    "        # Save the model scores to disk\n",
    "        with open(f'processed_data/models/raw_within_scores/{model.__class__.__name__}_scores.json', 'w') as f:\n",
    "            json.dump(model_score, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Connectivity Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traditional Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_traditional_con_within_decoding:\n",
    "    models_to_run = [\n",
    "        RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=n_jobs),\n",
    "        LogisticRegression(C=0.5, random_state=42, n_jobs=n_jobs),\n",
    "        LGBMClassifier(random_state=42, n_jobs=n_jobs),\n",
    "        KNeighborsClassifier(n_neighbors=15, n_jobs=n_jobs),\n",
    "        MLPClassifier(random_state=42, verbose=False),\n",
    "        QuadraticDiscriminantAnalysis(),\n",
    "        LinearDiscriminantAnalysis(),\n",
    "        GaussianNB()\n",
    "    ]\n",
    "\n",
    "    for model in models_to_run:\n",
    "        model_score = {mode: None for mode in modes}\n",
    "        for mode in modes:\n",
    "            st_scores = []\n",
    "            for i in range(ind_con[modes[0]][conditions_list[modes[0]][0]].shape[0]):\n",
    "                participant_conds = []\n",
    "                labels = []\n",
    "\n",
    "                for condition in conditions_list[mode]:\n",
    "                    x = ind_con[mode][condition][i, dict_channel_types[channel_type], :, :, 0]\n",
    "                    label = np.array([condition] * x.shape[0])\n",
    "                    participant_conds.append(x)\n",
    "                    labels.append(label)\n",
    "                \n",
    "                # stack the data by the first axis\n",
    "                X = np.vstack(participant_conds)\n",
    "                \n",
    "                # stack the labels by the first axis\n",
    "                y = np.hstack(labels)\n",
    "\n",
    "                # map the labels to integers\n",
    "                label_mapping = {label: idx for idx, label in enumerate(np.unique(y))}\n",
    "                y = np.array([label_mapping[label] for label in y])\n",
    "\n",
    "                # Standardize the data\n",
    "                X = StandardScaler().fit_transform(X)\n",
    "\n",
    "                # Compute the cross-validated scores for the model\n",
    "                # Use StratifiedKFold with 5 splits\n",
    "                scores = 100 * cross_val_score(\n",
    "                    model, \n",
    "                    X,\n",
    "                    y, \n",
    "                    cv=5,\n",
    "                    n_jobs=n_jobs,\n",
    "                    scoring='accuracy',\n",
    "                    verbose=0\n",
    "                )\n",
    "            \n",
    "                # Append the scores to the list\n",
    "                st_scores.append(np.mean(scores, axis=0))\n",
    "            \n",
    "            # Return a dictionary with each score, and the mean score\n",
    "            model_score[mode] = {\n",
    "                'participant_scores': st_scores,\n",
    "                'mean_score': np.mean(st_scores)\n",
    "            }\n",
    "\n",
    "        # save the model scores to disk\n",
    "        with open(f'processed_data/models/con_within_scores/{model.__class__.__name__}_scores.json', 'w') as f:\n",
    "            json.dump(model_score, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Across Participants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traditional Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_traditional_raw_across_decoding:\n",
    "    models_to_run = [\n",
    "        RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=n_jobs),\n",
    "        LogisticRegression(C=0.5, random_state=42, n_jobs=n_jobs),\n",
    "        LGBMClassifier(random_state=42, n_jobs=n_jobs),\n",
    "        KNeighborsClassifier(n_neighbors=15, n_jobs=n_jobs),\n",
    "        MLPClassifier(random_state=42, verbose=False),\n",
    "        QuadraticDiscriminantAnalysis(),\n",
    "        LinearDiscriminantAnalysis(),\n",
    "        GaussianNB()\n",
    "    ]\n",
    "\n",
    "    logo = LeaveOneGroupOut()\n",
    "\n",
    "    for model in models_to_run:\n",
    "        model_score = {mode: None for mode in modes}\n",
    "        for mode in modes:\n",
    "            X_list, y_list, groups_list = [], [], []\n",
    "            for i, raw_haemo in enumerate(raw_haemo_good_recordings, 1):\n",
    "\n",
    "                # Relabel the annotations\n",
    "                epochs = relabel_annotations(raw_haemo.copy(), mode=mode)[conditions_list[mode]]\n",
    "\n",
    "                # Select the channel\n",
    "                epochs = pick_channels(epochs, channel_type)\n",
    "\n",
    "                # Get the data (shape: n_epochs x n_channels x n_times) and labels\n",
    "                X = epochs.get_data(copy=False)\n",
    "                y = epochs.events[:, 2]\n",
    "\n",
    "                X_list.append(X)\n",
    "                y_list.append(y)\n",
    "                # Create a group array identifying the participant index for each epoch\n",
    "                groups_list.append(np.full(len(y), i))\n",
    "\n",
    "            # Concatenate data across participants\n",
    "            X = np.concatenate(X_list, axis=0)\n",
    "            y = np.concatenate(y_list, axis=0)\n",
    "            groups = np.concatenate(groups_list, axis=0)\n",
    "\n",
    "            # Reshape the data to (n_epochs, n_channels * n_times)\n",
    "            X = X.reshape(X.shape[0], -1)\n",
    "\n",
    "            # Create a pipeline that scales then fits the model.\n",
    "            pipe = make_pipeline(StandardScaler(), model)\n",
    "\n",
    "            scores = 100 * cross_val_score(pipe, \n",
    "                                        X, \n",
    "                                        y, \n",
    "                                        groups=groups, \n",
    "                                        cv=logo, \n",
    "                                        n_jobs=n_jobs, \n",
    "                                        scoring='accuracy', \n",
    "                                        verbose=0)\n",
    "            \n",
    "            # Return a dictionary with each score, and the mean score\n",
    "            model_score[mode] = {\n",
    "                'logo_scores': scores.tolist(),\n",
    "                'mean_score': np.mean(scores)\n",
    "            }\n",
    "\n",
    "        # Save the model scores to disk\n",
    "        with open(f'processed_data/models/raw_across_scores/{model.__class__.__name__}_scores.json', 'w') as f:\n",
    "            json.dump(model_score, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_dl_raw_across_decoding:\n",
    "\n",
    "    logo = LeaveOneGroupOut()\n",
    "\n",
    "    model_score = {mode: None for mode in modes}\n",
    "    for mode in modes:\n",
    "        X_list, y_list, groups_list = [], [], []\n",
    "        for i, raw_haemo in enumerate(raw_haemo_good_recordings, 1):\n",
    "            # Relabel the annotations\n",
    "            epochs = relabel_annotations(raw_haemo.copy(), mode=mode)[conditions_list[mode]]\n",
    "            \n",
    "            # Select the channel(s)\n",
    "            epochs = pick_channels(epochs, channel_type)\n",
    "            \n",
    "            # Get the data and labels:\n",
    "            # X shape: (n_epochs, n_channels, n_times)\n",
    "            X = epochs.get_data()\n",
    "            y = epochs.events[:, 2]\n",
    "            \n",
    "            X_list.append(X)\n",
    "            y_list.append(y)\n",
    "            # Assign a group ID (participant index) for each epoch\n",
    "            groups_list.append(np.full(len(y), i))\n",
    "        \n",
    "        # Concatenate data across participants\n",
    "        X = np.concatenate(X_list, axis=0)\n",
    "        y = np.concatenate(y_list, axis=0)\n",
    "\n",
    "        # Map labels to integers\n",
    "        label_mapping = {label: idx for idx, label in enumerate(np.unique(y))}\n",
    "        y = np.array([label_mapping[label] for label in y])\n",
    "\n",
    "        # Create a group array identifying the participant index for each epoch\n",
    "        groups = np.concatenate(groups_list, axis=0)\n",
    "\n",
    "        # New shape: (n_epochs, n_times, n_channels)\n",
    "        X = np.transpose(X, (0, 2, 1))\n",
    "\n",
    "        # Flatten epochs and time for scaling, then reshape back.\n",
    "        X_reshaped = X.reshape(-1, X.shape[-1])\n",
    "        X_reshaped = StandardScaler().fit_transform(X_reshaped)\n",
    "        X = X_reshaped.reshape(X.shape)\n",
    "\n",
    "        # Leave-One-Group-Out Cross Validation:\n",
    "        fold_scores = []\n",
    "        for train_idx, test_idx in logo.split(X, y, groups=groups):\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "            # Define a new model for the current fold\n",
    "            model = tf.keras.Sequential([\n",
    "                tf.keras.layers.Input(shape=(X_train.shape[1], X_train.shape[2])),\n",
    "                tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),\n",
    "                tf.keras.layers.LSTM(64, return_sequences=False, dropout=0.2, recurrent_dropout=0.2),\n",
    "                tf.keras.layers.Dense(128, activation='relu'),\n",
    "                tf.keras.layers.Dropout(0.3),\n",
    "                tf.keras.layers.Dense(64, activation='relu'),\n",
    "                tf.keras.layers.Dropout(0.3),\n",
    "                tf.keras.layers.Dense(len(np.unique(y)), activation='softmax')\n",
    "            ])\n",
    "\n",
    "            model.compile(\n",
    "                optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "\n",
    "            # Train the model on the training fold\n",
    "            history = model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                epochs=100,\n",
    "                batch_size=16,\n",
    "                validation_split=0.2,\n",
    "                callbacks=[\n",
    "                    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "                    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5)\n",
    "                ],\n",
    "                verbose=0\n",
    "            )\n",
    "\n",
    "            # Evaluate on the test fold (the left-out group)\n",
    "            score = model.evaluate(X_test, y_test, verbose=0)\n",
    "            fold_scores.append(100 * score[1])\n",
    "\n",
    "        # Return a dictionary with each score, and the mean score\n",
    "        model_score[mode] = {\n",
    "            'logo_scores': fold_scores,\n",
    "            'mean_score': np.mean(fold_scores)\n",
    "        }\n",
    "\n",
    "    # Save the model scores to disk\n",
    "    with open(f'processed_data/models/raw_across_scores/{model.__class__.__name__}_scores.json', 'w') as f:\n",
    "        json.dump(model_score, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Connectivity Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traditional Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_traditional_con_across_decoding:\n",
    "    models_to_run = [\n",
    "        RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=n_jobs),\n",
    "        LogisticRegression(C=0.5, random_state=42, n_jobs=n_jobs),\n",
    "        LGBMClassifier(random_state=42, n_jobs=n_jobs),\n",
    "        KNeighborsClassifier(n_neighbors=15, n_jobs=n_jobs),\n",
    "        MLPClassifier(random_state=42, verbose=False),\n",
    "        QuadraticDiscriminantAnalysis(),\n",
    "        LinearDiscriminantAnalysis(),\n",
    "        GaussianNB()\n",
    "    ]\n",
    "\n",
    "    logo = LeaveOneGroupOut()\n",
    "\n",
    "    for model in models_to_run:\n",
    "        model_score = {mode: None for mode in modes}\n",
    "        for mode in modes:\n",
    "            X_list, y_list, groups_list = [], [], []\n",
    "            for i in range(ind_con[modes[0]][conditions_list[modes[0]][0]].shape[0]):\n",
    "                participant_conds = []\n",
    "                labels = []\n",
    "                \n",
    "                for condition in conditions_list[mode]:\n",
    "                    x = ind_con[mode][condition][i, dict_channel_types[channel_type], :, :, 0]\n",
    "                    label = np.array([condition] * x.shape[0])\n",
    "                    participant_conds.append(x)\n",
    "                    labels.append(label)\n",
    "                \n",
    "                # Stack epochs and labels for the participant\n",
    "                X_part = np.vstack(participant_conds)\n",
    "                y_part = np.hstack(labels)\n",
    "                \n",
    "                # Map labels to integers consistently across participants\n",
    "                label_mapping = {label: idx for idx, label in enumerate(np.unique(y_part))}\n",
    "                y_part = np.array([label_mapping[label] for label in y_part])\n",
    "                \n",
    "                # Collect data across participants and assign a group label (i)\n",
    "                X_list.append(X_part)\n",
    "                y_list.append(y_part)\n",
    "                groups_list.append(np.full(len(y_part), i))\n",
    "            \n",
    "            # Concatenate all participants' data\n",
    "            X = np.concatenate(X_list, axis=0)\n",
    "            y = np.concatenate(y_list, axis=0)\n",
    "            groups = np.concatenate(groups_list, axis=0)\n",
    "\n",
    "            # Instead of scaling the entire dataset at once, we create a pipeline\n",
    "            # that scales the data within each CV fold.\n",
    "            pipe = make_pipeline(StandardScaler(), model)\n",
    "\n",
    "            scores = 100 * cross_val_score(\n",
    "                pipe,\n",
    "                X,\n",
    "                y,\n",
    "                cv=logo,\n",
    "                groups=groups,\n",
    "                scoring=\"accuracy\",\n",
    "                n_jobs=n_jobs,\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            # Return a dictionary with each score, and the mean score\n",
    "            model_score[mode] = {\n",
    "                'logo_scores': scores.tolist(),\n",
    "                'mean_score': np.mean(scores)\n",
    "            }\n",
    "        \n",
    "        # Save the model scores to disk\n",
    "        with open(f'processed_data/models/con_across_scores/{model.__class__.__name__}_scores.json', 'w') as f:\n",
    "            json.dump(model_score, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\super\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 72\u001b[39m\n\u001b[32m     65\u001b[39m model.compile(\n\u001b[32m     66\u001b[39m     optimizer=\u001b[33m'\u001b[39m\u001b[33madam\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     67\u001b[39m     loss=\u001b[33m'\u001b[39m\u001b[33msparse_categorical_crossentropy\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     68\u001b[39m     metrics=[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     69\u001b[39m )\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# Train the model using a validation split for early stopping.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     76\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     77\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mval_loss\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestore_best_weights\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mReduceLROnPlateau\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mval_loss\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfactor\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\n\u001b[32m     83\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# Evaluate on the test fold\u001b[39;00m\n\u001b[32m     86\u001b[39m score = model.evaluate(X_test, y_test, verbose=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:371\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    370\u001b[39m     callbacks.on_train_batch_begin(step)\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    372\u001b[39m     callbacks.on_train_batch_end(step, logs)\n\u001b[32m    373\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:219\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    217\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    218\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    221\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if run_dl_con_across_decoding:\n",
    "\n",
    "    logo = LeaveOneGroupOut()\n",
    "\n",
    "    model_score = {mode: None for mode in modes}\n",
    "    for mode in modes:\n",
    "        X_list, y_list, groups_list = [], [], []\n",
    "        for i in range(ind_con[modes[0]][conditions_list[modes[0]][0]].shape[0]):\n",
    "            participant_conds = []\n",
    "            labels = []\n",
    "            for condition in conditions_list[mode]:\n",
    "                x = ind_con[mode][condition][i, dict_channel_types[channel_type], :, :, 0]\n",
    "                label = np.array([condition] * x.shape[0])\n",
    "                participant_conds.append(x)\n",
    "                labels.append(label)\n",
    "\n",
    "            # Stack the epochs and labels for this participant\n",
    "            X_part = np.vstack(participant_conds)\n",
    "            y_part = np.hstack(labels)\n",
    "\n",
    "            # Standardize the data\n",
    "            X_part = StandardScaler().fit_transform(X_part)\n",
    "            \n",
    "            # Map the labels to integers consistently\n",
    "            label_mapping = {label: idx for idx, label in enumerate(np.unique(y_part))}\n",
    "            y_part = np.array([label_mapping[label] for label in y_part])\n",
    "            X_list.append(X_part)\n",
    "            y_list.append(y_part)\n",
    "            groups_list.append(np.full(len(y_part), i))\n",
    "        \n",
    "        # Concatenate data across all participants\n",
    "        X = np.concatenate(X_list, axis=0)\n",
    "        y = np.concatenate(y_list, axis=0)\n",
    "        groups = np.concatenate(groups_list, axis=0)\n",
    "        \n",
    "        # Reshape the data to (n_samples, sqrt(n_features), sqrt(n_features))\n",
    "        # This assumes that X.shape[1] is a perfect square.\n",
    "        side_length = int(np.sqrt(X.shape[1]))\n",
    "        X = X.reshape(X.shape[0], side_length, side_length)\n",
    "\n",
    "        # Perform Leave-One-Group-Out CV; for each fold, reinitialize and train a new CNN model.\n",
    "        fold_scores = []\n",
    "        for train_idx, test_idx in logo.split(X, y, groups=groups):\n",
    "            X_train, X_test = X[train_idx], X[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "            # Define a more advanced CNN model for this fold.\n",
    "            model = tf.keras.Sequential([\n",
    "                tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(X_train.shape[1], X_train.shape[2], 1)),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "                tf.keras.layers.Dropout(0.25),\n",
    "                tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "                tf.keras.layers.Dropout(0.25),\n",
    "                tf.keras.layers.Conv2D(256, (3, 3), activation='relu'),\n",
    "                tf.keras.layers.BatchNormalization(),\n",
    "                tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.Dense(128, activation='relu'),\n",
    "                tf.keras.layers.Dropout(0.5),\n",
    "                tf.keras.layers.Dense(len(np.unique(y)), activation='softmax')\n",
    "            ])\n",
    "\n",
    "            model.compile(\n",
    "                optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "\n",
    "            # Train the model using a validation split for early stopping.\n",
    "            history = model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                epochs=100,\n",
    "                batch_size=16,\n",
    "                validation_split=0.2,\n",
    "                callbacks=[\n",
    "                    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "                    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5)\n",
    "                ],\n",
    "                verbose=0\n",
    "            )\n",
    "\n",
    "            # Evaluate on the test fold\n",
    "            score = model.evaluate(X_test, y_test, verbose=0)\n",
    "            fold_scores.append(100 * score[1])  # score[1] is the accuracy\n",
    "\n",
    "        # Compute the mean score for this mode\n",
    "        model_score[mode] = {\n",
    "            'logo_scores': fold_scores,\n",
    "            'mean_score': np.mean(fold_scores)\n",
    "        }\n",
    "\n",
    "    # Save the model scores to disk\n",
    "    with open(f'processed_data/models/con_across_scores/{model.__class__.__name__}_scores.json', 'w') as f:\n",
    "        json.dump(model_score, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding Analysis Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_types = ['raw_within_scores', 'con_within_scores', 'raw_across_scores', 'con_across_scores']\n",
    "\n",
    "if get_decoding_table_scores_plots:\n",
    "    for score_type in score_types:\n",
    "        # load data from 'processed_data/models' folder\n",
    "        model_scores = {}\n",
    "        # for any csv file in the folder\n",
    "        for file in os.listdir(f'processed_data/models/{score_type}'):\n",
    "            # if the file is a json file\n",
    "            if file.endswith('.json'):\n",
    "                # open the file\n",
    "                with open(f'processed_data/models/{score_type}/{file}', 'r') as f:\n",
    "                    # load the data from the file\n",
    "                    model_scores[file.split('_')[0]] = json.load(f)\n",
    "\n",
    "        # split score_type by '_'\n",
    "        score_type_split = score_type.split('_')\n",
    "        if score_type_split[0] == 'con':\n",
    "            score_type_split[0] = 'Connectivity'\n",
    "        elif score_type_split[0] == 'raw':\n",
    "            score_type_split[0] = 'Raw'\n",
    "\n",
    "        score_type_split[1] = score_type_split[1].capitalize()\n",
    "\n",
    "        # Create a DataFrame from the model scores\n",
    "        model_scores_df = pd.DataFrame([\n",
    "            {'Model': model_name, 'Mode': mode, 'Mean Score': score['mean_score']}\n",
    "            for model_name, scores in model_scores.items()\n",
    "            for mode, score in scores.items()\n",
    "        ])\n",
    "\n",
    "        # Pivot the DataFrame to make 'Model' the first column and each 'Mode' a separate column\n",
    "        model_scores_df = model_scores_df.pivot(index='Model', columns='Mode', values='Mean Score').reset_index()\n",
    "\n",
    "        # Fill NaN values with 0 or any other value if needed\n",
    "        model_scores_df.fillna(np.nan, inplace=True)\n",
    "\n",
    "        # Remove the Mode column name\n",
    "        model_scores_df.columns.name = None\n",
    "\n",
    "        # Reorder the columns to have 'Model', 'all', 'face_type', 'emotion'\n",
    "        model_scores_df = model_scores_df[['Model', 'all', 'face_type', 'emotion']]\n",
    "\n",
    "        # Round the scores to 2 decimal places\n",
    "        model_scores_df = model_scores_df.round(2)\n",
    "\n",
    "        # Create a matplotlib figure and add a table\n",
    "        fig, ax = plt.subplots(figsize=(6, len(model_scores_df) / 4))\n",
    "        ax.axis('tight')\n",
    "        ax.axis('off')\n",
    "        table = ax.table(cellText=model_scores_df.values, colLabels=model_scores_df.columns, loc='center')\n",
    "\n",
    "        # Set the biggest number in each column to be bold\n",
    "        for i, col in enumerate(model_scores_df.columns):\n",
    "            if col == 'Model':\n",
    "                continue\n",
    "            max_val = model_scores_df[col].max()\n",
    "            for j, val in enumerate(model_scores_df[col]):\n",
    "                if val == max_val:\n",
    "                    table[(j + 1, i)].set_text_props(weight='bold')\n",
    "\n",
    "        # Add a title to the plot\n",
    "        ax.set_title(f'{score_type_split[0]} {score_type_split[1]} Scores (Accuracy%)')\n",
    "\n",
    "        # Save the figure as a PNG file, bbox_inches='tight'\n",
    "        plt.savefig(f'plots/models/tables/{score_type}.png', dpi=dpi, bbox_inches='tight')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Participant/LOGO Scores Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "if get_decoding_individual_scores_plots:\n",
    "    for score_type in score_types:\n",
    "        # load data from 'processed_data/models' folder\n",
    "        model_scores = {}\n",
    "        # for any csv file in the folder\n",
    "        for file in os.listdir(f'processed_data/models/{score_type}'):\n",
    "            # if the file is a json file\n",
    "            if file.endswith('.json'):\n",
    "                # open the file\n",
    "                with open(f'processed_data/models/{score_type}/{file}', 'r') as f:\n",
    "                    # load the data from the file\n",
    "                    model_scores[file.split('_')[0]] = json.load(f)\n",
    "\n",
    "        # split score_type by '_'\n",
    "        score_type_split = score_type.split('_')\n",
    "        if score_type_split[0] == 'con':\n",
    "            score_type_split[0] = 'Connectivity'\n",
    "        elif score_type_split[0] == 'raw':\n",
    "            score_type_split[0] = 'Raw'\n",
    "\n",
    "        score_type_split[1] = score_type_split[1].capitalize()\n",
    "\n",
    "        models = list(model_scores.keys())\n",
    "        modes = list(model_scores[list(model_scores.keys())[0]].keys())\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        colors = plt.cm.tab10.colors  # Use a colormap for distinct colors\n",
    "\n",
    "        # Assign a consistent color to each model\n",
    "        model_colors = {model: colors[idx % len(colors)] for idx, model in enumerate(models)}\n",
    "\n",
    "        for mode_idx, mode in enumerate(modes):\n",
    "            for model_idx, model in enumerate(models):\n",
    "                scores = list(model_scores[model][mode].keys())[0]\n",
    "                vp = plt.violinplot(\n",
    "                    model_scores[model][mode][scores],\n",
    "                    positions=[model_idx + mode_idx * (len(models) + 1)],\n",
    "                    showmeans=True,\n",
    "                    showextrema=True,\n",
    "                    widths=0.8,\n",
    "                    bw_method=0.5,\n",
    "                )\n",
    "                # Set the color for the violin parts\n",
    "                for part in vp['bodies']:\n",
    "                    part.set_facecolor(model_colors[model])\n",
    "                    part.set_edgecolor('black')\n",
    "                    part.set_alpha(0.7)\n",
    "                vp['cmeans'].set_color('black')  # Median line color\n",
    "                vp['cbars'].set_color('black')    # Whisker line color\n",
    "                vp['cmaxes'].set_color('black')   # Max line color\n",
    "                vp['cmins'].set_color('black')    # Min line color\n",
    "\n",
    "        # Set x-ticks and labels\n",
    "        x_ticks = [(len(models) + 1) * i + len(models) / 2 - 0.5 for i in range(len(modes))]\n",
    "        ax.set_xticks(x_ticks)\n",
    "        ax.set_xticklabels(modes)\n",
    "        ax.set_xlabel('Modes')\n",
    "        ax.set_ylabel('Scores')\n",
    "        ax.set_title(f'{score_type_split[0]} {score_type_split[1]} Scores (Accuracy%)')\n",
    "        ax.set_ylim(0, 100)\n",
    "        ax.set_yticks(np.arange(0, 101, 10))\n",
    "        ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        ax.legend(\n",
    "            handles=[mpatches.Patch(color=model_colors[model], label=model) for model in models],\n",
    "            title='Models',\n",
    "            loc='upper right'\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'plots/models/violin_plots/{score_type}.png', dpi=dpi / 2)\n",
    "        plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
