As avatars permeate social media, gaming, and telecommunications, understanding how the brain reads emotions from virtual faces is increasingly important. 
We recorded functional near-infrared spectroscopy (fNIRS) data from adults viewing real photographs and matched computer-generated faces expressing Anger, Disgust, Fear, Joy, Sadness, Surprise, or Neutrality. 
General-linear-model mapping revealed differences in activation between multiple brain regions. 
Functional-connectivity analysis provided insights into synchronization between brain regions in the time-frequency domain, finding differences in synchrony across certain brain regions. 
Collectively, the results demonstrate differences primarily in the visual cortex across face and emotion types, and across other brain regions, including the left prefrontal and right parietal regions. 
These neural signatures provide quantitative targets for refining the realism and emotional efficacy of digital characters in virtual and augmented environments.