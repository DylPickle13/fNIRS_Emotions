This study investigates the neural mechanisms underlying face and emotion perception in both real and virtual stimuli, a question of growing importance given the increasing prevalence of avatars/virtual characters in our lives. 
Participants were presented with real and virtual faces displaying various emotions while their brain activity was recorded using functional near-infrared spectroscopy (fNIRS).
A General Linear Model (GLM) analysis was used to identify brain regions activated by different face types and emotions, revealing differences in activation between multiple brain regions. 
Functional connectivity analysis provided insights into synchronization between brain regions in the time-frequency domain, finding differences in synchrony across certain brain regions. 
Our findings reveal differences primarily in the visual cortex across face and emotion types, and across other brain regions, including the left prefrontal and right parietal regions. 
This research has implications for the development of virtual environments and the design of more effective virtual characters.