As avatars permeate social media, gaming, and telecommunications, understanding how the brain reads emotions from virtual faces is increasingly important. 
We recorded functional near-infrared spectroscopy (fNIRS) data from adults viewing real photographs and matched computer-generated faces expressing Anger, Disgust, Fear, Joy, Sadness, Surprise, or Neutral (control). 
General-linear-model mapping revealed higher activation in virtual faces in the left occipital region, and higher activation in Neutral and Surprise compared to the other emotions in parietal and occipital regions. 
Functional-connectivity analysis revealed higher connectivity in real faces across the brain, and higher connectivity across the brain in Anger and Fear compared to the other emotions. 
Collectively, the results demonstrate differences in activation in occipital areas, and differential processing of face and emotion types across the whole brain.  
These neural signatures provide quantitative targets for refining the realism and emotional efficacy of digital characters in virtual and augmented environments.