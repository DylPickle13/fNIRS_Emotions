This study investigates the neural mechanisms underlying face and emotion perception in both real and virtual stimuli using functional near-infrared spectroscopy (fNIRS), a question of growing importance given the increasing prevalence of avatars/virtual characters in our lives. 
We employed multiple analyses, including General Linear Model (GLM) analysis, and functional connectivity. 
The GLM analysis was used to identify brain regions activated by different face types and emotions, revealing differences in activation mainly in occipital regions. 
Functional connectivity analysis provided insights into the correlation between brain regions in the time-frequency domain, finding significantly correlated brain regions. 
Our findings reveal the similarities and differences in how the brain distinguishes between real and virtual emotional expressions. 
This research has implications for the development of virtual environments and the design of more effective virtual characters.