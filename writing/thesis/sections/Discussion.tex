With the rise of video games and generative AI technologies, humans are exposed to virtual faces daily. 
Yet, how our brain processes these faces, and their emotions has received little attention. 
Here we explored neural responses to presentations of different face-types (real, virtual), across a range of emotions (Anger, Disgust, Fear, Joy, Sadness, Surprise, Neutral control). 
Hemodynamic activity (HbT) was measured with functional near-infrared spectroscopy (fNIRS) and evaluated in two complementary ways: Activation analysis (GLM): estimated change in HbT amplitude in each channel, quantifying how strongly each Face-type \texorpdfstring{$\times$}{x} Emotion combination modulated local cortical activity; and Spectral functional connectivity: sliding-window cross-spectral density was used to compute coherence between channel pairs, revealing how neural signals became temporally synchronized (or desynchronized) across regions during stimulus processing. 
We hypothesized participants would exhibit differences in activation and connectivity to both different face-types and emotions. 

Distinct mechanisms were observed in the processing of face-types and emotions. 
Activation analyses indicated that differences for both occurred primarily in the occipital region, the brain's primary visual processing center. 
The occipital region's role extends beyond simple feature detection, as it processes complex visual patterns and configurations that differentiate face types and facial expressions at a perceptual level. 
These occipital responses likely reflect the visual system's ability to encode the physical differences in facial features and expressions before this information is forwarded to temporal and parietal cortices for higher-order processing. 
Our connectivity analyses revealed widespread differences in the frontal, parietal and temporal areas, in line with this view. 
The increased parietal-temporal-frontal coherence suggests greater network-level integration of these regions, becoming more functionally coupled when processing specific affective information (e.g., Fear $>$ Neutral) and face realism (real $>$ virtual).
Interestingly, this juxtaposition suggests that while the level of activation in these regions remains stable, the similarity in HbT activity suggests that these regions are prioritized for the processing of real faces over virtual faces, and differential processing for different emotions.

Our finding that virtual faces elicit greater left occipital activation compared to real faces aligns with evidence that face realism modulates visual processing in a complex manner. 
The N170 EEG component, reflecting early face perception, is influenced by perceived realism in a u-shaped fashion, where highly stylized faces and real faces elicit stronger responses than moderately realistic faces, while the later Late Positive Potential (LPP) increases continuously with face realism \citep{schindler_differential_2017}. 
This suggests that the brain processes different types of faces using different strategies, real faces are processed as a whole, while stylized faces require more detailed analysis of individual features.
As well, the LPP enhancement with increasing realism is associated with broader occipito-parietal activity, converging with our fNIRS results that show stronger left occipital activation for virtual faces.
This indicates that the neural response to virtual faces suggests that artificial stimuli place distinct perceptual demands generating distinct activity in the occipital area. 
Although face perception (i.e., stronger responses to faces than to non-face objects) reliably and primarily activates the right lateral fusiform gyrus \citep{haxby_distributed_2000}, we did not find differences in that region. 
Our results likely reflect compensatory processing in functionally connected occipital regions that support the fusiform face area in response to perceptually challenging stimuli. 
This compensatory mechanism may manifest as the increased left occipital activation we observed for virtual faces, as these upstream visual areas work harder to provide adequate facial feature extraction to downstream face-processing networks.

Conversely, our functional connectivity analysis revealed widespread but distinct neural connectivity patterns associated with processing real versus virtual faces, suggesting that the realism of facial stimuli modulates the underlying brain network dynamics during face processing, supporting our first hypothesis.
We observed stronger connectivity when processing real faces across parietal, frontal, and central/temporal regions. 
Cross-brain coherence in left frontal, temporal, and parietal regions has been observed during social interaction and live eye-to-eye contact \citep{hirsch_frontal_2017}, 
Supporting this, EEG studies show that processing faces trigger stronger theta-band phase synchronization between brain regions within 120 ms compared to processing non-face stimuli between parietal and occipito-temporal areas, suggesting that different stages of face processing occur in distinct parts of the brain \citep{yang_dynamic_2015}. 
Together, these studies align with our connectivity analysis, further supporting the notion that real face recognition engages a broader and more cohesive cortical network configuration, supported by connectivity between higher-order socio-cognitive regions. 

The emotional expressions independent of whether they were on real or virtual faces revealed differences in the activation in occipital, right parietal, and left central/temporal regions, supporting our second hypothesis.
Neutral and surprise expressions elicited a stronger neural response relative to all other emotions.
This might be explained by the ambiguous nature of the two emotional states which requires greater effort to interpret them. 
There is consistent evidence of increased brain activation in response to neutral and surprise facial expressions, sometimes even exceeding responses to more subtle emotional expressions.
Both avatar and human emotional faces activate regions typically involved in emotion processing, such as the bilateral amygdala, fusiform gyri, cerebellum, and superior temporal gyrus, with neutral face conditions also producing strong amygdala activation, suggesting that the amygdala may not only respond to emotional intensity, but also to the social relevance of faces in general \citep{moser_amygdala_2007}. 
This aligns with previous findings showing that some neurons in the temporal lobe and amygdala are tuned to detect faces rather than specific emotions.
fMRI studies comparing responses to neutral and scrambled faces have revealed robust activation in regions like the fusiform gyri, amygdalae, entorhinal cortices, and superior temporal sulcus during neutral face viewing \citep{keslerwest_neural_2001}. 
These findings confirm that emotionally neutral faces still engage core regions involved in face perception and social processing, likely due to their ambiguous or uncertain emotional content.
Prior fNIRS and fMRI literature has found inconsistent results for emotional face processing in the PFC, with some studies reporting increases in oxygenated hemoglobin in regions like the medial and ventral PFC, while others showed decreases or no change, depending on the emotion or brain region \citep{westgarth_systematic_2021}. 
These findings and our own challenge the assumption that processing more salient emotions result in greater activation, perhaps the way we process faces has more to do with the connectivity patterns and the networks that are engaged, rather than the magnitude of activation in specific regions.

The functional connectivity analysis showed that there are distinct connectivity patterns associated with processing different emotions, with Fear and Anger eliciting the strongest connectivity, particularly in the left central/temporal cortex across frontal and parietal regions, supporting our second hypothesis.
Distinct clusters of emotions emerged from the connectivity analysis, with Fear, Anger, and Joy forming one cluster with stronger connectivity than the other cluster of emotions, which included Disgust, Sadness, Surprise, and Neutral faces.
The activation and connectivity analyses show divergent findings, with activation differences primarily in visual regions, while connectivity differences were more widespread across the brain, suggesting that while the initial perception of faces is localized, the emotional processing involves higher-order association areas. 
We found that across all emotions, visual regions (occipital) showed less connectivity differences, while central/temporal and parietal regions exhibited strong variations, suggesting that the emotional nuances of faces are primarily encoded in these higher-order association areas. 
This lines up with the constructionist view of emotion, that emotional expressions are decoded in distributed brain networks rather than isolated regions \citep{barrett_solving_2006}. 

Fearful faces signal potential threat, and the interpretation of these cues depends on contextual factors, whereas Anger signals the source of threat. 
Fear processing engages the amygdala, particularly when eye gaze is not directed at the viewer compared to when it is \citep{cushing_neurodynamics_2018}.
Connectivity from the amygdala to the dorsolateral prefrontal cortex (dlPFC) differs when processing fearful versus sad facial expressions, with increased connectivity when processing fearful faces compared to sad faces, attributing this to the higher salience and arousal associated with fear \citep{jamieson_differential_2021, adolphs_biology_2013}.
This pattern lines up with our emotion connectivity summary heatmap (Figure \ref{fig:fc_emotion_summary_analysis}), where fearful faces show robust network engagement relative to others. 
Additionally, facial emotion expressions can be successfully decoded from functional connectivity patterns, and the networks identified include brain regions beyond the conventional face-selective areas \citep{liang_multivariate_2018}. 
This points to Fear as a highly salient emotion that engages a broader network of brain regions, compared to other emotions. 

Our count of significantly different channels pairs across all emotions found minimal connectivity differences among left and right occipital ROI, while central/temporal and parietal regions varied strongly, as indicated by the asterisks and carets (Figure \ref{fig:fc_region_summary_analysis}).
We found only 20-40 significantly different channels in the occipital regions, compared to 100-120 in the central/temporal and parietal regions, indicating a structural difference in how these regions process emotional faces.
This fits with models of face perception, where early visual areas (e.g., occipital face area) feed into higher-level hubs like the fusiform face area. 
Effective connectivity analyses reveal dynamic modulation between occipital, frontal, and subcortical regions, but early visual areas remain relatively stable across emotion conditions, with emotional discrimination only arising in higher circuits \citep{underwood_networks_2021}. 
This suggests that while occipital areas are crucial for initial face processing, the emotional nuances of faces are primarily encoded in more distributed networks involving parietal and central/temporal regions, regardless of the emotion presented.

The functional connectivity analysis revealed distinct emotional clusters that reflect underlying similarities in neural processing patterns and arousal characteristics, as seen in Figure \ref{fig:fc_emotion_summary_analysis}.
Anger and Joy also produced notably strong connectivity, clustering with Fear in our summary heatmap, suggesting these emotions share common neural network engagement patterns despite their different valence profiles.
This clustering pattern aligns with dimensional models of emotion that emphasize arousal as a key organizing principle, where high-arousal emotions like Fear, Anger, and Joy engage more extensive and coherent neural networks than their low-arousal counterparts \citep{ke_dynamic_2025}.
Lower connectivity for Disgust, Sadness, Surprise, and Neutral faces supports the idea that these emotions have lower arousal or social salience compared to Fear, Anger, and Joy.
The emergence of these two distinct clusters, one characterized by high connectivity (Fear, Anger, Joy) and another by relatively lower connectivity (Disgust, Sadness, Surprise, Neutral), suggests that the brain's functional architecture organizes emotional face processing along arousal dimensions. 
This finding is particularly important for understanding how virtual characters and avatars might be designed to maximize/minimize neural engagement, as high-arousal emotional expressions appear to recruit broader brain networks regardless of face realism.

Our findings align with a growing consensus: real faces are remembered more accurately than artificial ones, even though both are processed as faces. 
Artificial faces are remembered less efficiently and discriminated slightly worse than real faces, supporting the hypothesis that "out-group" faces, those that are less familiar or realistic, are processed differently \citep{balas_artificial_2015}.
Complementing this, virtual faces trigger higher false alarm rates, a sign of reduced memory specificity, despite matched visual features and equivalent overall recognition sensitivity \citep{katsyri_those_2018}. 
Participants also rated these faces as eerier, highlighting a connection between reduced perceptual expertise and the uncanny valley experience. 
This suggests that artificial faces engage face-specific processing yet are represented more weakly in memory. 
Real faces, compared to computer-generated faces, have been associated with poorer performance in an implicit catch trial task, which was interpreted as reflecting an involuntary attentional response toward human faces, which are highly familiar and socially salient visual stimuli \citep{katsyri_amygdala_2020}. 
The automatic allocation of attention to real faces may have interfered with participants' ability to withhold a motor response until the catch stimulus appeared, as required by the task. 
This is consistent with our finding that participants performed better on our memory task for real faces than virtual ones.
Enhancing familiarity through exposure \citep{park_individuals_2021} or increasing realism could help bridge this memory gap. 
Neuroscientific and evolutionary theories also propose that the human brain includes specialized modules (e.g., the fusiform face area), are highly attuned to natural facial features \citep{burke_evolution_2013}. 
These modules are refined by experience and optimized for identity recognition, which may not be fully triggered by virtual faces, due to limited exposure or differences in visual features.
This may signal evolutionary utility in processing real, familiar faces, and why virtual faces, even when perceived as faces, do not engage the same neural mechanisms as real faces.

\section{Limitations and Future Directions}
This study was the first to use fNIRS to examine neural responses to real versus virtual emotional faces, providing novel insights into how face realism and emotion interact in the brain.
However, several limitations should be acknowledged.
First, since we recruited only from Ontario Tech University's undergraduate student body, our sample of students were limited to that relatively young age range and community, and likely are more educated and technologically savvy than the general population.
This may limit the generalizability of our findings to broader populations, particularly older adults or those with less exposure to virtual characters/avatars.
As well, many participants did not pass our signal quality checks, due to reasons such as hair color/texture affecting the signal quality \citep{holmes_opening_2024}. 
Additionally, our sample was predominantly female, which may have influenced our results as females and males may differ in their accuracy and sensitivity when categorizing facial emotions; for example, females have been shown to outperform males in identifying fearful faces \citep{weisenbach_reduced_2014}.
Men have also been found to have a differential neural response depending upon the emotion presented \citep{keslerwest_neural_2001}. 
Future studies should include a wider range of participants, ideally representing the general population. 

Second, since the virtual faces used in this study were all from one dataset (UIBVFED), they were all of similar realism and stylization.
Since neural responses to virtual faces vary with realism, our findings may not generalize to other virtual face datasets with different levels of realism or stylization \citep{schindler_differential_2017}. 
Future work should explore a wider range of virtual face styles and realism levels to better understand how these factors influence neural processing.

\section{Conclusion}
The present study investigated how the human brain processes emotional expressions on real versus virtual faces, using functional near-infrared spectroscopy (fNIRS) to assess both activation magnitude and functional connectivity. 
By directly comparing hemodynamic responses to real and virtual faces across a range of basic emotions, we aimed to determine whether face realism modulates neural responses, and whether these effects extend beyond local activation to distributed network interactions. 
We found that virtual faces elicited greater activation in the left occipital region, suggesting increased perceptual demands, while real faces were associated with stronger distributed connectivity and better recall performance in a memory task. 
Neutral and surprise expressions evoked the strongest activation, whereas fear and anger engaged broader neural networks. 
These findings highlight the complex interplay between face realism and emotion processing, suggesting that while virtual faces are perceived as faces, they do not recruit the same neural mechanisms as real ones. 
Emotional expressions appear to be decoded in distributed brain networks rather than isolated regions. 
These results offer novel insights into the neural mechanisms of emotional face perception in the context of increasingly prevalent virtual social interactions. 
Future research should examine how varying levels of virtual face realism and stylization influence neural responses and social cognition.