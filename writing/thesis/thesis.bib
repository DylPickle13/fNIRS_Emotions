@misc{bulgarelli_growth_2025,
	title = {Growth in early infancy drives optimal brain functional connectivity which predicts cognitive flexibility in later childhood},
	copyright = {© 2025, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/2024.01.02.573930v2},
	doi = {10.1101/2024.01.02.573930},
	abstract = {Functional brain network organisation, measured by functional connectivity (FC), reflects key neurodevelopmental processes for healthy development. Early exposure to adversity, e.g. undernutrition, affects neurodevelopment, observable via disrupted FC, and leads to poorer outcomes from preschool age onward. We assessed longitudinally the impact of early growth trajectories on developmental FC in a rural Gambian population from age 5 to 24 months. To investigate how these early trajectories relate to later childhood outcomes, we assessed cognitive flexibility at 3-5 years. We observed that early physical growth before the fifth month of life drove optimal developmental trajectories of FC that in turn predicted cognitive flexibility at pre-school age. In contrast to previously studied developmental populations, this Gambian sample exhibited long-range interhemispheric FC that decreased with age. Our results highlight the measurable effects that poor growth in early infancy has on brain development and the subsequent impact on pre-school age cognitive development, underscoring the need for early life interventions throughout global settings of adversity},
	language = {en},
	urldate = {2025-04-02},
	publisher = {bioRxiv},
	author = {Bulgarelli, Chiara and Blasi, Anna and McCann, Samantha and Milosavljevic, Bosiljka and Ghillia, Giulia and Mbye, Ebrima and Touray, Ebou and Fadera, Tijan and Acolatse, Lena and Moore, Sophie E. and Lloyd-Fox, Sarah and Elwell, Clare E. and Eggebrecht, Adam T. and Team, the BRIGHT Study},
	month = mar,
	year = {2025},
	note = {Pages: 2024.01.02.573930
Section: New Results},
	file = {Full Text PDF:C\:\\Users\\super\\Zotero\\storage\\AFLEHZS8\\Bulgarelli et al. - 2025 - Growth in early infancy drives optimal brain funct.pdf:application/pdf},
}

@article{pollonini_phoebe_2016,
	title = {{PHOEBE}: a method for real time mapping of optodes-scalp coupling in functional near-infrared spectroscopy},
	volume = {7},
	copyright = {© 2016 Optical Society of America},
	issn = {2156-7085},
	shorttitle = {{PHOEBE}},
	url = {https://opg.optica.org/boe/abstract.cfm?uri=boe-7-12-5104},
	doi = {10.1364/BOE.7.005104},
	abstract = {Recent functional near-infrared spectroscopy (fNIRS) instrumentation encompasses several dozen of optodes to enable reconstructing a hemodynamic image of the entire cerebral cortex. Despite its potential clinical applicability, widespread use of fNIRS with human subjects is currently limited by unresolved issues, namely the collection from the entirety of optical channels of signals with a signal-to-noise ratio (SNR) sufficient to carry out a reliable estimation of cortical hemodynamics, and the considerable amount of time that placing numerous optodes take with individuals for whom achieving good optical coupling to the scalp is difficult due to thick or dark hair. To address these issues, we developed a numerical method that: 1) at the channel level, computes an objective measure of the signal-to-noise ratio (SNR) related to its optical coupling to the scalp, akin to electrode conductivity used in electroencephalography (EEG), and 2) at the optode level, determines and displays the coupling status of all individual optodes in real time on a model of a human head. This approach aims to shorten the pre-acquisition preparation time by visually displaying which optodes require further adjustment for optimum scalp coupling, and to maximize the signal-to-noise ratio (SNR) of all optical channels contributing to the functional hemodynamic mapping. The methodology described in this paper has been implemented in a software tool named PHOEBE (placing headgear optodes efficiently before experimentation) that is freely available for use by the fNIRS community.},
	language = {EN},
	number = {12},
	urldate = {2024-10-18},
	journal = {Biomedical Optics Express},
	author = {Pollonini, Luca and Bortfeld, Heather and Oghalai, John S.},
	month = dec,
	year = {2016},
	note = {Publisher: Optica Publishing Group},
	keywords = {Imaging techniques, Magnetic resonance imaging, Near infrared spectroscopy, Optical imaging, Optical signal to noise ratio, Optical signals},
	pages = {5104--5119},
	file = {Full Text PDF:C\:\\Users\\super\\Zotero\\storage\\7US2JFA7\\Pollonini et al. - 2016 - PHOEBE a method for real time mapping of optodes-.pdf:application/pdf},
}

@article{holmes_opening_2024,
	title = {Opening the dialogue: {A} preliminary exploration of hair color, hair cleanliness, light, and motion effects on {fNIRS} signal quality},
	volume = {19},
	issn = {1932-6203},
	shorttitle = {Opening the dialogue},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0304356},
	doi = {10.1371/journal.pone.0304356},
	abstract = {Introduction Functional near-infrared spectroscopy (fNIRS) is a promising tool for studying brain activity, offering advantages such as portability and affordability. However, challenges in data collection persist due to factors like participant physiology, environmental light, and gross-motor movements, with limited literature on their impact on fNIRS signal quality. This study addresses four potentially influential factors–hair color, hair cleanliness, environmental light, and gross-motor movements–on fNIRS signal quality. Our aim is to raise awareness and offer insights for future fNIRS research. Methods Six participants (4 Females, 2 Males) took part in four different experiments investigating the effects of hair color, hair cleanliness, environmental light, and gross-motor movements on fNIRS signal quality. Participants in Experiment 1, categorized by hair color, completed a finger-tapping task in a between-subjects block design. Signal quality was compared between each hair color. Participants in Experiments 2 and 3 completed a finger-tapping task in a within-subjects block design, with signal quality being compared across hair cleanliness (i.e., five consecutive days without washing the hair) and environmental light (i.e., sunlight, artificial light, no light, etc.), respectively. Experiment 4 assessed three gross-motor movements (i.e., walking, turning and nodding the head) in a within-subjects block design. Motor movements were then compared to resting blocks. Signal quality was evaluated using Scalp Coupling Index (SCI) measurements. Results Lighter hair produced better signals than dark hair, while the impact of environmental light remains uncertain. Hair cleanliness showed no significant effects, but gross motor movements notably reduced signal quality. Conclusion Our results suggest that hair color, environmental light, and gross-motor movements affect fNIRS signal quality while hair cleanliness does not. Nevertheless, future studies with larger sample sizes are warranted to fully understand these effects. To advance future research, comprehensive documentation of participant demographics and lab conditions, along with signal quality analyses, is essential.},
	language = {en},
	number = {5},
	urldate = {2025-04-04},
	journal = {PLOS ONE},
	author = {Holmes, Mitchell and Aalto, Daniel and Cummine, Jacqueline},
	month = may,
	year = {2024},
	note = {Publisher: Public Library of Science},
	keywords = {Artificial light, Computer software, Hair, Neuroimaging, Scalp, Signal processing, Speech signal processing, Sunlight},
	pages = {e0304356},
	file = {Full Text PDF:C\:\\Users\\super\\Zotero\\storage\\CBRE9HDP\\Holmes et al. - 2024 - Opening the dialogue A preliminary exploration of.pdf:application/pdf},
}

@inproceedings{hernandez_nirsplot_2020,
	address = {Washington, DC},
	title = {{NIRSplot}: {A} {Tool} for {Quality} {Assessment} of {fNIRS} {Scans}},
	isbn = {978-1-943580-74-3},
	shorttitle = {{NIRSplot}},
	url = {https://opg.optica.org/abstract.cfm?URI=BRAIN-2020-BM2C.5},
	doi = {10.1364/BRAIN.2020.BM2C.5},
	abstract = {For a rapid and intuitive visual assessment of the quality of functional near infrared spectroscopy scans, we developed a software tool that automatically identifies entire optical channels or time intervals with compromised optical signals.},
	language = {en},
	urldate = {2025-04-05},
	booktitle = {Biophotonics {Congress}: {Biomedical} {Optics} 2020 ({Translational}, {Microscopy}, {OCT}, {OTS}, {BRAIN})},
	publisher = {Optica Publishing Group},
	author = {Hernandez, Samuel Montero and Pollonini, Luca},
	year = {2020},
	pages = {BM2C.5},
	file = {Hernandez and Pollonini - 2020 - NIRSplot A Tool for Quality Assessment of fNIRS S.pdf:C\:\\Users\\super\\Zotero\\storage\\MHBU7NPY\\Hernandez and Pollonini - 2020 - NIRSplot A Tool for Quality Assessment of fNIRS S.pdf:application/pdf},
}

@article{lindquist_brain_2012,
	title = {The brain basis of emotion: {A} meta-analytic review},
	volume = {35},
	issn = {1469-1825, 0140-525X},
	shorttitle = {The brain basis of emotion},
	url = {https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/brain-basis-of-emotion-a-metaanalytic-review/80F95F093305C76BA2C66BBA48D4BC8A},
	doi = {10.1017/S0140525X11000446},
	abstract = {Researchers have wondered how the brain creates emotions since the early days of psychological science. With a surge of studies in affective neuroscience in recent decades, scientists are poised to answer this question. In this target article, we present a meta-analytic summary of the neuroimaging literature on human emotion. We compare the locationist approach (i.e., the hypothesis that discrete emotion categories consistently and specifically correspond to distinct brain regions) with the psychological constructionist approach (i.e., the hypothesis that discrete emotion categories are constructed of more general brain networks not specific to those categories) to better understand the brain basis of emotion. We review both locationist and psychological constructionist hypotheses of brain–emotion correspondence and report meta-analytic findings bearing on these hypotheses. Overall, we found little evidence that discrete emotion categories can be consistently and specifically localized to distinct brain regions. Instead, we found evidence that is consistent with a psychological constructionist approach to the mind: A set of interacting brain regions commonly involved in basic psychological operations of both an emotional and non-emotional nature are active during emotion experience and perception across a range of discrete emotion categories.},
	language = {en},
	number = {3},
	urldate = {2025-04-05},
	journal = {Behavioral and Brain Sciences},
	author = {Lindquist, Kristen A. and Wager, Tor D. and Kober, Hedy and Bliss-Moreau, Eliza and Barrett, Lisa Feldman},
	month = jun,
	year = {2012},
	keywords = {Discrete emotion, emotion experience, emotion perception, meta-analysis, neuroimaging, psychological construction},
	pages = {121--143},
	file = {Full Text PDF:C\:\\Users\\super\\Zotero\\storage\\A5E64WCH\\Lindquist et al. - 2012 - The brain basis of emotion A meta-analytic review.pdf:application/pdf},
}

@article{peirce_psychopy2_2019,
	title = {{PsychoPy2}: {Experiments} in behavior made easy},
	volume = {51},
	issn = {1554-3528},
	shorttitle = {{PsychoPy2}},
	url = {https://doi.org/10.3758/s13428-018-01193-y},
	doi = {10.3758/s13428-018-01193-y},
	abstract = {PsychoPy is an application for the creation of experiments in behavioral science (psychology, neuroscience, linguistics, etc.) with precise spatial control and timing of stimuli. It now provides a choice of interface; users can write scripts in Python if they choose, while those who prefer to construct experiments graphically can use the new Builder interface. Here we describe the features that have been added over the last 10 years of its development. The most notable addition has been that Builder interface, allowing users to create studies with minimal or no programming, while also allowing the insertion of Python code for maximal flexibility. We also present some of the other new features, including further stimulus options, asynchronous time-stamped hardware polling, and better support for open science and reproducibility. Tens of thousands of users now launch PsychoPy every month, and more than 90 people have contributed to the code. We discuss the current state of the project, as well as plans for the future.},
	language = {en},
	number = {1},
	urldate = {2025-04-05},
	journal = {Behavior Research Methods},
	author = {Peirce, Jonathan and Gray, Jeremy R. and Simpson, Sol and MacAskill, Michael and Höchenberger, Richard and Sogo, Hiroyuki and Kastman, Erik and Lindeløv, Jonas Kristoffer},
	month = feb,
	year = {2019},
	keywords = {Experiment, Open science, Open-source, Psychology, Reaction time, Software, Timing},
	pages = {195--203},
	file = {Full Text PDF:C\:\\Users\\super\\Zotero\\storage\\EEA6BV79\\Peirce et al. - 2019 - PsychoPy2 Experiments in behavior made easy.pdf:application/pdf},
}

@article{oliver_uibvfed_2020,
	title = {{UIBVFED}: {Virtual} facial expression dataset},
	volume = {15},
	issn = {1932-6203},
	shorttitle = {{UIBVFED}},
	url = {https://dx.plos.org/10.1371/journal.pone.0231266},
	doi = {10.1371/journal.pone.0231266},
	abstract = {Facial expression classification requires large amounts of data to reflect the diversity of conditions in the real world. Public databases support research tasks providing researchers an appropriate work framework. However, often these databases do not focus on artistic creation. We developed an innovative facial expression dataset that can help both artists and researchers in the field of affective computing. This dataset can be managed interactively by an intuitive and easy to use software application. The dataset is composed of 640 facial images from 20 virtual characters each creating 32 facial expressions. The avatars represent 10 men and 10 women, aged between 20 and 80, from different ethnicities. Expressions are classified by the six universal expressions according to Gary Faigin classification.},
	language = {en},
	number = {4},
	urldate = {2023-10-17},
	journal = {PLOS ONE},
	author = {Oliver, Miquel Mascaró and Amengual Alcover, Esperança},
	editor = {Li, Zezhi},
	month = apr,
	year = {2020},
	pages = {e0231266},
	file = {2020 - UIBVFED- Virtual facial expression dataset.pdf:G\:\\My Drive\\School Work\\Ontario Tech\\Research\\Lit Review\\2020 - UIBVFED- Virtual facial expression dataset.pdf:application/pdf},
}

@article{conley_racially_2018,
	title = {The racially diverse affective expression ({RADIATE}) face stimulus set},
	volume = {270},
	issn = {0165-1781},
	url = {https://www.sciencedirect.com/science/article/pii/S0165178117321893},
	doi = {10.1016/j.psychres.2018.04.066},
	abstract = {Faces are often used in psychological and neuroimaging research to assess perceptual and emotional processes. Most available stimulus sets, however, represent minimal diversity in both race and ethnicity, which may confound understanding of these processes in diverse/racially heterogeneous samples. Having a diverse stimulus set of faces and emotional expressions could mitigate these biases and may also be useful in research that specifically examines the effects of race and ethnicity on perceptual, emotional and social processes. The racially diverse affective expression (RADIATE) face stimulus set is designed to provide an open-access set of 1,721 facial expressions of Black, White, Hispanic and Asian adult models. Moreover, the diversity of this stimulus set reflects census data showing a change in demographics in the United States from a white majority to a nonwhite majority by 2020. Psychometric results are provided describing the initial validity and reliability of the stimuli based on judgments of the emotional expressions.},
	urldate = {2023-10-18},
	journal = {Psychiatry Research},
	author = {Conley, May I. and Dellarco, Danielle V. and Rubien-Thomas, Estee and Cohen, Alexandra O. and Cervera, Alessandra and Tottenham, Nim and Casey, BJ},
	month = dec,
	year = {2018},
	keywords = {Emotion, Facial expressions, Multiracial, Reliability, Stimuli, Validity},
	pages = {1059--1067},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\super\\Zotero\\storage\\FZK4VSTR\\Conley et al. - 2018 - The racially diverse affective expression (RADIATE.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\super\\Zotero\\storage\\SRBFZ2FL\\S0165178117321893.html:text/html},
}

@article{fishburn_temporal_2019,
	title = {Temporal {Derivative} {Distribution} {Repair} ({TDDR}): {A} motion correction method for {fNIRS}},
	volume = {184},
	issn = {1053-8119},
	shorttitle = {Temporal {Derivative} {Distribution} {Repair} ({TDDR})},
	url = {https://www.sciencedirect.com/science/article/pii/S1053811918308103},
	doi = {10.1016/j.neuroimage.2018.09.025},
	abstract = {Functional near-infrared spectroscopy (fNIRS) is an optical neuroimaging technique of growing interest as a tool for investigation of cortical activity. Due to the on-head placement of optodes, artifacts arising from head motion are relatively less severe than for functional magnetic resonance imaging (fMRI). However, it is still necessary to remove motion artifacts. We present a novel motion correction procedure based on robust regression, which effectively removes baseline shift and spike artifacts without the need for any user-supplied parameters. Our simulations show that this method yields better activation detection performance than 5 other current motion correction methods. In our empirical validation on a working memory task in a sample of children 7–15 years, our method produced stronger and more extensive activation than any of the other methods tested. The new motion correction method enhances the viability of fNIRS as a functional neuroimaging modality for use in populations not amenable to fMRI.},
	urldate = {2024-11-02},
	journal = {NeuroImage},
	author = {Fishburn, Frank A. and Ludlum, Ruth S. and Vaidya, Chandan J. and Medvedev, Andrei V.},
	month = jan,
	year = {2019},
	keywords = {Artifact, Children, Denoising, Functional near-infrared spectroscopy, Head motion, NIRS},
	pages = {171--179},
	file = {Accepted Version:C\:\\Users\\super\\Zotero\\storage\\X9RUVKVK\\Fishburn et al. - 2019 - Temporal Derivative Distribution Repair (TDDR) A .pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\super\\Zotero\\storage\\EECC3VHZ\\S1053811918308103.html:text/html},
}

@article{scholkmann_measuring_2014,
	title = {Measuring tissue hemodynamics and oxygenation by continuous-wave functional near-infrared spectroscopy—how robust are the different calculation methods against movement artifacts?},
	volume = {35},
	issn = {0967-3334},
	url = {https://dx.doi.org/10.1088/0967-3334/35/4/717},
	doi = {10.1088/0967-3334/35/4/717},
	abstract = {Continuous-wave near-infrared spectroscopy and imaging enable tissue hemodynamics and oxygenation to be determined non-invasively. Movements of the investigated subject can cause movement artifacts (MAs) in the recorded signals. The strength and type of MAs induced depend on the measurement principle. The aim of the present study was to investigate the quantitative relationship between different single-distance (SD) and multi-distance (MD) measurement methods and their susceptibility to MAs. We found that each method induces MAs to a different degree, and that MD methods are more robust against MAs than SD methods.},
	language = {en},
	number = {4},
	urldate = {2025-04-06},
	journal = {Physiological Measurement},
	author = {Scholkmann, Felix and Metz, Andreas Jaakko and Wolf, Martin},
	month = mar,
	year = {2014},
	note = {Publisher: IOP Publishing},
	pages = {717},
	file = {IOP Full Text PDF:C\:\\Users\\super\\Zotero\\storage\\VAEIYI6Q\\Scholkmann et al. - 2014 - Measuring tissue hemodynamics and oxygenation by c.pdf:application/pdf},
}

@article{cui_functional_2010,
	title = {Functional near infrared spectroscopy ({NIRS}) signal improvement based on negative correlation between oxygenated and deoxygenated hemoglobin dynamics},
	volume = {49},
	issn = {1053-8119},
	url = {https://www.sciencedirect.com/science/article/pii/S105381190901235X},
	doi = {10.1016/j.neuroimage.2009.11.050},
	abstract = {Near infrared spectroscopy (NIRS) is a promising technology for functional brain imaging which measures hemodynamic signals from the cortex, similar to functional magnetic resonance imaging (fMRI), but does not require the participant to lie motionless in a confined space. NIRS can therefore be used for more naturalistic experiments, including face to face communication, or natural body movements, and is well suited for real-time applications that may require lengthy training. However, improving signal quality and reducing noise, especially noise induced by head motion, is challenging, particularly for real time applications. Here we study the properties of head motion induced noise, and find that motion noise causes the measured oxygenated and deoxygenated hemoglobin signals, which are typically strongly negatively correlated, to become more positively correlated. Next, we develop a method to reduce noise based on the principle that the concentration changes of oxygenated and deoxygenated hemoglobin should be negatively correlated. We show that despite its simplicity, this method is effective in reducing noise and improving signal quality, for both online and offline noise reduction.},
	number = {4},
	urldate = {2025-04-06},
	journal = {NeuroImage},
	author = {Cui, Xu and Bray, Signe and Reiss, Allan L.},
	month = feb,
	year = {2010},
	pages = {3039--3046},
	file = {Accepted Version:C\:\\Users\\super\\Zotero\\storage\\IB4HWQPQ\\Cui et al. - 2010 - Functional near infrared spectroscopy (NIRS) signa.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\super\\Zotero\\storage\\DCBK5WPE\\S105381190901235X.html:text/html},
}

@article{pinti_current_2019,
	title = {Current {Status} and {Issues} {Regarding} {Pre}-processing of {fNIRS} {Neuroimaging} {Data}: {An} {Investigation} of {Diverse} {Signal} {Filtering} {Methods} {Within} a {General} {Linear} {Model} {Framework}},
	volume = {12},
	issn = {1662-5161},
	shorttitle = {Current {Status} and {Issues} {Regarding} {Pre}-processing of {fNIRS} {Neuroimaging} {Data}},
	url = {https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2018.00505/full},
	doi = {10.3389/fnhum.2018.00505},
	abstract = {{\textless}p{\textgreater}Functional near-infrared spectroscopy (fNIRS) research articles show a large heterogeneity in the analysis approaches and pre-processing procedures. Additionally, there is often a lack of a complete description of the methods applied, necessary for study replication or for results comparison. The aims of this paper were (i) to review and investigate which information is generally included in published fNIRS papers, and (ii) to define a signal pre-processing procedure to set a common ground for standardization guidelines. To this goal, we have reviewed 110 fNIRS articles published in 2016 in the field of cognitive neuroscience, and performed a simulation analysis with synthetic fNIRS data to optimize the signal filtering step before applying the GLM method for statistical inference. Our results highlight the fact that many papers lack important information, and there is a large variability in the filtering methods used. Our simulations demonstrated that the optimal approach to remove noise and recover the hemodynamic response from fNIRS data in a GLM framework is to use a 1000th order band-pass Finite Impulse Response filter. Based on these results, we give preliminary recommendations as to the first step toward improving the analysis of fNIRS data and dissemination of the results.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2025-02-16},
	journal = {Frontiers in Human Neuroscience},
	author = {Pinti, Paola and Scholkmann, Felix and Hamilton, Antonia and Burgess, Paul and Tachtsidis, Ilias},
	month = jan,
	year = {2019},
	keywords = {Digital filter, functional data analysis, Functional Near Infrared Spectroscopy (fNIRS), General Linear Model (GLM), Pre-processing guidelines, Pre-processing standardization},
	file = {Full Text PDF:C\:\\Users\\super\\Zotero\\storage\\RTN2VG7M\\Pinti et al. - 2019 - Current Status and Issues Regarding Pre-processing.pdf:application/pdf},
}

@article{gramfort_meg_2013,
	title = {{MEG} and {EEG} data analysis with {MNE}-{Python}},
	volume = {7},
	issn = {1662-453X},
	url = {https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2013.00267/full},
	doi = {10.3389/fnins.2013.00267},
	abstract = {Magnetoencephalography and electroencephalography (M/EEG) measure the weak electromagnetic signals generated by neuronal activity in the brain. Using these signals to characterize and locate neural activation in the brain is a challenge that requires expertise in physics, signal processing, statistics, and numerical methods. As part of the MNE software suite, MNE-Python is an open-source software package that addresses this challenge by providing state-of-the-art algorithms implemented in Python that cover multiple methods of data preprocessing, source localization, statistical analysis, and estimation of functional connectivity between distributed brain regions. All algorithms and utility functions are implemented in a consistent manner with well-documented interfaces, enabling users to create M/EEG data analysis pipelines by writing Python scripts. Moreover, MNE-Python is tightly integrated with the core Python libraries for scientific comptutation (NumPy, SciPy) and visualization (matplotlib and Mayavi), as well as the greater neuroimaging ecosystem in Python via the Nibabel package. The code is provided under the new BSD license allowing code reuse, even in commercial products. Although MNE-Python has only been under heavy development for a couple of years, it has rapidly evolved with expanded analysis capabilities and pedagogical tutorials because multiple labs have collaborated during code development to help share best practices. MNE-Python also gives easy access to preprocessed datasets, helping users to get started quickly and facilitating reproducibility of methods by other researchers. Full documentation, including dozens of examples, is available at http://martinos.org/mne.},
	language = {English},
	urldate = {2025-04-06},
	journal = {Frontiers in Neuroscience},
	author = {Gramfort, Alexandre and Luessi, Martin and Larson, Eric and Engemann, Denis A. and Strohmeier, Daniel and Brodbeck, Christian and Goj, Roman and Jas, Mainak and Brooks, Teon and Parkkonen, Lauri and Hämäläinen, Matti},
	month = dec,
	year = {2013},
	note = {Publisher: Frontiers},
	keywords = {Electroencephalography (EEG), Magnetoencephalography (MEG), Neuroimaging, open-source, python, Software},
	file = {Full Text PDF:C\:\\Users\\super\\Zotero\\storage\\ZU27R59B\\Gramfort et al. - 2013 - MEG and EEG data analysis with MNE-Python.pdf:application/pdf},
}

@article{luke_analysis_2021,
	title = {Analysis methods for measuring passive auditory {fNIRS} responses generated by a block-design paradigm},
	volume = {8},
	issn = {2329-423X, 2329-4248},
	url = {https://www.spiedigitallibrary.org/journals/neurophotonics/volume-8/issue-2/025008/Analysis-methods-for-measuring-passive-auditory-fNIRS-responses-generated-by/10.1117/1.NPh.8.2.025008.full},
	doi = {10.1117/1.NPh.8.2.025008},
	abstract = {Significance: Functional near-infrared spectroscopy (fNIRS) is an increasingly popular tool in auditory research, but the range of analysis procedures employed across studies may complicate the interpretation of data. Aim: We aim to assess the impact of different analysis procedures on the morphology, detection, and lateralization of auditory responses in fNIRS. Specifically, we determine whether averaging or generalized linear model (GLM)-based analysis generates different experimental conclusions when applied to a block-protocol design. The impact of parameter selection of GLMs on detecting auditory-evoked responses was also quantified. Approach: 17 listeners were exposed to three commonly employed auditory stimuli: noise, speech, and silence. A block design, comprising sounds of 5 s duration and 10 to 20 s silent intervals, was employed. Results: Both analysis procedures generated similar response morphologies and amplitude estimates, and both indicated that responses to speech were significantly greater than to noise or silence. Neither approach indicated a significant effect of brain hemisphere on responses to speech. Methods to correct for systemic hemodynamic responses using short channels improved detection at the individual level. Conclusions: Consistent with theoretical considerations, simulations, and other experimental domains, GLM and averaging analyses generate the same group-level experimental conclusions. We release this dataset publicly for use in future development and optimization of algorithms.},
	number = {2},
	urldate = {2025-04-06},
	journal = {Neurophotonics},
	author = {Luke, Robert and Larson, Eric D. and Shader, Maureen J. and Innes-Brown, Hamish and Yper, Lindsey Van and Lee, Adrian K. C. and Sowman, Paul F. and McAlpine, David},
	month = may,
	year = {2021},
	note = {Publisher: SPIE},
	pages = {025008},
	file = {Full Text PDF:C\:\\Users\\super\\Zotero\\storage\\MU7PRXUF\\Luke et al. - 2021 - Analysis methods for measuring passive auditory fN.pdf:application/pdf},
}

@article{tachtsidis_false_2016,
	title = {False positives and false negatives in functional near-infrared spectroscopy: issues, challenges, and the way forward},
	volume = {3},
	issn = {2329-423X, 2329-4248},
	shorttitle = {False positives and false negatives in functional near-infrared spectroscopy},
	url = {https://www.spiedigitallibrary.org/journals/neurophotonics/volume-3/issue-3/031405/False-positives-and-false-negatives-in-functional-near-infrared-spectroscopy/10.1117/1.NPh.3.3.031405.full},
	doi = {10.1117/1.NPh.3.3.031405},
	abstract = {We highlight a significant problem that needs to be considered and addressed when performing functional near-infrared spectroscopy (fNIRS) studies, namely the possibility of inadvertently measuring fNIRS hemodynamic responses that are not due to neurovascular coupling. These can be misinterpreted as brain activity, i.e., “false positives” (errors caused by wrongly assigning a detected hemodynamic response to functional brain activity), or mask brain activity, i.e., “false negatives” (errors caused by wrongly assigning a not observed hemodynamic response in the presence of functional brain activity). Here, we summarize the possible physiological origins of these issues and suggest ways to avoid and remove them.},
	number = {3},
	urldate = {2025-04-07},
	journal = {Neurophotonics},
	author = {Tachtsidis, Ilias and Scholkmann, Felix},
	month = mar,
	year = {2016},
	note = {Publisher: SPIE},
	pages = {031405},
	file = {Full Text PDF:C\:\\Users\\super\\Zotero\\storage\\TRI3YXE3\\Tachtsidis and Scholkmann - 2016 - False positives and false negatives in functional .pdf:application/pdf},
}

@article{kocsis_modified_2006,
	title = {The modified {Beer}–{Lambert} law revisited},
	volume = {51},
	issn = {0031-9155},
	url = {https://dx.doi.org/10.1088/0031-9155/51/5/N02},
	doi = {10.1088/0031-9155/51/5/N02},
	abstract = {The modified Beer–Lambert law (MBLL) is the basis of continuous-wave near-infrared tissue spectroscopy (cwNIRS). The differential form of MBLL (dMBLL) states that the change in light attenuation is proportional to the changes in the concentrations of tissue chromophores, mainly oxy- and deoxyhaemoglobin. If attenuation changes are measured at two or more wavelengths, concentration changes can be calculated. The dMBLL is based on two assumptions: (1) the absorption of the tissue changes homogeneously, and (2) the scattering loss is constant. It is known that absorption changes are usually inhomogeneous, and therefore dMBLL underestimates the changes in concentrations (partial volume effect) and every calculated value is influenced by the change in the concentration of other chromophores (cross-talk between chromophores). However, the error introduced by the second assumption (cross-talk of scattering changes) has not been assessed previously. An analytically treatable special case (semi-infinite, homogeneous medium, with optical properties of the cerebral cortex) is utilized here to estimate its order of magnitude. We show that the per cent change of the transport scattering coefficient and that of the absorption coefficient have an approximately equal effect on the changes of attenuation, and a 1\% increase in scattering increases the estimated concentration changes by about 0.5 µM.},
	language = {en},
	number = {5},
	urldate = {2025-04-07},
	journal = {Physics in Medicine \& Biology},
	author = {Kocsis, L and Herman, P and Eke, A},
	month = feb,
	year = {2006},
	pages = {N91},
	file = {IOP Full Text PDF:C\:\\Users\\super\\Zotero\\storage\\Q44F96AH\\Kocsis et al. - 2006 - The modified Beer–Lambert law revisited.pdf:application/pdf},
}

@article{kinder_systematic_2022,
	title = {Systematic review of {fNIRS} studies reveals inconsistent chromophore data reporting practices},
	volume = {9},
	issn = {2329-423X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9780687/},
	doi = {10.1117/1.NPh.9.4.040601},
	abstract = {Significance
Functional near-infrared spectroscopy (fNIRS) is unique among neuroimaging techniques in its ability to estimate changes in both oxyhemoglobin (HbO) and deoxyhemoglobin (HbR). However, fNIRS research has applied various data reporting practices based on these chromophores as measures of neural activation.

Aim
To quantify the variability of fNIRS chromophore data reporting practices and to explore recent data reporting trends in the literature.

Approach
We reviewed 660 fNIRS papers from 2015, 2018, and 2021 to extract information on fNIRS chromophore data reporting practices.

Results
Our review revealed five general practices for reporting fNIRS chromophores: (1) HbO only, (2) HbR only, (3) HbO and HbR, (4) correlation-based signal improvement, and (5) either the total (HbT) or difference (HbDiff) in concentration between chromophores. The field was primarily divided between reporting HbO only and reporting HbO and HbR. However, reporting one chromophore (HbO) was consistently observed as the most popular data reporting practice for each year reviewed.

Conclusions
Our results highlight the high heterogeneity of chromophore data reporting in fNIRS research. We discuss its potential implications for study comparison efforts and interpretation of results. Most importantly, our review demonstrates the need for a standard chromophore reporting practice to improve scientific transparency and, ultimately, to better understand how neural events relate to cognitive phenomena.},
	number = {4},
	urldate = {2025-04-01},
	journal = {Neurophotonics},
	author = {Kinder, Kaleb T. and Heim, Hollis L. R. and Parker, Jessica and Lowery, Kara and McCraw, Alexis and Eddings, Rachel N. and Defenderfer, Jessica and Sullivan, Jacqueline and Buss, Aaron T.},
	month = oct,
	year = {2022},
	pmid = {36578778},
	pmcid = {PMC9780687},
	pages = {040601},
	file = {Full Text PDF:C\:\\Users\\super\\Zotero\\storage\\WB44I7TI\\Kinder et al. - 2022 - Systematic review of fNIRS studies reveals inconsi.pdf:application/pdf},
}

@article{hocke_automated_2018,
	title = {Automated {Processing} of {fNIRS} {Data}—{A} {Visual} {Guide} to the {Pitfalls} and {Consequences}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1999-4893},
	url = {https://www.mdpi.com/1999-4893/11/5/67},
	doi = {10.3390/a11050067},
	abstract = {With the rapid increase in new fNIRS users employing commercial software, there is a concern that many studies are biased by suboptimal processing methods. The purpose of this study is to provide a visual reference showing the effects of different processing methods, to help inform researchers in setting up and evaluating a processing pipeline. We show the significant impact of pre- and post-processing choices and stress again how important it is to combine data from both hemoglobin species in order to make accurate inferences about the activation site.},
	language = {en},
	number = {5},
	urldate = {2025-04-07},
	journal = {Algorithms},
	author = {Hocke, Lia M. and Oni, Ibukunoluwa K. and Duszynski, Chris C. and Corrigan, Alex V. and Frederick, Blaise DeB and Dunn, Jeff F.},
	month = may,
	year = {2018},
	note = {Number: 5
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {channel exclusion, functional Near-Infrared Spectroscopy, GLM, LF de-noising, motion correction, post-processing, pre-processing, single subject},
	pages = {67},
	file = {Full Text PDF:C\:\\Users\\super\\Zotero\\storage\\9G9WPM3W\\Hocke et al. - 2018 - Automated Processing of fNIRS Data—A Visual Guide .pdf:application/pdf},
}

@book{friston_statistical_2007,
	address = {Amsterdam Boston},
	edition = {1st ed},
	title = {Statistical parametric mapping: the analysis of functional brain images},
	isbn = {978-0-12-372560-8},
	shorttitle = {Statistical parametric mapping},
	language = {en},
	publisher = {Elsevier / Academic Press},
	author = {Friston, Karl J.},
	year = {2007},
	file = {Friston - 2007 - Statistical parametric mapping the analysis of fu.pdf:C\:\\Users\\super\\Zotero\\storage\\4KZADBEW\\Friston - 2007 - Statistical parametric mapping the analysis of fu.pdf:application/pdf},
}

@article{singh_exploring_2006,
	title = {Exploring the false discovery rate in multichannel {NIRS}},
	volume = {33},
	issn = {1053-8119},
	url = {https://www.sciencedirect.com/science/article/pii/S105381190600735X},
	doi = {10.1016/j.neuroimage.2006.06.047},
	abstract = {Near infrared spectroscopy (NIRS), an emerging non-invasive tool for functional neuroimaging, has evolved as a multichannel technique allowing simultaneous measurements through many channels ranging from below ten to above hundred. Simultaneous testing of such a large number of channels escalates the risk of Type I error, therefore multiplicity correction is unavoidable. To date, only a few studies have considered this issue using Bonferroni correction, which is an effective conservative solution, but may be too severe for neuroimaging. Its power varies in inverse proportion of the number of channels, which varies among NIRS studies depending on selected region of interest (ROI), thereby leading to a subjective inference. This problem may be well circumvented by a more contemporary approach, called false discovery rate (FDR) that is widely being adopted in functional neuroimaging. An FDR-based procedure controls the expected proportion of erroneously rejected hypotheses among the rejected hypotheses, which offers a more objective, powerful, and consistent measure of Type I error than Bonferroni correction and maintains a better balance between power and specificity. In this technical note, we examine FDR approach using examples from simulated and real NIRS data. The FDR-based procedure could yield 52\% more power than Bonferroni correction in a 172-channel real NIRS study and proved to be more robust against the changing number of channels.},
	number = {2},
	urldate = {2025-05-09},
	journal = {NeuroImage},
	author = {Singh, Archana K. and Dan, Ippeita},
	month = nov,
	year = {2006},
	keywords = {Diffused optical imaging, Familywise error, Optical topography, Statistical testing},
	pages = {542--549},
	file = {1-s2.0-S105381190600735X-main.pdf:C\:\\Users\\super\\Downloads\\1-s2.0-S105381190600735X-main.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\super\\Zotero\\storage\\Q4U6K8WM\\S105381190600735X.html:text/html},
}

@article{hakim_quantification_2023,
	title = {Quantification of inter-brain coupling: {A} review of current methods used in haemodynamic and electrophysiological hyperscanning studies},
	volume = {280},
	issn = {1053-8119},
	shorttitle = {Quantification of inter-brain coupling},
	url = {https://www.sciencedirect.com/science/article/pii/S1053811923005050},
	doi = {10.1016/j.neuroimage.2023.120354},
	abstract = {Hyperscanning is a form of neuroimaging experiment where the brains of two or more participants are imaged simultaneously whilst they interact. Within the domain of social neuroscience, hyperscanning is increasingly used to measure inter-brain coupling (IBC) and explore how brain responses change in tandem during social interaction. In addition to cognitive research, some have suggested that quantification of the interplay between interacting participants can be used as a biomarker for a variety of cognitive mechanisms aswell as to investigate mental health and developmental conditions including schizophrenia, social anxiety and autism. However, many different methods have been used to quantify brain coupling and this can lead to questions about comparability across studies and reduce research reproducibility. Here, we review methods for quantifying IBC, and suggest some ways moving forward. Following the PRISMA guidelines, we reviewed 215 hyperscanning studies, across four different brain imaging modalities: functional near-infrared spectroscopy (fNIRS), functional magnetic resonance (fMRI), electroencephalography (EEG) and magnetoencephalography (MEG). Overall, the review identified a total of 27 different methods used to compute IBC. The most common hyperscanning modality is fNIRS, used by 119 studies, 89 of which adopted wavelet coherence. Based on the results of this literature survey, we first report summary statistics of the hyperscanning field, followed by a brief overview of each signal that is obtained from each neuroimaging modality used in hyperscanning. We then discuss the rationale, assumptions and suitability of each method to different modalities which can be used to investigate IBC. Finally, we discuss issues surrounding the interpretation of each method.},
	urldate = {2025-01-21},
	journal = {NeuroImage},
	author = {Hakim, U and De Felice, S and Pinti, P and Zhang, X and Noah, J．A and Ono, Y and Burgess, P. W. and Hamilton, A and Hirsch, J and Tachtsidis, I},
	month = oct,
	year = {2023},
	keywords = {Dyadic neuroscience, Hyperscanning, Interbrain interaction, Interbrain synchrony, Neuroimaging, Social neuroscience, Two person neuroscience},
	pages = {120354},
	file = {Full Text:C\:\\Users\\super\\Zotero\\storage\\IUG4XSLI\\Hakim et al. - 2023 - Quantification of inter-brain coupling A review o.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\super\\Zotero\\storage\\P7VNY6G9\\S1053811923005050.html:text/html},
}

@article{reddy_evaluation_2021,
	title = {Evaluation of {fNIRS} signal components elicited by cognitive and hypercapnic stimuli},
	volume = {11},
	copyright = {2021 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-021-02076-7},
	doi = {10.1038/s41598-021-02076-7},
	abstract = {Functional near infrared spectroscopy (fNIRS) measurements are confounded by signal components originating from multiple physiological causes, whose activities may vary temporally and spatially (across tissue layers, and regions of the cortex). Furthermore, the stimuli can induce evoked effects, which may lead to over or underestimation of the actual effect of interest. Here, we conducted a temporal, spectral, and spatial analysis of fNIRS signals collected during cognitive and hypercapnic stimuli to characterize effects of functional versus systemic responses. We utilized wavelet analysis to discriminate physiological causes and employed long and short source-detector separation (SDS) channels to differentiate tissue layers. Multi-channel measures were analyzed further to distinguish hemispheric differences. The results highlight cardiac, respiratory, myogenic, and very low frequency (VLF) activities within fNIRS signals. Regardless of stimuli, activity within the VLF band had the largest contribution to the overall signal. The systemic activities dominated the measurements from the short SDS channels during cognitive stimulus, but not hypercapnic stimulus. Importantly, results indicate that characteristics of fNIRS signals vary with type of the stimuli administered as cognitive stimulus elicited variable responses between hemispheres in VLF band and task-evoked temporal effect in VLF, myogenic and respiratory bands, while hypercapnic stimulus induced a global response across both hemispheres.},
	language = {en},
	number = {1},
	urldate = {2025-05-12},
	journal = {Scientific Reports},
	author = {Reddy, Pratusha and Izzetoglu, Meltem and Shewokis, Patricia A. and Sangobowale, Michael and Diaz-Arrastia, Ramon and Izzetoglu, Kurtulus},
	month = dec,
	year = {2021},
	note = {Publisher: Nature Publishing Group},
	keywords = {Cognitive neuroscience, Neuro–vascular interactions, Neuronal physiology},
	pages = {23457},
	file = {Full Text PDF:C\:\\Users\\super\\Zotero\\storage\\9ZZQAWPK\\Reddy et al. - 2021 - Evaluation of fNIRS signal components elicited by .pdf:application/pdf},
}

@article{bergmann_evaluation_2023,
	title = {Evaluation of {Morlet} {Wavelet} {Analysis} for {Artifact} {Detection} in {Low}-{Frequency} {Commercial} {Near}-{Infrared} {Spectroscopy} {Systems}},
	volume = {11},
	issn = {2306-5354},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11154537/},
	doi = {10.3390/bioengineering11010033},
	abstract = {Regional cerebral oxygen saturation (rSO2), a method of cerebral tissue oxygenation measurement, is recorded using non-invasive near-infrared Spectroscopy (NIRS) devices. A major limitation is that recorded signals often contain artifacts. Manually removing these artifacts is both resource and time consuming. The objective was to evaluate the applicability of using wavelet analysis as an automated method for simple signal loss artifact clearance of rSO2 signals obtained from commercially available devices. A retrospective observational study using existing populations (healthy control (HC), elective spinal surgery patients (SP), and traumatic brain injury patients (TBI)) was conducted. Arterial blood pressure (ABP) and rSO2 data were collected in all patients. Wavelet analysis was determined to be successful in removing simple signal loss artifacts using wavelet coefficients and coherence to detect signal loss artifacts in rSO2 signals. The removal success rates in HC, SP, and TBI populations were 100\%, 99.8\%, and 99.7\%, respectively (though it had limited precision in determining the exact point in time). Thus, wavelet analysis may prove to be useful in a layered approach NIRS signal artifact tool utilizing higher-frequency data; however, future work is needed.},
	number = {1},
	urldate = {2025-05-12},
	journal = {Bioengineering},
	author = {Bergmann, Tobias and Froese, Logan and Gomez, Alwyn and Sainbhi, Amanjyot Singh and Vakitbilir, Nuray and Islam, Abrar and Stein, Kevin and Marquez, Izzy and Amenta, Fiorella and Park, Kevin and Ibrahim, Younis and Zeiler, Frederick A.},
	month = dec,
	year = {2023},
	pmid = {38247909},
	pmcid = {PMC11154537},
	pages = {0},
	file = {Full Text:C\:\\Users\\super\\Zotero\\storage\\CZ33A2MR\\Bergmann et al. - 2023 - Evaluation of Morlet Wavelet Analysis for Artifact.pdf:application/pdf},
}

@article{xu_functional_2017,
	title = {Functional connectivity analysis of distracted drivers based on the wavelet phase coherence of functional near-infrared spectroscopy signals},
	volume = {12},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0188329},
	doi = {10.1371/journal.pone.0188329},
	abstract = {The present study aimed to evaluate the functional connectivity (FC) in relevant cortex areas during simulated driving with distraction based on functional near-infrared spectroscopy (fNIRS) method. Twelve subjects were recruited to perform three types of driving tasks, namely, straight driving, straight driving with secondary auditory task, and straight driving with secondary visual vigilance task, on a driving simulator. The wavelet amplitude (WA) and wavelet phase coherence (WPCO) of the fNIRS signals were calculated in six frequency intervals: I, 0.6–2 Hz; II, 0.145–0.6 Hz; III, 0.052–0.145 Hz; IV, 0.021–0.052 Hz; and V, 0.0095–0.021 Hz, VI, 0.005–0.0095Hz. Results showed that secondary tasks during driving led to worse driving performance, brain activity changes, and dynamic configuration of the connectivity. The significantly lower WA value in the right motor cortex in interval IV, and higher WPCO values in intervals II, V, and VI were found with additional auditory task. Significant standard deviation of speed and lower WA values in the left prefrontal cortex and right prefrontal cortex in interval VI, and lower WPCO values in intervals I, IV, V, and VI were found under the additional visual vigilance task. The results suggest that the changed FC levels in intervals IV, V, and VI were more likely to reflect the driver’s distraction condition. The present study provides new insights into the relationship between distracted driving behavior and brain activity. The method may be used for the evaluation of drivers’ attention level.},
	language = {en},
	number = {11},
	urldate = {2025-05-12},
	journal = {PLOS ONE},
	author = {Xu, Gongcheng and Zhang, Ming and Wang, Yan and Liu, Zhian and Huo, Congcong and Li, Zengyong and Huo, Mengyou},
	month = nov,
	year = {2017},
	note = {Publisher: Public Library of Science},
	keywords = {Attention, Blood pressure, Cognitive science, Motor cortex, Neuroimaging, Prefrontal cortex, Vigilance, Vision},
	pages = {e0188329},
	file = {Full Text PDF:C\:\\Users\\super\\Zotero\\storage\\M2SWMQGF\\Xu et al. - 2017 - Functional connectivity analysis of distracted dri.pdf:application/pdf},
}

@article{bastos_tutorial_2016,
	title = {A {Tutorial} {Review} of {Functional} {Connectivity} {Analysis} {Methods} and {Their} {Interpretational} {Pitfalls}},
	volume = {9},
	issn = {1662-5137},
	url = {https://www.frontiersin.org/journals/systems-neuroscience/articles/10.3389/fnsys.2015.00175/full},
	doi = {10.3389/fnsys.2015.00175},
	abstract = {Oscillatory neuronal activity may provide a mechanism for dynamic network coordination. Rhythmic neuronal interactions can be quantified using multiple metrics, each with their own advantages and disadvantages. This tutorial will review and summarize current analysis methods used in the field of invasive and non-invasive electrophysiology to study the dynamic connections between neuronal populations. First, we review metrics for functional connectivity, including coherence, phase synchronization, phase-slope index, and Granger causality, with the specific aim to provide an intuition for how these metrics work, as well as their quantitative definition. Next, we highlight a number of interpretational caveats and common pitfalls that can arise when performing functional connectivity analysis, including the common reference problem, the signal to noise ratio problem, the volume conduction problem, the common input problem, and the sample size bias problem. These pitfalls will be illustrated by presenting a set of MATLAB-scripts, which can be executed by the reader to simulate each of these potential problems. We discuss how these issues can be addressed using current methods.},
	language = {English},
	urldate = {2025-05-12},
	journal = {Frontiers in Systems Neuroscience},
	author = {Bastos, André M. and Schoffelen, Jan-Mathijs},
	month = jan,
	year = {2016},
	note = {Publisher: Frontiers},
	keywords = {coherence analysis, Electrophysiology, functional connectivity (FC), Granger causality, oscillations, phase synchronization},
	file = {Full Text PDF:C\:\\Users\\super\\Zotero\\storage\\R7UCAUHZ\\Bastos and Schoffelen - 2016 - A Tutorial Review of Functional Connectivity Analy.pdf:application/pdf},
}

@article{miranda_de_sa_coherence_2009,
	title = {Coherence estimate between a random and a periodic signal: {Bias}, variance, analytical critical values, and normalizing transforms},
	volume = {346},
	issn = {0016-0032},
	shorttitle = {Coherence estimate between a random and a periodic signal},
	url = {https://www.sciencedirect.com/science/article/pii/S0016003209000738},
	doi = {10.1016/j.jfranklin.2009.07.009},
	abstract = {The present work deals with recent results on the sampling distribution of the magnitude-squared coherence (also called just coherence) estimate between a random (Gaussian) and a periodic signal, in order to obtain analytical critical values, alternative expressions for the probability density function (PDF) as well as the variance and bias of the estimate. A comparison with the more general case of coherence estimation when both signals are Gaussian was also provided. The results indicate that the smaller the true coherence (TC) values the closer both distributions become. The behaviour of variance and bias as a function of the number of data segments and the TC is similar for both coherence estimates. Additionally, the effect of a normalizing function (Fisher's z transform) in the coherence estimated between a random and a periodic signal was also evaluated and normality has been nearly achieved. However, the variance was less equalized in comparison with coherence estimate between two Gaussian signals.},
	number = {9},
	urldate = {2025-05-12},
	journal = {Journal of the Franklin Institute},
	author = {Miranda de Sá, Antonio Mauricio F. L. and Ferreira, Danton D. and Dias, Edson W. and Mendes, Eduardo M. A. M. and Felix, Leonardo B.},
	month = nov,
	year = {2009},
	keywords = {Coherence, Estimation, Fisher's z transform, Periodic input, Statistics},
	pages = {841--853},
	file = {1-s2.0-S0016003209000738-main.pdf:C\:\\Users\\super\\Downloads\\1-s2.0-S0016003209000738-main.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\super\\Zotero\\storage\\WD35LZY8\\S0016003209000738.html:text/html},
}

@article{hu_characterizing_2023,
	title = {Characterizing the spatiotemporal features of functional connectivity across the white matter and gray matter during the naturalistic condition},
	volume = {17},
	issn = {1662-453X},
	url = {https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2023.1248610/full},
	doi = {10.3389/fnins.2023.1248610},
	abstract = {Introduction
The naturalistic stimuli due to its ease of operability has attracted many researchers in recent years. However, the influence of the naturalistic stimuli for whole-brain functions compared with the resting state is still unclear.

Methods
In this study, we clustered gray matter (GM) and white matter (WM) masks both at the ROI- and network-levels. Functional connectivity (FC) and inter-subject functional connectivity (ISFC) were calculated in GM, WM, and between GM and WM under the movie-watching and the resting-state conditions. Furthermore, intra-class correlation coefficients (ICC) of FC and ISFC were estimated on different runs of fMRI data to denote the reliability of them during the two conditions. In addition, static and dynamic connectivity indices were calculated with Pearson correlation coefficient to demonstrate the associations between the movie-watching and the resting-state.

Results
As the results, we found that the movie-watching significantly affected FC in whole-brain compared with the resting-state, but ISFC did not show significant connectivity induced by the naturalistic condition. ICC of FC and ISFC was generally higher during movie-watching compared with the resting-state, demonstrating that naturalistic stimuli could promote the reliability of connectivity. The associations between static and dynamic ISFC were weakly negative correlations in the naturalistic stimuli while there is no correlation between them under resting-state condition.

Discussion
Our findings confirmed that compared to resting-state condition, the connectivity indices under the naturalistic stimuli were more reliable and stable to investigate the normal functional activities of the human brain, and might promote the applications of FC in the cerebral dysfunction in various mental disorders.},
	language = {English},
	urldate = {2025-05-12},
	journal = {Frontiers in Neuroscience},
	author = {Hu, Peng and Wang, Pan and Zhao, Rong and Yang, Hang and Biswal, Bharat B.},
	month = nov,
	year = {2023},
	note = {Publisher: Frontiers},
	keywords = {functional connectivity, inter-subject functional connectivity, Intra-class correlation coefficient, naturalistic condition, white matter},
	file = {Full Text PDF:C\:\\Users\\super\\Zotero\\storage\\ASDH782Z\\Hu et al. - 2023 - Characterizing the spatiotemporal features of func.pdf:application/pdf},
}

@inproceedings{seabold2010statsmodels,
  title={statsmodels: Econometric and statistical modeling with python},
  author={Seabold, Skipper and Perktold, Josef},
  booktitle={9th Python in Science Conference},
  year={2010},
}

@article{garcia_design_2020,
	title = {Design of reliable virtual human facial expressions and validation by healthy people},
	volume = {27},
	issn = {10692509, 18758835},
	url = {https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/ICA-200623},
	doi = {10.3233/ICA-200623},
	abstract = {The level of realism that real-time virtual humans have reached in the last years enables their use as an alternative to pictures and videos in the remediation of social cognition deﬁcits. This paper presents the engineering principles and tools used to design facial expressions on virtual humans to play basic emotions. The proposal is based on the Facial Action Coding System that makes it possible to easily represent facial expressions. Then, the paper describes how the designed virtual human facial emotions have been assessed by healthy people. For that purpose, 204 healthy participants have taken part in an experiment in which they had to recognize the six basic emotions (each of them with two levels of intensity) depicted by the virtual humans. The overall accuracy of the emotion identiﬁcation task was 88.25\%, which outperforms most results obtained by other authors using virtual humans and/or pictures. The best recognized emotions were neutral, happiness and anger. Remarkably striking was the high success rate gotten for disgust, far superior to previous studies based on virtual reality. Unlike other works, no signiﬁcant differences were found between women and men in the recognition of emotions, probably due to an enhanced dynamism and realism of the designed human faces. However, age-related differences were found for some emotions in favor of the younger participants. In addition, higher emotion identiﬁcation rates were detected for higher intensity representations of each emotion, for more dynamic avatars and for faces shown frontally compared to lateral ones. Therefore, the results of the evaluation experiment have demonstrated that virtual humans perfectly convey emotions using facial expressions.},
	language = {en},
	number = {3},
	urldate = {2023-10-17},
	journal = {Integrated Computer-Aided Engineering},
	author = {García, Arturo S. and Fernández-Sotos, Patricia and Vicente-Querol, Miguel A. and Lahera, Guillermo and Rodriguez-Jimenez, Roberto and Fernández-Caballero, Antonio},
	editor = {Ferrández, José Manuel and Andina, Diego and Fernández, Eduardo},
	month = may,
	year = {2020},
	pages = {287--299},
	file = {2020 - Design of reliable virtual human facial expressions and validation by healthy people .pdf:G\:\\My Drive\\School Work\\Ontario Tech\\Research\\Lit Review\\2020 - Design of reliable virtual human facial expressions and validation by healthy people .pdf:application/pdf},
}

@article{kegel_dynamic_2020,
	title = {Dynamic human and avatar facial expressions elicit differential brain responses},
	volume = {15},
	issn = {1749-5016},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7235958/},
	doi = {10.1093/scan/nsaa039},
	abstract = {Computer-generated characters, so-called avatars, are widely used in advertising, entertainment, human–computer interaction or as research tools to investigate human emotion perception. However, brain responses to avatar and human faces have scarcely been studied to date. As such, it remains unclear whether dynamic facial expressions of avatars evoke different brain responses than dynamic facial expressions of humans. In this study, we designed anthropomorphic avatars animated with motion tracking and tested whether the human brain processes fearful and neutral expressions in human and avatar faces differently. Our fMRI results showed that fearful human expressions evoked stronger responses than fearful avatar expressions in the ventral anterior and posterior cingulate gyrus, the anterior insula, the anterior and posterior superior temporal sulcus, and the inferior frontal gyrus. Fearful expressions in human and avatar faces evoked similar responses in the amygdala. We did not find different responses to neutral human and avatar expressions. Our results highlight differences, but also similarities in the processing of fearful human expressions and fearful avatar expressions even if they are designed to be highly anthropomorphic and animated with motion tracking. This has important consequences for research using dynamic avatars, especially when processes are investigated that involve cortical and subcortical regions.},
	number = {3},
	urldate = {2025-06-17},
	journal = {Social Cognitive and Affective Neuroscience},
	author = {Kegel, Lorena C and Brugger, Peter and Frühholz, Sascha and Grunwald, Thomas and Hilfiker, Peter and Kohnen, Oona and Loertscher, Miriam L and Mersch, Dieter and Rey, Anton and Sollfrank, Teresa and Steiger, Bettina K and Sternagel, Joerg and Weber, Michel and Jokeit, Hennric},
	month = mar,
	year = {2020},
	pmid = {32232359},
	pmcid = {PMC7235958},
	pages = {303--317},
	file = {Full Text:C\:\\Users\\super\\Zotero\\storage\\TMG7Y3XA\\Kegel et al. - 2020 - Dynamic human and avatar facial expressions elicit.pdf:application/pdf},
}

@article{park_individuals_2021,
	title = {Individual’s {Social} {Perception} of {Virtual} {Avatars} {Embodied} with {Their} {Habitual} {Facial} {Expressions} and {Facial} {Appearance}},
	volume = {21},
	issn = {1424-8220},
	url = {https://www.mdpi.com/1424-8220/21/17/5986},
	doi = {10.3390/s21175986},
	abstract = {With the prevalence of virtual avatars and the recent emergence of metaverse technology, there has been an increase in users who express their identity through an avatar. The research community focused on improving the realistic expressions and non-verbal communication channels of virtual characters to create a more customized experience. However, there is a lack in the understanding of how avatars can embody a user’s signature expressions (i.e., user’s habitual facial expressions and facial appearance) that would provide an individualized experience. Our study focused on identifying elements that may affect the user’s social perception (similarity, familiarity, attraction, liking, and involvement) of customized virtual avatars engineered considering the user’s facial characteristics. We evaluated the participant’s subjective appraisal of avatars that embodied the participant’s habitual facial expressions or facial appearance. Results indicated that participants felt that the avatar that embodied their habitual expressions was more similar to them than the avatar that did not. Furthermore, participants felt that the avatar that embodied their appearance was more familiar than the avatar that did not. Designers should be mindful about how people perceive individuated virtual avatars in order to accurately represent the user’s identity and help users relate to their avatar.},
	language = {en},
	number = {17},
	urldate = {2023-10-17},
	journal = {Sensors},
	author = {Park, Sung and Kim, Si Pyoung and Whang, Mincheol},
	month = sep,
	year = {2021},
	pages = {5986},
	file = {2021 - Individual’s Social Perception of Virtual Avatars Embodied with Their Habitual Facial Expressions and Facial Appearance.pdf:G\:\\My Drive\\School Work\\Ontario Tech\\Research\\Lit Review\\2021 - Individual’s Social Perception of Virtual Avatars Embodied with Their Habitual Facial Expressions and Facial Appearance.pdf:application/pdf},
}

@article{de_borst_is_2015,
	title = {Is it the real deal? {Perception} of virtual characters versus humans: an affective cognitive neuroscience perspective},
	volume = {6},
	issn = {1664-1078},
	shorttitle = {Is it the real deal?},
	url = {http://www.frontiersin.org/Cognitive_Science/10.3389/fpsyg.2015.00576/abstract},
	doi = {10.3389/fpsyg.2015.00576},
	abstract = {Recent developments in neuroimaging research support the increased use of naturalistic stimulus material such as film, avatars, or androids. These stimuli allow for a better understanding of how the brain processes information in complex situations while maintaining experimental control. While avatars and androids are well suited to study human cognition, they should not be equated to human stimuli. For example, the uncanny valley hypothesis theorizes that artificial agents with high human-likeness may evoke feelings of eeriness in the human observer. Here we review if, when, and how the perception of human-like avatars and androids differs from the perception of humans and consider how this influences their utilization as stimulus material in social and affective neuroimaging studies. First, we discuss how the appearance of virtual characters affects perception. When stimuli are morphed across categories from non-human to human, the most ambiguous stimuli, rather than the most human-like stimuli, show prolonged classification times and increased eeriness. Human-like to human stimuli show a positive linear relationship with familiarity. Secondly, we show that expressions of emotions in human-like avatars can be perceived similarly to human emotions, with corresponding behavioral, physiological and neuronal activations, with exception of physical dissimilarities. Subsequently, we consider if and when one perceives differences in action representation by artificial agents versus humans. Motor resonance and predictive coding models may account for empirical findings, such as an interference effect on action for observed human-like, natural moving characters. However, the expansion of these models to explain more complex behavior, such as empathy, still needs to be investigated in more detail. Finally, we broaden our outlook to social interaction, where virtual reality stimuli can be utilized to imitate complex social situations.},
	language = {en},
	urldate = {2023-10-17},
	journal = {Frontiers in Psychology},
	author = {De Borst, Aline W. and De Gelder, Beatrice},
	month = may,
	year = {2015},
	file = {2015 - Is it the real deal- Perception of virtual characters versus humans- an affective cognitive neuroscience perspective.pdf:G\:\\My Drive\\School Work\\Ontario Tech\\Research\\Lit Review\\2015 - Is it the real deal- Perception of virtual characters versus humans- an affective cognitive neuroscience perspective.pdf:application/pdf},
}

@incollection{de_paolis_perception_2015,
	address = {Cham},
	title = {Perception of {Basic} {Emotions} from {Facial} {Expressions} of {Dynamic} {Virtual} {Avatars}},
	volume = {9254},
	isbn = {978-3-319-22887-7 978-3-319-22888-4},
	url = {https://link.springer.com/10.1007/978-3-319-22888-4_30},
	abstract = {Virtual Reality experiences featuring realistic Virtual Humans with convincing facial expressions are a useful tool to improve social skill in humans. For this reason several investigations have been carried out on the recognition of virtual avatar emotions, based on dynamic and static facial cues originated by basic emotions developed by Ekman. Dynamism and realism of facial expressions are both important aspects of the process of face-to-face interaction in everyday life. In this paper we present the results of a research aiming at investigating the impact of the combination of dynamic facial expressions corresponding to particular emotions with a high level of realism of virtual faces. A study where we have measured the level of intensity in the correspondence between facial expressions of virtual avatars and emotional stimuli perceived by an observer was carried out on two groups of participants with diﬀerent expertise in Virtual Reality. Results show a high level of intensity in this correspondence in both groups through the evaluation of two variables: time response and the score assigned to each emotion. We suggest that the use of dynamic virtual avatars oﬀers advantages for studying emotion recognition in a face in that they recreate a realistic stimuli in emotion research.},
	language = {en},
	urldate = {2023-10-17},
	booktitle = {Augmented and {Virtual} {Reality}},
	publisher = {Springer International Publishing},
	author = {Faita, Claudia and Vanni, Federico and Lorenzini, Cristian and Carrozzino, Marcello and Tanca, Camilla and Bergamasco, Massimo},
	editor = {De Paolis, Lucio Tommaso and Mongelli, Antonio},
	year = {2015},
	doi = {10.1007/978-3-319-22888-4_30},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {409--419},
	file = {2015 - Perception of Basic Emotions from Facial Expressions of Dynamic Virtual Avatars.pdf:G\:\\My Drive\\School Work\\Ontario Tech\\Research\\Lit Review\\2015 - Perception of Basic Emotions from Facial Expressions of Dynamic Virtual Avatars.pdf:application/pdf},
}

@article{snoek_testing_2023,
	title = {Testing, explaining, and exploring models of facial expressions of emotions},
	volume = {9},
	issn = {2375-2548},
	url = {https://www.science.org/doi/10.1126/sciadv.abq8421},
	doi = {10.1126/sciadv.abq8421},
	abstract = {Models are the hallmark of mature scientific inquiry. In psychology, this maturity has been reached in a pervasive question—what models best represent facial expressions of emotion? Several hypotheses propose different combinations of facial movements [action units (AUs)] as best representing the six basic emotions and four conversational signals across cultures. We developed a new framework to formalize such hypotheses as predictive models, compare their ability to predict human emotion categorizations in Western and East Asian cultures, explain the causal role of individual AUs, and explore updated, culture-accented models that improve performance by reducing a prevalent Western bias. Our predictive models also provide a noise ceiling to inform the explanatory power and limitations of different factors (e.g., AUs and individual differences). Thus, our framework provides a new approach to test models of social signals, explain their predictive power, and explore their optimization, with direct implications for theory development.
          , 
            A framework evaluates facial expression models, reveals their Western bias, and develops better, culture-accented models.},
	language = {en},
	number = {6},
	urldate = {2024-04-04},
	journal = {Science Advances},
	author = {Snoek, Lukas and Jack, Rachael E. and Schyns, Philippe G. and Garrod, Oliver G. B. and Mittenbühler, Maximilian and Chen, Chaona and Oosterwijk, Suzanne and Scholte, H. Steven},
	month = feb,
	year = {2023},
	pages = {eabq8421},
	file = {Full Text PDF:C\:\\Users\\super\\Zotero\\storage\\AVB7WEHK\\Snoek et al. - 2023 - Testing, explaining, and exploring models of facia.pdf:application/pdf},
}

@article{hortensius_perception_2018,
	title = {The {Perception} of {Emotion} in {Artificial} {Agents}},
	volume = {10},
	issn = {2379-8920, 2379-8939},
	url = {https://ieeexplore.ieee.org/document/8341761/},
	doi = {10.1109/TCDS.2018.2826921},
	abstract = {Given recent technological developments in robotics, artiﬁcial intelligence, and virtual reality, it is perhaps unsurprising that the arrival of emotionally expressive and reactive artiﬁcial agents is imminent. However, if such agents are to become integrated into our social milieu, it is imperative to establish an understanding of whether and how humans perceive emotion in artiﬁcial agents. In this review, we incorporate recent ﬁndings from social robotics, virtual reality, psychology, and neuroscience to examine how people recognize and respond to emotions displayed by artiﬁcial agents. First, we review how people perceive emotions expressed by an artiﬁcial agent, such as facial and bodily expressions. Second, we evaluate the similarities and differences in the consequences of perceived emotions in artiﬁcial compared to human agents. Besides accurately recognizing the emotional state of an artiﬁcial agent, it is critical to understand how humans respond to those emotions. Does interacting with an angry robot induce the same responses in people as interacting with an angry person? Similarly, does watching a robot rejoice when it wins a game elicit similar feelings of elation in the human observer? Here, we provide an overview of the current state of emotion expression and perception during interactions with artiﬁcial agents, as well as a clear articulation of the challenges and guiding principles to be addressed as we move ever closer to truly emotional artiﬁcial agents.},
	language = {en},
	number = {4},
	urldate = {2023-10-17},
	journal = {IEEE Transactions on Cognitive and Developmental Systems},
	author = {Hortensius, Ruud and Hekele, Felix and Cross, Emily S.},
	month = dec,
	year = {2018},
	pages = {852--864},
	file = {2018 - The Perception of Emotion in Artificial Agents.pdf:G\:\\My Drive\\School Work\\Ontario Tech\\Research\\Lit Review\\2018 - The Perception of Emotion in Artificial Agents.pdf:application/pdf},
}

@article{westgarth_systematic_2021,
	title = {A systematic review of studies that used {NIRS} to measure neural activation during emotion processing in healthy individuals},
	volume = {16},
	copyright = {http://creativecommons.org/licenses/by-nc/4.0/},
	issn = {1749-5016, 1749-5024},
	url = {https://academic.oup.com/scan/article/16/4/345/6126231},
	doi = {10.1093/scan/nsab017},
	abstract = {Functional neuroimaging provides an avenue for earlier diagnosis and tailored treatment of psychological disorders characterised by emotional impairment. Near-infrared spectroscopy (NIRS) offers ecological advantages compared to other neuroimaging techniques and suitability of measuring regions involved in emotion functions. A systematic review was conducted to evaluate the capacity of NIRS to detect activation during emotion processing and to provide recommendations for future research. Following a comprehensive literature search, we reviewed 85 journal articles, which compared activation during emotional experience, regulation or perception with either a neutral condition or baseline period among healthy participants. The quantitative synthesis of outcomes was limited to thematical analysis, owing to the lack of standardisation between studies. Although most studies found increased prefrontal activity during emotional experience and regulation, the findings were more inconsistent for emotion perception. Some researchers reported increased activity during the task, some reported decreases, some no significant changes, and some reported mixed findings depending on the valence and region. We propose that variations in the cognitive task and stimuli, recruited sample, and measurement and analysis of data are the primary causes of inconsistency. Recommendations to improve consistency in future research by carefully considering the choice of population, cognitive task and analysis approach are provided.},
	language = {en},
	number = {4},
	urldate = {2025-04-20},
	journal = {Social Cognitive and Affective Neuroscience},
	author = {Westgarth, Matthew M P and Hogan, Christy A and Neumann, David L and Shum, David H K},
	month = mar,
	year = {2021},
	pages = {345--369},
	file = {Westgarth et al. - 2021 - A systematic review of studies that used NIRS to m.pdf:C\:\\Users\\super\\Zotero\\storage\\VRCLZ459\\Westgarth et al. - 2021 - A systematic review of studies that used NIRS to m.pdf:application/pdf},
}

@article{kawasaki_processing_2012,
	title = {Processing of {Facial} {Emotion} in the {Human} {Fusiform} {Gyrus}},
	volume = {24},
	issn = {0898-929X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3566877/},
	doi = {10.1162/jocn_a_00175},
	abstract = {Electrophysiological and fMRI-based investigations of the ventral temporal cortex of primates provide strong support for regional specialization for the processing of faces. These responses are most frequently found in or near the fusiform gyrus, but there is substantial variability in their anatomical location and response properties. An outstanding question is the extent to which ventral temporal cortex participates in processing dynamic, expressive aspects of faces, a function usually attributed to regions near the superior temporal cortex. Here, we investigated these issues through intracranial recordings from eight human surgical patients. We compared several different aspects of face processing (static and dynamic faces; happy, neutral, and fearful expressions) with power in the high-gamma band (70–150 Hz) from a spectral analysis. Detailed mapping of the response characteristics as a function of anatomical location was conducted in relation to the gyral and sulcal pattern on each patient’s brain. The results document responses with high responsiveness for static or dynamic faces, often showing abrupt changes in response properties between spatially close recording sites and idiosyncratic across different subjects. Notably, strong responses to dynamic facial expressions can be found in the fusiform gyrus, just as can responses to static faces. The findings suggest a more complex, fragmented architecture of ventral temporal cortex around the fusiform gyrus, one that includes focal regions of cortex that appear relatively specialized for either static or dynamic aspects of faces.},
	number = {6},
	urldate = {2025-04-23},
	journal = {Journal of cognitive neuroscience},
	author = {Kawasaki, Hiroto and Tsuchiya, Naotsugu and Kovach, Christopher K. and Nourski, Kirill V. and Oya, Hiroyuki and Howard, Matthew A. and Adolphs, Ralph},
	month = jun,
	year = {2012},
	pmid = {22185494},
	pmcid = {PMC3566877},
	pages = {1358--1370},
	file = {Full Text PDF:C\:\\Users\\super\\Zotero\\storage\\98FMLVVN\\Kawasaki et al. - 2012 - Processing of Facial Emotion in the Human Fusiform.pdf:application/pdf},
}

@article{tak_statistical_2014,
	series = {Celebrating 20 {Years} of {Functional} {Near} {Infrared} {Spectroscopy} ({fNIRS})},
	title = {Statistical analysis of {fNIRS} data: {A} comprehensive review},
	volume = {85},
	issn = {1053-8119},
	shorttitle = {Statistical analysis of {fNIRS} data},
	url = {https://www.sciencedirect.com/science/article/pii/S1053811913006538},
	doi = {10.1016/j.neuroimage.2013.06.016},
	abstract = {Functional near-infrared spectroscopy (fNIRS) is a non-invasive method to measure brain activities using the changes of optical absorption in the brain through the intact skull. fNIRS has many advantages over other neuroimaging modalities such as positron emission tomography (PET), functional magnetic resonance imaging (fMRI), or magnetoencephalography (MEG), since it can directly measure blood oxygenation level changes related to neural activation with high temporal resolution. However, fNIRS signals are highly corrupted by measurement noises and physiology-based systemic interference. Careful statistical analyses are therefore required to extract neuronal activity-related signals from fNIRS data. In this paper, we provide an extensive review of historical developments of statistical analyses of fNIRS signal, which include motion artifact correction, short source-detector separation correction, principal component analysis (PCA)/independent component analysis (ICA), false discovery rate (FDR), serially-correlated errors, as well as inference techniques such as the standard t-test, F-test, analysis of variance (ANOVA), and statistical parameter mapping (SPM) framework. In addition, to provide a unified view of various existing inference techniques, we explain a linear mixed effect model with restricted maximum likelihood (ReML) variance estimation, and show that most of the existing inference methods for fNIRS analysis can be derived as special cases. Some of the open issues in statistical analysis are also described.},
	urldate = {2025-02-16},
	journal = {NeuroImage},
	author = {Tak, Sungho and Ye, Jong Chul},
	month = jan,
	year = {2014},
	keywords = {fNIRS, -Test, Correlation analysis, Data-driven analysis, GLM, Group analysis, Multi-level analysis, Multiple comparison, Spectral analysis, Statistical parameter mapping},
	pages = {72--91},
	file = {ScienceDirect Snapshot:C\:\\Users\\super\\Zotero\\storage\\NBSX69XF\\S1053811913006538.html:text/html;Tak and Ye - 2014 - Statistical analysis of fNIRS data A comprehensiv.pdf:C\:\\Users\\super\\Zotero\\storage\\YY7S757B\\Tak and Ye - 2014 - Statistical analysis of fNIRS data A comprehensiv.pdf:application/pdf},
}

@article{kragel_decoding_2016,
	title = {Decoding the {Nature} of {Emotion} in the {Brain}},
	volume = {20},
	issn = {1364-6613},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4875847/},
	doi = {10.1016/j.tics.2016.03.011},
	abstract = {A central, unresolved problem in affective neuroscience is understanding how emotions are represented in nervous system activity. After prior localization approaches largely failed, researchers began applying multivariate statistical tools to reconceptualize how emotion constructs might be embedded in large-scale brain networks. Findings from pattern analyses of neuroimaging data show that affective dimensions and emotion categories are uniquely represented in the activity of distributed neural systems that span cortical and subcortical regions. Results from multiple-category decoding studies are incompatible with theories postulating that specific emotions emerge from the neural coding of valence and arousal. This ‘new look’ into emotion representation promises to improve and reformulate neurobiological models of affect.},
	number = {6},
	urldate = {2025-06-18},
	journal = {Trends in cognitive sciences},
	author = {Kragel, Philip A. and LaBar, Kevin S.},
	month = jun,
	year = {2016},
	pmid = {27133227},
	pmcid = {PMC4875847},
	pages = {444--455},
	file = {Accepted Version:C\:\\Users\\super\\Zotero\\storage\\5IYDPQ6B\\Kragel and LaBar - 2016 - Decoding the Nature of Emotion in the Brain.pdf:application/pdf},
}

@article{katsyri_testing_2017,
	title = {Testing the ‘uncanny valley’ hypothesis in semirealistic computer-animated film characters: {An} empirical evaluation of natural film stimuli},
	volume = {97},
	issn = {1071-5819},
	shorttitle = {Testing the ‘uncanny valley’ hypothesis in semirealistic computer-animated film characters},
	url = {https://www.sciencedirect.com/science/article/pii/S1071581916301227},
	doi = {10.1016/j.ijhcs.2016.09.010},
	abstract = {The uncanny valley (UV) hypothesis, which predicts that almost but not fully humanlike artificial characters elicit negative evaluations, has become increasingly influential. At the same time, the hypothesis has become associated with many computer-animated films that have aimed at high realism. In the present investigation, we tested whether semirealistic animated film characters do in fact elicit negative evaluations. Fifty-four participants were asked to evaluate five matched film excerpts from each of cartoonish, semirealistic, and human-acted films. Mixed model analyses were conducted to reduce the effects of participant and stimulus related confounds. Explicit selections made after the experiment confirmed that participants associated semirealistic film characters correctly with the UV. Semirealistic animated characters also received higher eeriness ratings than the other film characters. In particular, two semirealistic films ‘Beowulf’ and ‘The Polar Express’ were selected the most often explicitly, and ‘Beowulf’ also received higher eeriness ratings than any other film. Somewhat unexpectedly, cartoonish characters received the highest strangeness ratings and (after confound correction) the lowest likability ratings. Taken together, the present findings demonstrate that semirealistic animated film characters are more eerie than cartoonish characters or real actors, and hence provide evidence for the existence of the UV in animated film characters.},
	urldate = {2025-06-18},
	journal = {International Journal of Human-Computer Studies},
	author = {Kätsyri, Jari and Mäkäräinen, Meeri and Takala, Tapio},
	month = jan,
	year = {2017},
	keywords = {Animation films, Anthropomorphism, Computer animation, Human-computer interaction, Uncanny valley},
	pages = {149--161},
	file = {1-s2.0-S1071581916301227-main.pdf:C\:\\Users\\super\\Downloads\\1-s2.0-S1071581916301227-main.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\super\\Zotero\\storage\\39ZYXHMQ\\S1071581916301227.html:text/html},
}

@article{sollfrank_effects_2021,
	title = {The {Effects} of {Dynamic} and {Static} {Emotional} {Facial} {Expressions} of {Humans} and {Their} {Avatars} on the {EEG}: {An} {ERP} and {ERD}/{ERS} {Study}},
	volume = {15},
	issn = {1662-453X},
	shorttitle = {The {Effects} of {Dynamic} and {Static} {Emotional} {Facial} {Expressions} of {Humans} and {Their} {Avatars} on the {EEG}},
	url = {https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2021.651044/full},
	doi = {10.3389/fnins.2021.651044},
	abstract = {This study aimed to examine whether the cortical processing of emotional faces is modulated by the computerization of face stimuli ("avatars") in a group of 25 healthy participants. Subjects were passively viewing 128 static and dynamic facial expressions of female and male actors and their respective avatars in neutral or fearful conditions. Event-related potentials (ERPs), as well as alpha and theta event-related synchronization and desynchronization (ERD/ERS), were derived from the EEG that was recorded during the task. All ERP features, except for the very early N100, differed in their response to avatar and actor faces. Whereas the N170 showed differences only for the neutral avatar condition, later potentials (N300 and LPP) differed in both emotional conditions (neutral and fear) and the presented agents (actor and avatar). In addition, we found that the avatar faces elicited significantly stronger reactions than the actor face for theta and alpha oscillations. Especially theta EEG frequencies responded specifically to visual emotional stimulation and were revealed to be sensitive to the emotional content of the face, whereas alpha frequency was modulated by all the stimulus types. We can conclude that the computerized avatar faces affect both, ERP components and ERD/ERS and evoke neural effects that are different from the ones elicited by real faces. This was true, although the avatars were replicas of the human faces and contained similar characteristics in their expression.},
	language = {English},
	urldate = {2025-06-18},
	journal = {Frontiers in Neuroscience},
	author = {Sollfrank, Teresa and Kohnen, Oona and Hilfiker, Peter and Kegel, Lorena C. and Jokeit, Hennric and Brugger, Peter and Loertscher, Miriam L. and Rey, Anton and Mersch, Dieter and Sternagel, Joerg and Weber, Michel and Grunwald, Thomas},
	month = apr,
	year = {2021},
	note = {Publisher: Frontiers},
	keywords = {alpha, Avatar, EEG, emotion, ERP, Face, theta},
	file = {Full Text PDF:C\:\\Users\\super\\Zotero\\storage\\JYLE2U5X\\Sollfrank et al. - 2021 - The Effects of Dynamic and Static Emotional Facial.pdf:application/pdf},
}

@article{bendall_brief_2016,
	title = {A {Brief} {Review} of {Research} {Using} {Near}-{Infrared} {Spectroscopy} to {Measure} {Activation} of the {Prefrontal} {Cortex} during {Emotional} {Processing}: {The} {Importance} of {Experimental} {Design}},
	volume = {10},
	issn = {1662-5161},
	shorttitle = {A {Brief} {Review} of {Research} {Using} {Near}-{Infrared} {Spectroscopy} to {Measure} {Activation} of the {Prefrontal} {Cortex} during {Emotional} {Processing}},
	url = {https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2016.00529/full},
	doi = {10.3389/fnhum.2016.00529},
	abstract = {During the past two decades there has been a pronounced increase in the number of published research studies that have employed near-infrared spectroscopy (NIRS) to measure neural activation. The technique is now an accepted neuroimaging tool adopted by cognitive neuroscientists to investigate a number of fields, one of which is the study of emotional processing. Crucially, one brain region that is important to the processing of emotional information is the prefrontal cortex (PFC) and NIRS is ideally suited to measuring activity in this region. Compared to other methods used to record neural activation, NIRS reduces the discomfort to participants, makes data collection from larger sample sizes more achievable, and allows measurement of activation during tasks involving physical movement. However, the use of NIRS to investigate the links between emotion and cognition has revealed mixed findings. For instance, whilst some studies report increased PFC activity associated with the processing of negative information, others show increased activity in relation to positive information. Research shows differences in PFC activity between different cognitive tasks, yet findings also vary within similar tasks. This work reviews a selection of recent studies that have adopted NIRS to study PFC activity during emotional processing in both healthy individuals and patient populations. It highlights the key differences between research findings and argues that variations in experimental design could be a contributing factor to the mixed results. Guidance is provided for future work in this area in order to improve consistency within this growing field.},
	language = {English},
	urldate = {2025-06-18},
	journal = {Frontiers in Human Neuroscience},
	author = {Bendall, Robert C. A. and Eachus, Peter and Thompson, Catherine},
	month = oct,
	year = {2016},
	note = {Publisher: Frontiers},
	keywords = {emotion, mood, near-infrared spectroscopy, Neuroimaging, prefrontal cortex (PFC), review},
	file = {Full Text PDF:C\:\\Users\\super\\Zotero\\storage\\CLBV9VU8\\Bendall et al. - 2016 - A Brief Review of Research Using Near-Infrared Spe.pdf:application/pdf},
}

@article{dyck_recognition_2008,
	title = {Recognition {Profile} of {Emotions} in {Natural} and {Virtual} {Faces}},
	volume = {3},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0003628},
	doi = {10.1371/journal.pone.0003628},
	abstract = {Background Computer-generated virtual faces become increasingly realistic including the simulation of emotional expressions. These faces can be used as well-controlled, realistic and dynamic stimuli in emotion research. However, the validity of virtual facial expressions in comparison to natural emotion displays still needs to be shown for the different emotions and different age groups. Methodology/Principal Findings Thirty-two healthy volunteers between the age of 20 and 60 rated pictures of natural human faces and faces of virtual characters (avatars) with respect to the expressed emotions: happiness, sadness, anger, fear, disgust, and neutral. Results indicate that virtual emotions were recognized comparable to natural ones. Recognition differences in virtual and natural faces depended on specific emotions: whereas disgust was difficult to convey with the current avatar technology, virtual sadness and fear achieved better recognition results than natural faces. Furthermore, emotion recognition rates decreased for virtual but not natural faces in participants over the age of 40. This specific age effect suggests that media exposure has an influence on emotion recognition. Conclusions/Significance Virtual and natural facial displays of emotion may be equally effective. Improved technology (e.g. better modelling of the naso-labial area) may lead to even better results as compared to trained actors. Due to the ease with which virtual human faces can be animated and manipulated, validated artificial emotional expressions will be of major relevance in future research and therapeutic applications.},
	language = {en},
	number = {11},
	urldate = {2025-06-18},
	journal = {PLOS ONE},
	author = {Dyck, Miriam and Winbeck, Maren and Leiberg, Susanne and Chen, Yuhan and Gur, Rurben C. and Mathiak, Klaus},
	month = nov,
	year = {2008},
	note = {Publisher: Public Library of Science},
	keywords = {Age groups, Computer games, Emotions, Face, Face recognition, Facial expressions, Fear, Happiness},
	pages = {e3628},
	file = {Full Text PDF:C\:\\Users\\super\\Zotero\\storage\\2M54R2PP\\Dyck et al. - 2008 - Recognition Profile of Emotions in Natural and Vir.pdf:application/pdf},
}

@article{chen_realness_2024,
	title = {Realness of face images can be decoded from non-linear modulation of {EEG} responses},
	volume = {14},
	copyright = {2024 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-024-56130-1},
	doi = {10.1038/s41598-024-56130-1},
	abstract = {Artificially created human faces play an increasingly important role in our digital world. However, the so-called uncanny valley effect may cause people to perceive highly, yet not perfectly human-like faces as eerie, bringing challenges to the interaction with virtual agents. At the same time, the neurocognitive underpinnings of the uncanny valley effect remain elusive. Here, we utilized an electroencephalography (EEG) dataset of steady-state visual evoked potentials (SSVEP) in which participants were presented with human face images of different stylization levels ranging from simplistic cartoons to actual photographs. Assessing neuronal responses both in frequency and time domain, we found a non-linear relationship between SSVEP amplitudes and stylization level, that is, the most stylized cartoon images and the real photographs evoked stronger responses than images with medium stylization. Moreover, realness of even highly similar stylization levels could be decoded from the EEG data with task-related component analysis (TRCA). Importantly, we also account for confounding factors, such as the size of the stimulus face’s eyes, which previously have not been adequately addressed. Together, this study provides a basis for future research and neuronal benchmarking of real-time detection of face realness regarding three aspects: SSVEP-based neural markers, efficient classification methods, and low-level stimulus confounders.},
	language = {en},
	number = {1},
	urldate = {2025-06-18},
	journal = {Scientific Reports},
	author = {Chen, Yonghao and Stephani, Tilman and Bagdasarian, Milena Teresa and Hilsmann, Anna and Eisert, Peter and Villringer, Arno and Bosse, Sebastian and Gaebler, Michael and Nikulin, Vadim V.},
	month = mar,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Cognitive neuroscience, Neuroscience, Visual system},
	pages = {5683},
	file = {Full Text PDF:C\:\\Users\\super\\Zotero\\storage\\DAKIMBDS\\Chen et al. - 2024 - Realness of face images can be decoded from non-li.pdf:application/pdf},
}

@article{ekman1978facial,
  title={Facial action coding system},
  author={Ekman, Paul and Friesen, Wallace V},
  journal={Environmental Psychology \& Nonverbal Behavior},
  year={1978}
}

@article{powell_social_2018,
	title = {Social {Origins} of {Cortical} {Face} {Areas}},
	volume = {22},
	issn = {1364-6613},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6098735/},
	doi = {10.1016/j.tics.2018.06.009},
	abstract = {Recently acquired fMRI data from human and macaque infants provide novel insight into the origins of cortical networks specialized for perceiving faces. Data from both species converge: cortical regions responding preferentially to faces are present and spatially organized early in infancy, though fully selective face areas emerge much later. What explains the earliest cortical responses to faces? We review two proposed mechanisms: proto-organization for simple shapes in visual cortex, and an innate subcortical schematic face template. We propose, in addition, a third mechanism. Infants choose to look at faces in order to engage in positively-valenced, contingent social interactions. Activity in medial prefrontal cortex during social interactions may, directly or indirectly, guide the organization of cortical face areas.},
	number = {9},
	urldate = {2025-06-19},
	journal = {Trends in cognitive sciences},
	author = {Powell, Lindsey J. and Kosakowski, Heather L. and Saxe, Rebecca},
	month = sep,
	year = {2018},
	pmid = {30041864},
	pmcid = {PMC6098735},
	pages = {752--763},
	file = {Accepted Version:C\:\\Users\\super\\Zotero\\storage\\E6GFABUY\\Powell et al. - 2018 - Social Origins of Cortical Face Areas.pdf:application/pdf},
}

@article{ekman1971constants,
  title={Constants across cultures in the face and emotion.},
  author={Ekman, Paul and Friesen, Wallace V},
  journal={Journal of personality and social psychology},
  volume={17},
  number={2},
  pages={124},
  year={1971},
  publisher={American Psychological Association}
}

@article{mori_uncanny_2012,
	title = {The {Uncanny} {Valley} [{From} the {Field}]},
	volume = {19},
	issn = {1558-223X},
	url = {https://ieeexplore.ieee.org/document/6213238/},
	doi = {10.1109/MRA.2012.2192811},
	abstract = {More than 40 years ago, Masahiro Mori, a robotics professor at the Tokyo Institute of Technology, wrote an essay [1] on how he envisioned people's reactions to robots that looked and acted almost like a human. In particular, he hypothesized that a person's response to a humanlike robot would abruptly shift from empathy to revulsion as it approached, but failed to attain, a lifelike appearance. This descent into eeriness is known as the uncanny valley. The essay appeared in an obscure Japanese journal called Energy in 1970, and in subsequent years, it received almost no attention. However, more recently, the concept of the uncanny valley has rapidly attracted interest in robotics and other scientific circles as well as in popular culture. Some researchers have explored its implications for human-robot interaction and computer-graphics animation, whereas others have investigated its biological and social roots. Now interest in the uncanny valley should only intensify, as technology evolves and researchers build robots that look human. Although copies of Mori's essay have circulated among researchers, a complete version hasn't been widely available. The following is the first publication of an English translation that has been authorized and reviewed by Mori. (See “Turning Point” in this issue for an interview with Mori.).},
	number = {2},
	urldate = {2025-06-20},
	journal = {IEEE Robotics \& Automation Magazine},
	author = {Mori, Masahiro and MacDorman, Karl F. and Kageki, Norri},
	month = jun,
	year = {2012},
	pages = {98--100},
	file = {Full Text PDF:C\:\\Users\\super\\Zotero\\storage\\26PQDF72\\Mori et al. - 2012 - The Uncanny Valley [From the Field].pdf:application/pdf},
}

@article{schindler_differential_2017,
	title = {Differential effects of face-realism and emotion on event-related brain potentials and their implications for the uncanny valley theory},
	volume = {7},
	copyright = {2017 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/srep45003},
	doi = {10.1038/srep45003},
	abstract = {Cartoon characters are omnipresent in popular media. While few studies have scientifically investigated their processing, in computer graphics, efforts are made to increase realism. Yet, close approximations of reality have been suggested to evoke sometimes a feeling of eeriness, the “uncanny valley” effect. Here, we used high-density electroencephalography to investigate brain responses to professionally stylized happy, angry, and neutral character faces. We employed six face-stylization levels varying from abstract to realistic and investigated the N170, early posterior negativity (EPN), and late positive potential (LPP) event-related components. The face-specific N170 showed a u-shaped modulation, with stronger reactions towards both most abstract and most realistic compared to medium-stylized faces. For abstract faces, N170 was generated more occipitally than for real faces, implying stronger reliance on structural processing. Although emotional faces elicited highest amplitudes on both N170 and EPN, on the N170 realism and expression interacted. Finally, LPP increased linearly with face realism, reflecting activity increase in visual and parietal cortex for more realistic faces. Results reveal differential effects of face stylization on distinct face processing stages and suggest a perceptual basis to the uncanny valley hypothesis. They are discussed in relation to face perception, media design, and computer graphics.},
	language = {en},
	number = {1},
	urldate = {2025-06-20},
	journal = {Scientific Reports},
	author = {Schindler, Sebastian and Zell, Eduard and Botsch, Mario and Kissler, Johanna},
	month = mar,
	year = {2017},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computer science, Human behaviour, Sensory processing},
	pages = {45003},
	file = {Full Text PDF:C\:\\Users\\super\\Zotero\\storage\\SSCSMPTZ\\Schindler et al. - 2017 - Differential effects of face-realism and emotion o.pdf:application/pdf},
}
@article{barrett_solving_2006,
	author = {Barrett, Lisa Feldman},
	title = {Solving the Emotion Paradox: Categorization and the Experience of Emotion},
	journal = {Personality and Social Psychology Review},
	volume = {10},
	number = {1},
	pages = {20--46},
	year = {2006},
	doi = {10.1207/s15327957pspr1001_2},
	note = {PMID: 16430327},
	url = {https://doi.org/10.1207/s15327957pspr1001_2},
	eprint = {https://doi.org/10.1207/s15327957pspr1001_2},
	abstract = {In this article, I introduce an emotion paradox: People believe that they know an emotion when they see it, and as a consequence assume that emotions are discrete events that can be recognized with some degree of accuracy, but scientists have yet to produce a set of clear and consistent criteria for indicating when an emotion is present and when it is not. I propose one solution to this paradox: People experience an emotion when they conceptualize an instance of affective feeling. In this view, the experience of emotion is an act of categorization, guided by embodied knowledge about emotion. The result is a model of emotion experience that has much in common with the social psychological literature on person perception and with literature on embodied conceptual knowledge as it has recently been applied to social psychology.}
}

@article{yucel_functional_2017,
	series = {Synthetic {Biology} and {Biomedical} {Engineering} / {Neural} {Engineering}},
	title = {Functional {Near} {Infrared} {Spectroscopy}: {Enabling} routine functional brain imaging},
	volume = {4},
	issn = {2468-4511},
	shorttitle = {Functional {Near} {Infrared} {Spectroscopy}},
	url = {https://www.sciencedirect.com/science/article/pii/S2468451117300697},
	doi = {10.1016/j.cobme.2017.09.011},
	abstract = {Functional Near-Infrared Spectroscopy (fNIRS) maps human brain function by measuring and imaging local changes in hemoglobin concentrations in the brain that arise from the modulation of cerebral blood flow and oxygen metabolism by neural activity. Since its advent over 20 years ago, researchers have exploited and continuously advanced the ability of near infrared light to penetrate through the scalp and skull in order to non-invasively monitor changes in cerebral hemoglobin concentrations that reflect brain activity. We review recent advances in signal processing and hardware that significantly improve the capabilities of fNIRS by reducing the impact of confounding signals to improve statistical robustness of the brain signals and by enhancing the density, spatial coverage, and wearability of measuring devices respectively. We then summarize the application areas that are experiencing rapid growth as fNIRS begins to enable routine functional brain imaging.},
	urldate = {2025-06-20},
	journal = {Current Opinion in Biomedical Engineering},
	author = {Yücel, Meryem A. and Selb, Juliette J. and Huppert, Theodore J. and Franceschini, Maria Angela and Boas, David A.},
	month = dec,
	year = {2017},
	keywords = {Brain function, fNIRS, Functional brain imaging, Hemodynamics},
	pages = {78--86},
	file = {Accepted Version:C\:\\Users\\super\\Zotero\\storage\\EEP3FY3D\\Yücel et al. - 2017 - Functional Near Infrared Spectroscopy Enabling ro.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\super\\Zotero\\storage\\AGILC5E5\\S2468451117300697.html:text/html},
}

@article{barrett_are_2006,
	title = {Are {Emotions} {Natural} {Kinds}?},
	volume = {1},
	copyright = {https://journals.sagepub.com/page/policies/text-and-data-mining-license},
	issn = {1745-6916, 1745-6924},
	url = {https://journals.sagepub.com/doi/10.1111/j.1745-6916.2006.00003.x},
	doi = {10.1111/j.1745-6916.2006.00003.x},
	abstract = {Laypeople and scientists alike believe that they know anger, or sadness, or fear, when they see it. These emotions and a few others are presumed to have specific causal mechanisms in the brain and properties that are observable (on the face, in the voice, in the body, or in experience)—that is, they are assumed to be natural kinds. If a given emotion is a natural kind and can be identiﬁed objectively, then it is possible to make discoveries about that emotion. Indeed, the scientific study of emotion is founded on this assumption. In this article, I review the accumulating empirical evidence that is inconsistent with the view that there are kinds of emotion with boundaries that are carved in nature. I then consider what moving beyond a natural-kind view might mean for the scientific understanding of emotion.},
	language = {en},
	number = {1},
	urldate = {2025-06-19},
	journal = {Perspectives on Psychological Science},
	author = {Barrett, Lisa Feldman},
	month = mar,
	year = {2006},
	pages = {28--58},
	file = {Barrett - 2006 - Are Emotions Natural Kinds.pdf:C\:\\Users\\super\\Zotero\\storage\\8JUZF3AE\\Barrett - 2006 - Are Emotions Natural Kinds.pdf:application/pdf},
}

@article{sato_amygdala_2004,
	title = {The amygdala processes the emotional significance of facial expressions: an {fMRI} investigation using the interaction between expression and face direction},
	volume = {22},
	issn = {1053-8119},
	shorttitle = {The amygdala processes the emotional significance of facial expressions},
	url = {https://www.sciencedirect.com/science/article/pii/S1053811904001235},
	doi = {10.1016/j.neuroimage.2004.02.030},
	abstract = {Neuroimaging studies have shown activity in the amygdala in response to facial expressions of emotion, but the specific role of the amygdala remains unknown. We hypothesized that the amygdala is involved in emotional but not basic sensory processing for facial expressions. To test this hypothesis, we manipulated the face directions of emotional expressions in the unilateral visual fields; this manipulation made it possible to alter the emotional significance of the facial expression for the observer without affecting the physical features of the expression. We presented angry/neutral expressions looking toward/away from the subject and depicted brain activity using fMRI. After the image acquisitions, the subject's experience of negative emotion when perceiving each stimulus was also investigated. The left amygdala showed the interaction between emotional expression and face direction, indicating higher activity for angry expressions looking toward the subjects than angry expressions looking away from them. The experienced emotion showed the corresponding interaction. Regression analysis showed a positive relation between the left amygdala activity and experienced emotion. These results suggest that the amygdala is involved in emotional but not visuoperceptual processing for emotional facial expressions, which specifically includes the decoding of emotional significance and elicitation of one's own emotions corresponding to that significance.},
	number = {2},
	urldate = {2025-06-21},
	journal = {NeuroImage},
	author = {Sato, Wataru and Yoshikawa, Sakiko and Kochiyama, Takanori and Matsumura, Michikazu},
	month = jun,
	year = {2004},
	keywords = {Amygdala, Emotional facial expressions, Experienced emotion, Face direction, fMRI},
	pages = {1006--1013},
	file = {1-s2.0-S1053811904001235-main.pdf:C\:\\Users\\super\\Downloads\\1-s2.0-S1053811904001235-main.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\super\\Zotero\\storage\\HAYKA9EK\\S1053811904001235.html:text/html},
}

@article{huppert_commentary_2016,
	title = {Commentary on the statistical properties of noise and its implication on general linear models in functional near-infrared spectroscopy},
	volume = {3},
	issn = {2329-423X, 2329-4248},
	url = {https://www.spiedigitallibrary.org/journals/neurophotonics/volume-3/issue-1/010401/Commentary-on-the-statistical-properties-of-noise-and-its-implication/10.1117/1.NPh.3.1.010401.full},
	doi = {10.1117/1.NPh.3.1.010401},
	abstract = {Functional near-infrared spectroscopy (fNIRS) is a noninvasive neuroimaging technique that uses low levels of light to measure changes in cerebral blood oxygenation levels. In the majority of NIRS functional brain studies, analysis of this data is based on a statistical comparison of hemodynamic levels between a baseline and task or between multiple task conditions by means of a linear regression model: the so-called general linear model. Although these methods are similar to their implementation in other fields, particularly for functional magnetic resonance imaging, the specific application of these methods in fNIRS research differs in several key ways related to the sources of noise and artifacts unique to fNIRS. In this brief communication, we discuss the application of linear regression models in fNIRS and the modifications needed to generalize these models in order to deal with structured (colored) noise due to systemic physiology and noise heteroscedasticity due to motion artifacts. The objective of this work is to present an overview of these noise properties in the context of the linear model as it applies to fNIRS data. This work is aimed at explaining these mathematical issues to the general fNIRS experimental researcher but is not intended to be a complete mathematical treatment of these concepts.},
	number = {1},
	urldate = {2025-06-21},
	journal = {Neurophotonics},
	author = {Huppert, Theodore J.},
	month = mar,
	year = {2016},
	note = {Publisher: SPIE},
	pages = {010401},
	file = {Full Text PDF:C\:\\Users\\super\\Zotero\\storage\\8TYA73JM\\Huppert - 2016 - Commentary on the statistical properties of noise .pdf:application/pdf},
}

@article{weisenbach_reduced_2014,
	title = {Reduced emotion processing efficiency in healthy males relative to females},
	volume = {9},
	issn = {1749-5016},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3980801/},
	doi = {10.1093/scan/nss137},
	abstract = {This study examined sex differences in categorization of facial emotions and activation of brain regions supportive of those classifications. In Experiment 1, performance on the Facial Emotion Perception Test (FEPT) was examined among 75 healthy females and 63 healthy males. Females were more accurate in the categorization of fearful expressions relative to males. In Experiment 2, 3T functional magnetic resonance imaging data were acquired for a separate sample of 21 healthy females and 17 healthy males while performing the FEPT. Activation to neutral facial expressions was subtracted from activation to sad, angry, fearful and happy facial expressions. Although females and males demonstrated activation in some overlapping regions for all emotions, many regions were exclusive to females or males. For anger, sad and happy, males displayed a larger extent of activation than did females, and greater height of activation was detected in diffuse cortical and subcortical regions. For fear, males displayed greater activation than females only in right postcentral gyri. With one exception in females, performance was not associated with activation. Results suggest that females and males process emotions using different neural pathways, and these differences cannot be explained by performance variations.},
	number = {3},
	urldate = {2025-06-22},
	journal = {Social Cognitive and Affective Neuroscience},
	author = {Weisenbach, Sara L. and Rapport, Lisa J. and Briceno, Emily M. and Haase, Brennan D. and Vederman, Aaron C. and Bieliauskas, Linas A. and Welsh, Robert C. and Starkman, Monica N. and McInnis, Melvin G. and Zubieta, Jon-Kar and Langenecker, Scott A.},
	month = mar,
	year = {2014},
	pmid = {23196633},
	pmcid = {PMC3980801},
	pages = {316--325},
	file = {Full Text:C\:\\Users\\super\\Zotero\\storage\\SXXXS6LA\\Weisenbach et al. - 2014 - Reduced emotion processing efficiency in healthy m.pdf:application/pdf},
}

@article{haxby_distributed_2000,
	title = {The distributed human neural system for face perception},
	volume = {4},
	issn = {1364-6613, 1879-307X},
	url = {https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(00)01482-0},
	doi = {10.1016/S1364-6613(00)01482-0},
	language = {English},
	number = {6},
	urldate = {2025-06-25},
	journal = {Trends in Cognitive Sciences},
	author = {Haxby, James V. and Hoffman, Elizabeth A. and Gobbini, M. Ida and Haxby, James V. and Hoffman, Elizabeth A. and Gobbini, M. Ida and Haxby, James V. and Hoffman, Elizabeth A. and Gobbini, M. Ida},
	month = jun,
	year = {2000},
	pmid = {10827445},
	note = {Publisher: Elsevier},
	keywords = {Cognitive Science, Emotion, Face perception, Facial expression, Functional brain imaging, Neuroscience, Semantic knowledge, Spatial attention},
	pages = {223--233},
	file = {Full Text PDF:C\:\\Users\\super\\Zotero\\storage\\7JDHLP4B\\Haxby et al. - 2000 - The distributed human neural system for face perce.pdf:application/pdf},
}

@article{keslerwest_neural_2001,
	title = {Neural substrates of facial emotion processing using {fMRI}},
	volume = {11},
	issn = {0926-6410},
	url = {https://www.sciencedirect.com/science/article/pii/S0926641000000732},
	doi = {10.1016/S0926-6410(00)00073-2},
	abstract = {We identified human brain regions involved in the perception of sad, frightened, happy, angry, and neutral facial expressions using functional magnetic resonance imaging (fMRI). Twenty-one healthy right-handed adult volunteers (11 men, 10 women; aged 18–45; mean age 21.6 years) participated in four separate runs, one for each of the four emotions. Participants viewed blocks of emotionally expressive faces alternating with blocks of neutral faces and scrambled images. In comparison with scrambled images, neutral faces activated the fusiform gyri, the right lateral occipital gyrus, the right superior temporal sulcus, the inferior frontal gyri, and the amygdala/entorhinal cortex. In comparisons of emotional and neutral faces, we found that (1) emotional faces elicit increased activation in a subset of cortical regions involved in neutral face processing and in areas not activated by neutral faces; (2) differences in activation as a function of emotion category were most evident in the frontal lobes; (3) men showed a differential neural response depending upon the emotion expressed but women did not.},
	number = {2},
	urldate = {2025-06-26},
	journal = {Cognitive Brain Research},
	author = {Kesler/West, Marilyn L. and Andersen, Anders H. and Smith, Charles D. and Avison, Malcolm J. and Davis, C. Ervin and Kryscio, Richard J. and Blonder, Lee X.},
	month = apr,
	year = {2001},
	keywords = {Emotional processing, Facial expression, Functional imaging},
	pages = {213--226},
	file = {1-s2.0-S0926641000000732-main.pdf:C\:\\Users\\super\\Downloads\\1-s2.0-S0926641000000732-main.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\super\\Zotero\\storage\\TPJF4TUG\\S0926641000000732.html:text/html},
}

@article{moser_amygdala_2007,
	title = {Amygdala activation at {3T} in response to human and avatar facial expressions of emotions},
	volume = {161},
	issn = {0165-0270},
	url = {https://www.sciencedirect.com/science/article/pii/S0165027006005085},
	doi = {10.1016/j.jneumeth.2006.10.016},
	abstract = {Facial expressions of emotions are important in nonverbal communication. Although numerous neural structures have been identified to be involved in emotional face processing, the amygdala is thought to be a core moderator. While previous studies have relied on facial images of humans, the present study is concerned with the effect of computer-generated (avatar) emotional faces on amygdala activation. Moreover, elicited activation patterns in response to viewing avatar faces are compared with the neuronal responses to human facial expressions of emotions. Twelve healthy subjects (five females) performed facial emotion recognition tasks with optimized 3T event-related fMRI. Robust amygdala activation was apparent in response to both human and avatar emotional faces, but the response was significantly stronger to human faces in face-sensitive structures, i.e. fusiform gyri. We suggest that avatars could be a useful tool in neuroimaging studies of facial expression processing because they elicit amygdala activation similarly to human faces, yet have the advantage of being highly manipulable and fully controllable. However, the finding of differences between human and avatar faces in face-sensitive regions indicates the presence of mechanisms by which human brains can differentiate between them. This mechanism merits further investigation.},
	number = {1},
	urldate = {2025-06-26},
	journal = {Journal of Neuroscience Methods},
	author = {Moser, Ewald and Derntl, Birgit and Robinson, Simon and Fink, Bernhard and Gur, Ruben C. and Grammer, Karl},
	month = mar,
	year = {2007},
	keywords = {Amygdala, Avatar, Emotion, Facial expression, FACS, fMRI},
	pages = {126--133},
	file = {1-s2.0-S0165027006005085-main.pdf:C\:\\Users\\super\\Downloads\\1-s2.0-S0165027006005085-main.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\super\\Zotero\\storage\\L6TM8JA5\\S0165027006005085.html:text/html},
}

@article{balas_artificial_2015,
	title = {Artificial faces are harder to remember},
	volume = {52},
	issn = {0747-5632},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4505187/},
	doi = {10.1016/j.chb.2015.06.018},
	abstract = {Observers interact with artificial faces in a range of different settings and in many cases must remember and identify computer-generated faces. In general, however, most adults have heavily biased experience favoring real faces over synthetic faces. It is well known that face recognition abilities are affected by experience such that faces belonging to “out-groups” defined by race or age are more poorly remembered and harder to discriminate from one another than faces belonging to the “in-group.” Here, we examine the extent to which artificial faces form an “out-group” in this sense when other perceptual categories are matched. We rendered synthetic faces using photographs of real human faces and compared performance in a memory task and a discrimination task across real and artificial versions of the same faces. We found that real faces were easier to remember, but only slightly more discriminable than artificial faces. Artificial faces were also equally susceptible to the well-known face inversion effect, suggesting that while these patterns are still processed by the human visual system in a face-like manner, artificial appearance does compromise the efficiency of face processing.},
	urldate = {2025-06-27},
	journal = {Computers in human behavior},
	author = {Balas, Benjamin and Pacella, Jonathan},
	month = nov,
	year = {2015},
	pmid = {26195852},
	pmcid = {PMC4505187},
	pages = {331--337},
	file = {Accepted Version:C\:\\Users\\super\\Zotero\\storage\\XUKU345L\\Balas and Pacella - 2015 - Artificial faces are harder to remember.pdf:application/pdf},
}

@article{katsyri_those_2018,
	title = {Those {Virtual} {People} all {Look} the {Same} to me: {Computer}-{Rendered} {Faces} {Elicit} a {Higher} {False} {Alarm} {Rate} {Than} {Real} {Human} {Faces} in a {Recognition} {Memory} {Task}},
	volume = {9},
	issn = {1664-1078},
	shorttitle = {Those {Virtual} {People} all {Look} the {Same} to me},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6086000/},
	doi = {10.3389/fpsyg.2018.01362},
	abstract = {Virtual as compared with real human characters can elicit a sense of uneasiness in human observers, characterized by lack of familiarity and even feelings of eeriness (the “uncanny valley” hypothesis). Here we test the possibility that this alleged lack of familiarity is literal in the sense that people have lesser perceptual expertise in processing virtual as compared with real human faces. Sixty-four participants took part in a recognition memory study in which they first learned a set of faces and were then asked to recognize them in a testing session. We used real and virtual (computer-rendered) versions of the same faces, presented in either upright or inverted orientation. Real and virtual faces were matched for low-level visual features such as global luminosity and spatial frequency contents. Our results demonstrated a higher response bias toward responding “seen before” for virtual as compared with real faces, which was further explained by a higher false alarm rate for the former. This finding resembles a similar effect for recognizing human faces from other than one's own ethnic groups (the “other race effect”). Virtual faces received clearly higher subjective eeriness ratings than real faces. Our results did not provide evidence of poorer overall recognition memory or lesser inversion effect for virtual faces, however. The higher false alarm rate finding supports the notion that lesser perceptual expertise may contribute to the lack of subjective familiarity with virtual faces. We discuss alternative interpretations and provide suggestions for future research.},
	urldate = {2025-06-27},
	journal = {Frontiers in Psychology},
	author = {Kätsyri, Jari},
	month = aug,
	year = {2018},
	pmid = {30123166},
	pmcid = {PMC6086000},
	pages = {1362},
	file = {Full Text:C\:\\Users\\super\\Zotero\\storage\\6W255YTA\\Kätsyri - 2018 - Those Virtual People all Look the Same to me Comp.pdf:application/pdf},
}

@article{burke_evolution_2013,
	title = {The {Evolution} of {Holistic} {Processing} of {Faces}},
	volume = {4},
	issn = {1664-1078},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3560284/},
	doi = {10.3389/fpsyg.2013.00011},
	abstract = {In this paper we examine the holistic processing of faces from an evolutionary perspective, clarifying what such an approach entails, and evaluating the extent to which the evidence currently available permits any strong conclusions. While it seems clear that the holistic processing of faces depends on mechanisms evolved to perform that task, our review of the comparative literature reveals that there is currently insufficient evidence (or sometimes insufficiently compelling evidence) to decide when in our evolutionary past such processing may have arisen. It is also difficult to assess what kinds of selection pressures may have led to evolution of such a mechanism, or even what kinds of information holistic processing may have originally evolved to extract, given that many sources of socially relevant face-based information other than identity depend on integrating information across different regions of the face – judgments of expression, behavioral intent, attractiveness, sex, age, etc. We suggest some directions for future research that would help to answer these important questions.},
	urldate = {2025-06-27},
	journal = {Frontiers in Psychology},
	author = {Burke, Darren and Sulikowski, Danielle},
	month = jan,
	year = {2013},
	pmid = {23382721},
	pmcid = {PMC3560284},
	pages = {11},
	file = {Full Text:C\:\\Users\\super\\Zotero\\storage\\UK9SCCUZ\\Burke and Sulikowski - 2013 - The Evolution of Holistic Processing of Faces.pdf:application/pdf},
}

@article{hirsch_frontal_2017,
	title = {Frontal temporal and parietal systems synchronize within and across brains during live eye-to-eye contact},
	volume = {157},
	issn = {1053-8119},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5863547/},
	doi = {10.1016/j.neuroimage.2017.06.018},
	abstract = {Human eye-to-eye contact is a primary source of social cues and communication. In spite of the biological significance of this interpersonal interaction, the underlying neural processes are not well-understood. This knowledge gap, in part, reflects limitations of conventional neuroimaging methods, including solitary confinement in the bore of a scanner and minimal tolerance of head movement that constrain investigations of natural, two-person interactions. However, these limitations are substantially resolved by recent technical developments in functional near-infrared spectroscopy (fNIRS), a non-invasive spectral absorbance technique that detects changes in blood oxygen levels in the brain by using surface-mounted optical sensors. Functional NIRS is tolerant of limited head motion and enables simultaneous acquisitions of neural signals from two interacting partners in natural conditions. We employ fNIRS to advance a data-driven theoretical framework for two-person neuroscience motivated by the Interactive Brain Hypothesis which proposes that interpersonal interaction between individuals evokes neural mechanisms not engaged during solo, non-interactive, behaviors. Within this context, two specific hypotheses related to eye-to-eye contact, functional specificity and functional synchrony, were tested. The functional specificity hypothesis proposes that eye-to-eye contact engages specialized, within-brain, neural systems; and the functional synchrony hypothesis proposes that eye-to-eye contact engages specialized, across-brain, neural processors that are synchronized between dyads. Signals acquired during eye-to-eye contact between partners (interactive condition) were compared to signals acquired during mutual gaze at the eyes of a picture-face (non-interactive condition). In accordance with the specificity hypothesis, responses during eye-to-eye contact were greater than eye-to-picture gaze for a left frontal cluster that included pars opercularis (associated with canonical language production functions known as Broca’s region), pre- and supplementary motor cortices (associated with articulatory systems), as well as the subcentral area. This frontal cluster was also functionally connected to a cluster located in the left superior temporal gyrus (associated with canonical language receptive functions known as Wernicke’s region), primary somatosensory cortex, and the subcentral area. In accordance with the functional synchrony hypothesis, cross-brain coherence during eye-to-eye contact relative to eye-to-picture gaze increased for signals originating within left superior temporal, middle temporal, and supramarginal gyri as well as the pre- and supplementary motor cortices of both interacting brains. These synchronous cross-brain regions are also associated with known language functions, and were partner-specific (i.e., disappeared with randomly assigned partners). Together, both within and across-brain neural correlates of eye-to-eye contact included components of previously established productive and receptive language systems. These findings reveal a left frontal, temporal, and parietal long-range network that mediates neural responses during eye-to-eye contact between dyads, and advance insight into elemental mechanisms of social and interpersonal interactions.,},
	urldate = {2025-06-27},
	journal = {NeuroImage},
	author = {Hirsch, Joy and Zhang, Xian and Noah, J. Adam and Ono, Yumie},
	month = aug,
	year = {2017},
	pmid = {28619652},
	pmcid = {PMC5863547},
	pages = {314--330},
	file = {Accepted Version:C\:\\Users\\super\\Zotero\\storage\\VGM95M7R\\Hirsch et al. - 2017 - Frontal temporal and parietal systems synchronize .pdf:application/pdf},
}

@article{katsyri_amygdala_2020,
	title = {Amygdala responds to direct gaze in real but not in computer-generated faces},
	volume = {204},
	issn = {1053-8119},
	url = {https://www.sciencedirect.com/science/article/pii/S1053811919308079},
	doi = {10.1016/j.neuroimage.2019.116216},
	abstract = {Computer-generated (CG) faces are an important visual interface for human-computer interaction in social contexts. Here we investigated whether the human brain processes emotion and gaze similarly in real and carefully matched CG faces. Real faces evoked greater responses in the fusiform face area than CG faces, particularly for fearful expressions. Emotional (angry and fearful) facial expressions evoked similar activations in the amygdala in real and CG faces. Direct as compared with averted gaze elicited greater fMRI responses in the amygdala regardless of facial expression but only for real and not for CG faces. We observed an interaction effect between gaze and emotion (i.e., the shared signal effect) in the right posterior temporal sulcus and other regions, but not in the amygdala, and we found no evidence for different shared signal effects in real and CG faces. Taken together, the present findings highlight similarities (emotional processing in the amygdala) and differences (overall processing in the fusiform face area, gaze processing in the amygdala) in the neural processing of real and CG faces.},
	urldate = {2025-06-27},
	journal = {NeuroImage},
	author = {Kätsyri, Jari and de Gelder, Beatrice and de Borst, Aline W.},
	month = jan,
	year = {2020},
	pages = {116216},
	file = {Full Text:C\:\\Users\\super\\Zotero\\storage\\283DZMIL\\Kätsyri et al. - 2020 - Amygdala responds to direct gaze in real but not i.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\super\\Zotero\\storage\\SNBUCPF8\\S1053811919308079.html:text/html},
}

@article{cushing_neurodynamics_2018,
	title = {Neurodynamics and connectivity during facial fear perception: {The} role of threat exposure and signal congruity},
	volume = {8},
	copyright = {2018 The Author(s)},
	issn = {2045-2322},
	shorttitle = {Neurodynamics and connectivity during facial fear perception},
	url = {https://www.nature.com/articles/s41598-018-20509-8},
	doi = {10.1038/s41598-018-20509-8},
	abstract = {Fearful faces convey threat cues whose meaning is contextualized by eye gaze: While averted gaze is congruent with facial fear (both signal avoidance), direct gaze (an approach signal) is incongruent with it. We have previously shown using fMRI that the amygdala is engaged more strongly by fear with averted gaze during brief exposures. However, the amygdala also responds more to fear with direct gaze during longer exposures. Here we examined previously unexplored brain oscillatory responses to characterize the neurodynamics and connectivity during brief ({\textasciitilde}250 ms) and longer ({\textasciitilde}883 ms) exposures of fearful faces with direct or averted eye gaze. We performed two experiments: one replicating the exposure time by gaze direction interaction in fMRI (N = 23), and another where we confirmed greater early phase locking to averted-gaze fear (congruent threat signal) with MEG (N = 60) in a network of face processing regions, regardless of exposure duration. Phase locking to direct-gaze fear (incongruent threat signal) then increased significantly for brief exposures at {\textasciitilde}350 ms, and at {\textasciitilde}700 ms for longer exposures. Our results characterize the stages of congruent and incongruent facial threat signal processing and show that stimulus exposure strongly affects the onset and duration of these stages.},
	language = {en},
	number = {1},
	urldate = {2025-06-28},
	journal = {Scientific Reports},
	author = {Cushing, Cody A. and Im, Hee Yeon and Adams, Reginald B. and Ward, Noreen and Albohn, Daniel N. and Steiner, Troy G. and Kveraga, Kestutis},
	month = feb,
	year = {2018},
	note = {Publisher: Nature Publishing Group},
	keywords = {Cognitive neuroscience, Perception, Visual system},
	pages = {2776},
	file = {Full Text PDF:C\:\\Users\\super\\Zotero\\storage\\Q4LVCGEM\\Cushing et al. - 2018 - Neurodynamics and connectivity during facial fear .pdf:application/pdf},
}

@article{jamieson_differential_2021,
	title = {Differential {Modulation} of {Effective} {Connectivity} in the {Brain}’s {Extended} {Face} {Processing} {System} by {Fearful} and {Sad} {Facial} {Expressions}},
	volume = {8},
	copyright = {Copyright © 2021 Jamieson et al.. This is an open-access article distributed under the terms of the Creative Commons Attribution 4.0 International license, which permits unrestricted use, distribution and reproduction in any medium provided that the original work is properly attributed.},
	issn = {2373-2822},
	url = {https://www.eneuro.org/content/8/2/ENEURO.0380-20.2021},
	doi = {10.1523/ENEURO.0380-20.2021},
	abstract = {The processing of emotional facial expressions is underpinned by the integration of information from a distributed network of brain regions. Despite investigations into how different emotional expressions alter the functional relationships within this network, there remains limited research examining which regions drive these interactions. This study investigated effective connectivity during the processing of sad and fearful facial expressions to better understand how these stimuli differentially modulate emotional face processing circuitry. Ninety-eight healthy human adolescents and young adults, aged between 15 and 25 years, underwent an implicit emotional face processing fMRI task. Using dynamic causal modeling (DCM), we examined five brain regions implicated in face processing. These were restricted to the right hemisphere and included the occipital and fusiform face areas, amygdala, and dorsolateral prefrontal cortex (dlPFC) and ventromedial prefrontal cortex (vmPFC). Processing sad and fearful facial expressions were associated with greater positive connectivity from the amygdala to dlPFC. Only the processing of fearful facial expressions was associated with greater negative connectivity from the vmPFC to amygdala. Compared with processing sad faces, processing fearful faces was associated with significantly greater connectivity from the amygdala to dlPFC. No difference was found between the processing of these expressions and the connectivity from the vmPFC to amygdala. Overall, our findings indicate that connectivity from the amygdala and dlPFC appears to be responding to dimensional features which differ between these expressions, likely those relating to arousal. Further research is necessary to examine whether this relationship is also observable for positively valenced emotions.},
	language = {en},
	number = {2},
	urldate = {2025-06-28},
	journal = {eNeuro},
	author = {Jamieson, Alec J. and Davey, Christopher G. and Harrison, Ben J.},
	month = mar,
	year = {2021},
	pmid = {33658311},
	note = {Publisher: Society for Neuroscience
Section: Research Article: New Research},
	keywords = {dynamic causal modeling, effective connectivity, emotion processing, fMRI, youth},
	file = {Full Text PDF:C\:\\Users\\super\\Zotero\\storage\\VM2IQDJB\\Jamieson et al. - 2021 - Differential Modulation of Effective Connectivity .pdf:application/pdf},
}

@article{adolphs_biology_2013,
	title = {The {Biology} of {Fear}},
	volume = {23},
	issn = {0960-9822},
	url = {https://www.cell.com/current-biology/abstract/S0960-9822(12)01435-2},
	doi = {10.1016/j.cub.2012.11.055},
	language = {English},
	number = {2},
	urldate = {2025-06-28},
	journal = {Current Biology},
	author = {Adolphs, Ralph},
	month = jan,
	year = {2013},
	pmid = {23347946},
	note = {Publisher: Elsevier},
	pages = {R79--R93},
	file = {Full Text PDF:C\:\\Users\\super\\Zotero\\storage\\3TNRGPN7\\Adolphs - 2013 - The Biology of Fear.pdf:application/pdf},
}

@article{liang_multivariate_2018,
	title = {Multivariate {Pattern} {Classification} of {Facial} {Expressions} {Based} on {Large}-{Scale} {Functional} {Connectivity}},
	volume = {12},
	issn = {1662-5161},
	url = {https://www.frontiersin.org/journals/human-neuroscience/articles/10.3389/fnhum.2018.00094/full},
	doi = {10.3389/fnhum.2018.00094},
	abstract = {It is an important question how human beings achieve efficient recognition of others’ facial expressions in cognitive neuroscience, and it has been identified that specific cortical regions show preferential activation to facial expressions in previous studies. However, the potential contributions of the connectivity patterns in the processing of facial expressions remained unclear. The present functional magnetic resonance imaging (fMRI) study explored whether facial expressions could be decoded from the functional connectivity (FC) patterns using multivariate pattern analysis combined with machine learning algorithms (fcMVPA). We employed a block design experiment and collected neural activities while participants viewed facial expressions of six basic emotions (anger, disgust, fear, joy, sadness, and surprise). Both static and dynamic expression stimuli were included in our study. A behavioral experiment after scanning confirmed the validity of the facial stimuli presented during the fMRI experiment with classification accuracies and emotional intensities. We obtained whole-brain FC patterns for each facial expression and found that both static and dynamic facial expressions could be successfully decoded from the FC patterns. Moreover, we identified the expression-discriminative networks for the static and dynamic facial expressions, which span beyond the conventional face-selective areas. Overall, these results reveal that large-scale FC patterns may also contain rich expression information to accurately decode facial expressions, suggesting a novel mechanism, which includes general interactions between distributed brain regions, and that contributes to the human facial expression recognition.},
	language = {English},
	urldate = {2025-06-28},
	journal = {Frontiers in Human Neuroscience},
	author = {Liang, Yin and Liu, Baolin and Li, Xianglin and Wang, Peiyuan},
	month = mar,
	year = {2018},
	note = {Publisher: Frontiers},
	keywords = {facial expressions, fMRI, functional connectivity, Machine learning algorithm, multivariate pattern analysis},
	file = {Full Text PDF:C\:\\Users\\super\\Zotero\\storage\\28BPZ3MH\\Liang et al. - 2018 - Multivariate Pattern Classification of Facial Expr.pdf:application/pdf},
}

@article{underwood_networks_2021,
	title = {Networks underpinning emotion: {A} systematic review and synthesis of functional and effective connectivity},
	volume = {243},
	issn = {1053-8119},
	shorttitle = {Networks underpinning emotion},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8905299/},
	doi = {10.1016/j.neuroimage.2021.118486},
	abstract = {•We reviewed 33 studies of functional connectivity of emotion in healthy participants.•Our results challenge a hierarchical model of emotion processing.•Causal connectivity analyze identify dynamic modulatory relationships between regions.•We derive a quality tool to make recommendations addressing variability in study design., Existing models of emotion processing are based almost exclusively on brain activation data, yet make assumptions about network connectivity. There is a need to integrate connectivity findings into these models., We systematically reviewed all studies of functional and effective connectivity employing tasks to investigate negative emotion processing and regulation in healthy participants. Thirty-three studies met inclusion criteria. A quality assessment tool was derived from prominent neuroimaging papers. The evidence supports existing models, with primarily limbic regions for salience and identification, and frontal areas important for emotion regulation. There was mixed support for the assumption that regulatory influences on limbic and sensory areas come predominantly from prefrontal areas. Rather, studies quantifying effective connectivity reveal context-dependent dynamic modulatory relationships between occipital, subcortical, and frontal regions, arguing against purely top-down regulatory theoretical models. Our quality assessment tool found considerable variability in study design and tasks employed., The findings support and extend those of previous syntheses focused on activation studies, and provide evidence for a more nuanced view of connectivity in networks of human emotion processing and regulation.},
	urldate = {2025-06-28},
	journal = {Neuroimage},
	author = {Underwood, Raphael and Tolmeijer, Eva and Wibroe, Johannes and Peters, Emmanuelle and Mason, Liam},
	month = nov,
	year = {2021},
	pmid = {34438255},
	pmcid = {PMC8905299},
	pages = {118486},
	file = {Full Text:C\:\\Users\\super\\Zotero\\storage\\ZGUEKDVB\\Underwood et al. - 2021 - Networks underpinning emotion A systematic review.pdf:application/pdf},
}

@article{johnson_newborns_1991,
  title={Newborns' preferential tracking of face-like stimuli and its subsequent decline},
  author={Johnson, Mark H and Dziurawiec, Suzanne and Ellis, Hadyn and Morton, John},
  journal={Cognition},
  volume={40},
  number={1-2},
  pages={1--19},
  year={1991},
  publisher={Elsevier}
}

@article{palmer_face_2020,
	title = {Face {Pareidolia} {Recruits} {Mechanisms} for {Detecting} {Human} {Social} {Attention}},
	volume = {31},
	issn = {0956-7976},
	url = {https://doi.org/10.1177/0956797620924814},
	doi = {10.1177/0956797620924814},
	abstract = {Face pareidolia is the phenomenon of seeing facelike structures in everyday objects. Here, we tested the hypothesis that face pareidolia, rather than being limited to a cognitive or mnemonic association, reflects the activation of visual mechanisms that typically process human faces. We focused on sensory cues to social attention, which engage cell populations in temporal cortex that are susceptible to habituation effects. Repeated exposure to “pareidolia faces” that appear to have a specific direction of attention causes a systematic bias in the perception of where human faces are looking, indicating that overlapping sensory mechanisms are recruited when we view human faces and when we experience face pareidolia. These cross-adaptation effects are significantly reduced when pareidolia is abolished by removing facelike features from the objects. These results indicate that face pareidolia is essentially a perceptual phenomenon, occurring when sensory input is processed by visual mechanisms that have evolved to extract specific social content from human faces.},
	language = {EN},
	number = {8},
	urldate = {2025-07-02},
	journal = {Psychological Science},
	author = {Palmer, Colin J. and Clifford, Colin W. G.},
	month = aug,
	year = {2020},
	note = {Publisher: SAGE Publications Inc},
	pages = {1001--1012},
	file = {Accepted Version:C\:\\Users\\super\\Zotero\\storage\\LFBMJIPJ\\Palmer and Clifford - 2020 - Face Pareidolia Recruits Mechanisms for Detecting .pdf:application/pdf},
}

@article{sheehan_morphological_2014,
	title = {Morphological and population genomic evidence that human faces have evolved to signal individual identity},
	volume = {5},
	copyright = {2014 Springer Nature Limited},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/ncomms5800},
	doi = {10.1038/ncomms5800},
	abstract = {Facial recognition plays a key role in human interactions, and there has been great interest in understanding the evolution of human abilities for individual recognition and tracking social relationships. Individual recognition requires sufficient cognitive abilities and phenotypic diversity within a population for discrimination to be possible. Despite the importance of facial recognition in humans, the evolution of facial identity has received little attention. Here we demonstrate that faces evolved to signal individual identity under negative frequency-dependent selection. Faces show elevated phenotypic variation and lower between-trait correlations compared with other traits. Regions surrounding face-associated single nucleotide polymorphisms show elevated diversity consistent with frequency-dependent selection. Genetic variation maintained by identity signalling tends to be shared across populations and, for some loci, predates the origin of Homo sapiens. Studies of human social evolution tend to emphasize cognitive adaptations, but we show that social evolution has shaped patterns of human phenotypic and genetic diversity as well.},
	language = {en},
	number = {1},
	urldate = {2025-07-02},
	journal = {Nature Communications},
	author = {Sheehan, Michael J. and Nachman, Michael W.},
	month = sep,
	year = {2014},
	note = {Publisher: Nature Publishing Group},
	keywords = {Evolution, Genomics},
	pages = {4800},
	file = {Full Text PDF:C\:\\Users\\super\\Zotero\\storage\\LDEZZNUV\\Sheehan and Nachman - 2014 - Morphological and population genomic evidence that.pdf:application/pdf},
}

@article{cowen_sixteen_2021,
	title = {Sixteen facial expressions occur in similar contexts worldwide},
	volume = {589},
	copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-020-3037-7},
	doi = {10.1038/s41586-020-3037-7},
	abstract = {Understanding the degree to which human facial expressions co-vary with specific social contexts across cultures is central to the theory that emotions enable adaptive responses to important challenges and opportunities1–6. Concrete evidence linking social context to specific facial expressions is sparse and is largely based on survey-based approaches, which are often constrained by language and small sample sizes7–13. Here, by applying machine-learning methods to real-world, dynamic behaviour, we ascertain whether naturalistic social contexts (for example, weddings or sporting competitions) are associated with specific facial expressions14 across different cultures. In two experiments using deep neural networks, we examined the extent to which 16 types of facial expression occurred systematically in thousands of contexts in 6 million videos from 144 countries. We found that each kind of facial expression had distinct associations with a set of contexts that were 70\% preserved across 12 world regions. Consistent with these associations, regions varied in how frequently different facial expressions were produced as a function of which contexts were most salient. Our results reveal fine-grained patterns in human facial expressions that are preserved across the modern world.},
	language = {en},
	number = {7841},
	urldate = {2025-07-02},
	journal = {Nature},
	author = {Cowen, Alan S. and Keltner, Dacher and Schroff, Florian and Jou, Brendan and Adam, Hartwig and Prasad, Gautam},
	month = jan,
	year = {2021},
	note = {Publisher: Nature Publishing Group},
	keywords = {Emotion, Human behaviour},
	pages = {251--257},
	file = {Full Text PDF:C\:\\Users\\super\\Zotero\\storage\\8CGDIFHM\\Cowen et al. - 2021 - Sixteen facial expressions occur in similar contex.pdf:application/pdf},
}

@article{willis_first_2006,
	author = {Willis, Janine and Todorov, Alexander},
	title = {First Impressions: Making Up Your Mind After a 100-Ms Exposure to a Face},
	journal = {Psychological Science},
	volume = {17},
	number = {7},
	pages = {592--598},
	year = {2006},
	doi = {10.1111/j.1467-9280.2006.01750.x},
	note = {PMID: 16866745},
	url = {https://doi.org/10.1111/j.1467-9280.2006.01750.x},
	eprint = {https://doi.org/10.1111/j.1467-9280.2006.01750.x},
	abstract = {People often draw trait inferences from the facial appearance of other people. We investigated the minimal conditions under which people make such inferences. In five experiments, each focusing on a specific trait judgment, we manipulated the exposure time of unfamiliar faces. Judgments made after a 100-ms exposure correlated highly with judgments made in the absence of time constraints, suggesting that this exposure time was sufficient for participants to form an impression. In fact, for all judgments—attractiveness, likeability, trustworthiness, competence, and aggressiveness—increased exposure time did not significantly increase the correlations. When exposure time increased from 100 to 500 ms, participants' judgments became more negative, response times for judgments decreased, and confidence in judgments increased. When exposure time increased from 500 to 1,000 ms, trait judgments and response times did not change significantly (with one exception), but confidence increased for some of the judgments; this result suggests that additional time may simply boost confidence in judgments. However, increased exposure time led to more differentiated person impressions.}
}

@article{calbi_how_2017,
	title = {How {Context} {Influences} {Our} {Perception} of {Emotional} {Faces}: {A} {Behavioral} {Study} on the {Kuleshov} {Effect}},
	volume = {8},
	issn = {1664-1078},
	shorttitle = {How {Context} {Influences} {Our} {Perception} of {Emotional} {Faces}},
	url = {https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2017.01684/full},
	doi = {10.3389/fpsyg.2017.01684},
	abstract = {Facial expressions are of major importance in understanding the mental and emotional states of others. So far, most studies on the perception and comprehension of emotions have used isolated facial expressions as stimuli; for example, photographs of actors displaying facial expressions corresponding to one of the so called ‘basic emotions’. However, our real experience during social interactions is different: facial expressions of emotion are mostly perceived in a wider context, constituted by body language, the surrounding environment, and our beliefs and expectations. Already in the early twentieth century, the Russian filmmaker Lev Kuleshov argued that such context, established by intermediate shots of strong emotional content, could significantly change our interpretation of facial expressions in film. Prior experiments have shown behavioral effects pointing in this direction, but have only used static images as stimuli. Our study used a more ecological design with participants watching film sequences of neutral faces, crosscut with scenes of strong emotional content (evoking happiness or fear, plus neutral stimuli as a baseline condition). The task was to rate the emotion displayed by a target person’s face in terms of valence, arousal, and category. Results clearly demonstrated the presence of a significant effect in terms of both valence and arousal in the fear condition only. Moreover, participants tended to categorize the target person’s neutral facial expression choosing the emotion category congruent with the preceding context. Our results highlight the context-sensitivity of emotions and the importance of studying them under ecologically valid conditions.},
	language = {English},
	urldate = {2025-07-02},
	journal = {Frontiers in Psychology},
	author = {Calbi, Marta and Heimann, Katrin and Barratt, Daniel and Siri, Francesca and Umiltà, Maria A. and Gallese, Vittorio},
	month = oct,
	year = {2017},
	note = {Publisher: Frontiers},
	keywords = {Contexts, emotion, facial expressions, Film editing, Kuleshov Effect},
	file = {Full Text PDF:C\:\\Users\\super\\Zotero\\storage\\8VU6ITZG\\Calbi et al. - 2017 - How Context Influences Our Perception of Emotional.pdf:application/pdf},
}

@article{brooks_conceptual_2018,
	title = {Conceptual knowledge predicts the representational structure of facial emotion perception},
	volume = {2},
	issn = {2397-3374},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6788630/},
	doi = {10.1038/s41562-018-0376-6},
	abstract = {Recent theoretical accounts argue that conceptual knowledge dynamically interacts with processing of facial cues, fundamentally influencing visual perception of social and emotion categories. Evidence is accumulating for the idea that a perceiver’s conceptual knowledge about emotion is involved in emotion perception, even when stereotypic facial expressions are presented in isolation–. However, existing methods have not allowed a comprehensive assessment of the relationship between conceptual knowledge and emotion perception across individuals and emotion categories. Here we use a representational similarity analysis approach to show that conceptual knowledge predicts the representational structure of facial emotion perception. We conducted three studies using computer mouse-tracking and reverse-correlation paradigms. Overall, we found that when individuals believed two emotions to be conceptually more similar, faces from those categories were perceived with a corresponding similarity, even when controlling for any physical similarity in the stimuli themselves. When emotions were rated conceptually more similar, computer-mouse trajectories during emotion perception exhibited a greater simultaneous attraction to both category responses (despite only one emotion being depicted; studies  and ), and reverse-correlated face prototypes exhibited a greater visual resemblance (). Together, our findings suggest that differences in conceptual knowledge are reflected in the perceptual processing of facial emotion.},
	number = {8},
	urldate = {2025-07-02},
	journal = {Nature human behaviour},
	author = {Brooks, Jeffrey A. and Freeman, Jonathan B.},
	month = aug,
	year = {2018},
	pmid = {31209318},
	pmcid = {PMC6788630},
	pages = {581--591},
	file = {Full Text PDF:C\:\\Users\\super\\Zotero\\storage\\S28BVTFD\\Brooks and Freeman - 2018 - Conceptual knowledge predicts the representational.pdf:application/pdf},
}

@article{aviezer_automaticity_2011,
	title = {The automaticity of emotional face-context integration},
	volume = {11},
	issn = {1931-1516},
	doi = {10.1037/a0023578},
	abstract = {Recent studies have demonstrated that context can dramatically influence the recognition of basic facial expressions, yet the nature of this phenomenon is largely unknown. In the present paper we begin to characterize the underlying process of face-context integration. Specifically, we examine whether it is a relatively controlled or automatic process. In Experiment 1 participants were motivated and instructed to avoid using the context while categorizing contextualized facial expression, or they were led to believe that the context was irrelevant. Nevertheless, they were unable to disregard the context, which exerted a strong effect on their emotion recognition. In Experiment 2, participants categorized contextualized facial expressions while engaged in a concurrent working memory task. Despite the load, the context exerted a strong influence on their recognition of facial expressions. These results suggest that facial expressions and their body contexts are integrated in an unintentional, uncontrollable, and relatively effortless manner.},
	language = {eng},
	number = {6},
	journal = {Emotion (Washington, D.C.)},
	author = {Aviezer, Hillel and Bentin, Shlomo and Dudarev, Veronica and Hassin, Ran R.},
	month = dec,
	year = {2011},
	pmid = {21707150},
	pmcid = {PMC3242002},
	keywords = {Adolescent, Emotions, Facial Expression, Female, Humans, Male, Mental Recall, Photic Stimulation, Recognition, Psychology, Social Environment, Young Adult},
	pages = {1406--1414},
	file = {Accepted Version:C\:\\Users\\super\\Zotero\\storage\\EU8QWG28\\Aviezer et al. - 2011 - The automaticity of emotional face-context integra.pdf:application/pdf},
}

@article{lee_context_2012,
	title = {Context modulation of facial emotion perception differed by individual difference},
	volume = {7},
	issn = {1932-6203},
	doi = {10.1371/journal.pone.0032987},
	abstract = {BACKGROUND: Certain facial configurations are believed to be associated with distinct affective meanings (i.e. basic facial expressions), and such associations are common across cultures (i.e. universality of facial expressions). However, recently, many studies suggest that various types of contextual information, rather than facial configuration itself, are important factor for facial emotion perception.
METHODOLOGY/PRINCIPAL FINDINGS: To examine systematically how contextual information influences individuals' facial emotion perception, the present study estimated direct observers' perceptual thresholds for detecting negative facial expressions via a forced-choice psychophysical procedure using faces embedded in various emotional contexts. We additionally measured the individual differences in affective information-processing tendency (BIS/BAS) as a possible factor that may determine the extent to which contextual information on facial emotion perception is used. It was found that contextual information influenced observers' perceptual thresholds for facial emotion. Importantly, individuals' affective-information tendencies modulated the extent to which they incorporated context information into their facial emotion perceptions.
CONCLUSIONS/SIGNIFICANCE: The findings of this study suggest that facial emotion perception not only depends on facial configuration, but the context in which the face appears as well. This contextual influence appeared differently with individual's characteristics of information processing. In summary, we conclude that individual character traits, as well as facial configuration and the context in which a face appears, need to be taken into consideration regarding facial emotional perception.},
	language = {eng},
	number = {3},
	journal = {PloS One},
	author = {Lee, Tae-Ho and Choi, June-Seek and Cho, Yang Seok},
	year = {2012},
	pmid = {22431992},
	pmcid = {PMC3303876},
	keywords = {Behavior, Emotions, Facial Expression, Humans, Perception, Photic Stimulation, Sensory Thresholds},
	pages = {e32987},
	file = {Full Text:C\:\\Users\\super\\Zotero\\storage\\MQ422ERK\\Lee et al. - 2012 - Context modulation of facial emotion perception di.pdf:application/pdf},
}

@article{matsumoto_half-century_2022,
	author = {Matsumoto, David and Wilson, Matthew},
	title = {A Half-Century Assessment of the Study of Culture and Emotion},
	journal = {Journal of Cross-Cultural Psychology},
	volume = {53},
	number = {7-8},
	pages = {917--934},
	year = {2022},
	doi = {10.1177/00220221221084236},
	url = {https://doi.org/10.1177/00220221221084236},
	abstract = {Research on emotion and affective sciences is flourishing today like never before. The impetus for this surge is largely rooted in studies of emotion across cultures and coincides with the half century existence of the International Association for Cross-Cultural Psychology (IACCP). Beginning with studies initially documenting the universality of the expression and recognition of certain facial expressions of emotion in the 1970s, cross-cultural research was crucial in providing further evidence for the universality of antecedents, appraisals, subjective experiences, self-reported responses, and physiological reactions throughout the 1980s and 1990s. That same literature also demonstrated the existence of many cultural variations in these emotion domains, as well as in the concepts, language, attitudes, beliefs, and values about emotion. We review this literature with the goal of demonstrating some of the many meaningful and important contributions IACCP and cross-cultural studies have made to the field of emotion and affective sciences. This area of research has also been marred by considerable controversies for almost the entire period of study, and we describe those as well. We conclude with a presentation of current models of understanding the association between culture and emotion that integrate disparate cross-cultural findings and address controversies in the field, in the hope that such models can serve as a platform for renewed cross-cultural research in this area for the next half century and beyond.}
}

@article{jack_facial_2012,
	title = {Facial expressions of emotion are not culturally universal},
	volume = {109},
	url = {https://www.pnas.org/doi/full/10.1073/pnas.1200155109},
	doi = {10.1073/pnas.1200155109},
	abstract = {Since Darwin’s seminal works, the universality of facial expressions of emotion has remained one of the longest standing debates in the biological and social sciences. Briefly stated, the universality hypothesis claims that all humans communicate six basic internal emotional states (happy, surprise, fear, disgust, anger, and sad) using the same facial movements by virtue of their biological and evolutionary origins [Susskind JM, et al. (2008) Nat Neurosci 11:843–850]. Here, we refute this assumed universality. Using a unique computer graphics platform that combines generative grammars [Chomsky N (1965) MIT Press, Cambridge, MA] with visual perception, we accessed the mind’s eye of 30 Western and Eastern culture individuals and reconstructed their mental representations of the six basic facial expressions of emotion. Cross-cultural comparisons of the mental representations challenge universality on two separate counts. First, whereas Westerners represent each of the six basic emotions with a distinct set of facial movements common to the group, Easterners do not. Second, Easterners represent emotional intensity with distinctive dynamic eye activity. By refuting the long-standing universality hypothesis, our data highlight the powerful influence of culture on shaping basic behaviors once considered biologically hardwired. Consequently, our data open a unique nature–nurture debate across broad fields from evolutionary psychology and social neuroscience to social networking via digital avatars.},
	number = {19},
	urldate = {2025-07-02},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Jack, Rachael E. and Garrod, Oliver G. B. and Yu, Hui and Caldara, Roberto and Schyns, Philippe G.},
	month = may,
	year = {2012},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {7241--7244},
	file = {Full Text PDF:C\:\\Users\\super\\Zotero\\storage\\U6MYKKYQ\\Jack et al. - 2012 - Facial expressions of emotion are not culturally u.pdf:application/pdf},
}

@article{ruttkay_cultural_2009,
	title = {Cultural dialects of real and synthetic emotional facial expressions},
	volume = {24},
	issn = {1435-5655},
	url = {https://doi.org/10.1007/s00146-009-0219-0},
	doi = {10.1007/s00146-009-0219-0},
	abstract = {In this article we discuss the aspects of designing facial expressions for virtual humans (VHs) with a specific culture. First we explore the notion of cultures and its relevance for applications with a VH. Then we give a general scheme of designing emotional facial expressions, and identify the stages where a human is involved, either as a real person with some specific role, or as a VH displaying facial expressions. We discuss how the display and the emotional meaning of facial expressions may be measured in objective ways, and how the culture of displayers and the judges may influence the process of analyzing human facial expressions and evaluating synthesized ones. We review psychological experiments on cross-cultural perception of emotional facial expressions. By identifying the culturally critical issues of data collection and interpretation with both real and VHs, we aim at providing a methodological reference and inspiration for further research.},
	language = {en},
	number = {3},
	urldate = {2025-07-02},
	journal = {AI \& SOCIETY},
	author = {Ruttkay, Zsófia},
	month = oct,
	year = {2009},
	keywords = {Basketball Player, Cross-Cultural Psychology, Cultural  Psychology, Emotion Recognition, Emotion Theory, Emotional Development, Facial Expression, Facial Expression Recognition, Global and International Culture, Intercultural Communication, Language Usage},
	pages = {307--315},
	file = {Full Text PDF:C\:\\Users\\super\\Zotero\\storage\\WTQ3SZGL\\Ruttkay - 2009 - Cultural dialects of real and synthetic emotional .pdf:application/pdf},
}

@article{schneider_show_2014,
	title = {Show me how you walk and I tell you how you feel — A functional near-infrared spectroscopy study on emotion perception based on human gait},
	journal = {NeuroImage},
	volume = {85},
	pages = {380-390},
	year = {2014},
	note = {Celebrating 20 Years of Functional Near Infrared Spectroscopy (fNIRS)},
	issn = {1053-8119},
	doi = {https://doi.org/10.1016/j.neuroimage.2013.07.078},
	url = {https://www.sciencedirect.com/science/article/pii/S1053811913008471},
	author = {Sabrina Schneider and Andrea Christensen and Florian B. Häußinger and Andreas J. Fallgatter and Martin A. Giese and Ann-Christine Ehlis},
	keywords = {Emotion perception, Bodily expressions, Human gait, Near-infrared spectroscopy},
	abstract = {The ability to recognize and adequately interpret emotional states in others plays a fundamental role in regulating social interaction. Body language presents an essential element of nonverbal communication which is often perceived prior to mimic expression. However, the neural networks that underlie the processing of emotionally expressive body movement and body posture are poorly understood. 33 healthy subjects have been investigated using the optically based imaging method functional near-infrared spectroscopy (fNIRS) during the performance of a newly developed emotion discrimination paradigm consisting of faceless avatars expressing fearful, angry, sad, happy or neutral gait patterns. Participants were instructed to judge (a) the presented emotional state (emotion task) and (b) the observed walking speed of the respective avatar (speed task). We measured increases in cortical oxygenated haemoglobin (O2HB) in response to visual stimulation during emotion discrimination. These O2HB concentration changes were enhanced for negative emotions in contrast to neutral gait sequences in right occipito-temporal and left temporal and temporo-parietal brain regions. Moreover, fearful and angry bodies elicited higher activation increases during the emotion task compared to the speed task. Haemodynamic responses were correlated with a number of behavioural measures, whereby a positive relationship between emotion regulation strategy preference and O2HB concentration increases after sad walks was mediated by the ability to accurately categorize sad walks. Our results support the idea of a distributed brain network involved in the recognition of bodily emotion expression that comprises visual association areas as well as body/movement perception specific cortical regions that are also sensitive to emotion. This network is activated less when the emotion is not intentionally processed (i.e. during the speed task). Furthermore, activity of this perceptive network is, mediated by the ability to correctly recognize emotions, indirectly connected to active emotion regulation processes. We conclude that a full understanding of emotion perception and its neural substrate requires the investigation of dynamic representations and means of expression other than the face.}
}

@article{ke_dynamic_2025,
	title = {Dynamic brain connectivity predicts emotional arousal during naturalistic movie-watching},
	volume = {21},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1012994},
	doi = {10.1371/journal.pcbi.1012994},
	abstract = {Human affective experience varies along the dimensions of valence (positivity or negativity) and arousal (high or low activation). It remains unclear how these dimensions are represented in the brain and whether the representations are shared across different individuals and diverse situational contexts. In this study, we first utilized two publicly available functional MRI datasets of participants watching movies to build predictive models of moment-to-moment emotional arousal and valence from dynamic functional brain connectivity. We tested the models by predicting emotional arousal and valence both within and across datasets. Our results revealed a generalizable arousal representation characterized by the interactions between multiple large-scale functional networks. The arousal representation generalized to two additional movie-watching datasets with different participants viewing different movies. In contrast, we did not find evidence of a generalizable valence representation. Taken together, our findings reveal a generalizable representation of emotional arousal embedded in patterns of dynamic functional connectivity, suggesting a common underlying neural signature of emotional arousal across individuals and situational contexts. We have made our model and analysis scripts publicly available to facilitate its use by other researchers in decoding moment-to-moment emotional arousal in novel datasets, providing a new tool to probe affective experience using fMRI.},
	language = {en},
	number = {4},
	urldate = {2025-07-03},
	journal = {PLOS Computational Biology},
	author = {Ke, Jin and Song, Hayoung and Bai, Zihan and Rosenberg, Monica D. and Leong, Yuan Chang},
	month = apr,
	year = {2025},
	note = {Publisher: Public Library of Science},
	keywords = {Behavior, Connectomics, Emotions, Forecasting, Functional magnetic resonance imaging, Neural networks, Permutation, Preprocessing},
	pages = {e1012994},
	file = {Full Text PDF:C\:\\Users\\super\\Zotero\\storage\\INQ8FM54\\Ke et al. - 2025 - Dynamic brain connectivity predicts emotional arou.pdf:application/pdf},
}

@article{yang_dynamic_2015,
	title = {Dynamic {Functional} {Brain} {Connectivity} for {Face} {Perception}},
	volume = {9},
	issn = {1662-5161},
	doi = {10.3389/fnhum.2015.00662},
	abstract = {Face perception is mediated by a distributed brain network comprised of the core system at occipito-temporal areas and the extended system at other relevant brain areas involving bilateral hemispheres. In this study we explored how the brain connectivity changes over the time for face-sensitive processing. We investigated the dynamic functional connectivity in face perception by analyzing time-dependent EEG phase synchronization in four different frequency bands: theta (4-7 Hz), alpha (8-14 Hz), beta (15-24 Hz), and gamma (25-45 Hz) bands in the early stages of face processing from 30 to 300 ms. High-density EEG were recorded from subjects who were passively viewing faces, buildings, and chairs. The dynamic connectivity within the core system and between the extended system were investigated. Significant differences between faces and non-faces mainly appear in theta band connectivity: (1) at the time segment of 90-120 ms between parietal area and occipito-temporal area in the right hemisphere, and (2) at the time segment of 150-180 ms between bilateral occipito-temporal areas. These results indicate (1) the importance of theta-band connectivity in the face-sensitive processing, and (2) that different parts of network are involved for the initial stage of face categorization and the stage of face structural encoding.},
	language = {eng},
	journal = {Frontiers in Human Neuroscience},
	author = {Yang, Yuan and Qiu, Yihong and Schouten, Alfred C.},
	year = {2015},
	pmid = {26696870},
	pmcid = {PMC4672064},
	keywords = {dynamic functional connectivity, ERP, face perception, high-density EEG, phase lag index},
	pages = {662},
	file = {Full Text:C\:\\Users\\super\\Zotero\\storage\\5XE26NSM\\Yang et al. - 2015 - Dynamic Functional Brain Connectivity for Face Per.pdf:application/pdf},
}
